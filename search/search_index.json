{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"nwbmatic NWB creator from various data streams TODO badges Overview This package started as a main feature of pynapple IO module. It is now a standalone package to help create NWB from various data streams from electrophysiological and calcium imaging pipelines. It supports outputs from : Electrophysiology Calcium imaging Behavior Phy matlab CNMF-E DeepLabCut Neurosuite Inscopix CNMF-E Optitrack Minian Suite2P Warning A larger choice of data format is available from neuroconv Usage The general workflow of loading a session is described by the infographic below. As it is challenging to accomodate all possible types of format, we aimed to keep the IO of nwbmatic minimal while allowing the user to inherit the base loader and import their own custom io functions. The base loader is thus responsible for initializing the NWB file containing the tracking data, the epochs and the session informations. Getting Started Installation The best way to install nwbmatic is with pip within a new conda environment : $ conda create --name nwbmatic pip python=3.8 $ conda activate nwbmatic $ pip install nwbmatic or directly from the source code: $ conda create --name nwbmatic pip python=3.8 $ conda activate nwbmatic $ # clone the repository $ git clone https://github.com/pynapple-org/nwbmatic.git $ cd nwbmatic $ # Install in editable mode with `-e` or, equivalently, `--editable` $ pip install -e . This procedure will install all the dependencies including pynapple pandas numpy pynwb 2.0 h5py Example In this example, a session preprocessed with phy will be copied to NWB and loaded. import nwbmatic as ntm data = ntm . load_session ( \"path/to/my/session\" , \"phy\" ) Credits Thanks to Selen Calgin, Sara Mahallati and Luigi Petrucco for their contributions.","title":"Overview"},{"location":"#nwbmatic","text":"NWB creator from various data streams TODO badges","title":"nwbmatic"},{"location":"#overview","text":"This package started as a main feature of pynapple IO module. It is now a standalone package to help create NWB from various data streams from electrophysiological and calcium imaging pipelines. It supports outputs from : Electrophysiology Calcium imaging Behavior Phy matlab CNMF-E DeepLabCut Neurosuite Inscopix CNMF-E Optitrack Minian Suite2P Warning A larger choice of data format is available from neuroconv","title":"Overview"},{"location":"#usage","text":"The general workflow of loading a session is described by the infographic below. As it is challenging to accomodate all possible types of format, we aimed to keep the IO of nwbmatic minimal while allowing the user to inherit the base loader and import their own custom io functions. The base loader is thus responsible for initializing the NWB file containing the tracking data, the epochs and the session informations.","title":"Usage"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#installation","text":"The best way to install nwbmatic is with pip within a new conda environment : $ conda create --name nwbmatic pip python=3.8 $ conda activate nwbmatic $ pip install nwbmatic or directly from the source code: $ conda create --name nwbmatic pip python=3.8 $ conda activate nwbmatic $ # clone the repository $ git clone https://github.com/pynapple-org/nwbmatic.git $ cd nwbmatic $ # Install in editable mode with `-e` or, equivalently, `--editable` $ pip install -e . This procedure will install all the dependencies including pynapple pandas numpy pynwb 2.0 h5py","title":"Installation"},{"location":"#example","text":"In this example, a session preprocessed with phy will be copied to NWB and loaded. import nwbmatic as ntm data = ntm . load_session ( \"path/to/my/session\" , \"phy\" )","title":"Example"},{"location":"#credits","text":"Thanks to Selen Calgin, Sara Mahallati and Luigi Petrucco for their contributions.","title":"Credits"},{"location":"AUTHORS/","text":"Credits Development Lead Guillaume Viejo guillaume.viejo@gmail.com Contributors Sara Mahallati Luigi Petrucco Selen Calgin","title":"Authors"},{"location":"AUTHORS/#credits","text":"","title":"Credits"},{"location":"AUTHORS/#development-lead","text":"Guillaume Viejo guillaume.viejo@gmail.com","title":"Development Lead"},{"location":"AUTHORS/#contributors","text":"Sara Mahallati Luigi Petrucco Selen Calgin","title":"Contributors"},{"location":"CONTRIBUTING/","text":"Contributing Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: Types of Contributions Report Bugs Report bugs at https://github.com/pynapple-org/nwbmatic/issues . If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs Look through the GitHub issues for bugs. Anything tagged with \\\"bug\\\" and \\\"help wanted\\\" is open to whoever wants to implement it. Implement Features Look through the GitHub issues for features. Anything tagged with \\\"enhancement\\\" and \\\"help wanted\\\" is open to whoever wants to implement it. Write Documentation nwbmatic could always use more documentation, whether as part of the official nwbmatic docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback The best way to send feedback is to file an issue at https://github.com/pynapple-org/nwbmatic/issues . If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :) Get Started! Ready to contribute? Here's how to set up [nwbmatic]{https://github.com/pynapple-org/nwbmatic} for local development. Fork the [nwbmatic]{https://github.com/pynapple-org/nwbmatic} repo on GitHub. Clone your fork locally: $ git clone git@github.com:your_name_here/nwbmatic.git Install your local copy with pip. $ cd nwbmatic/ $ pip install -e . Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.nd. The pull request should work for Python 3.8, 3.9 and 3.10, and for PyPy.","title":"Contributing"},{"location":"CONTRIBUTING/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways:","title":"Contributing"},{"location":"CONTRIBUTING/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"CONTRIBUTING/#report-bugs","text":"Report bugs at https://github.com/pynapple-org/nwbmatic/issues . If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"CONTRIBUTING/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with \\\"bug\\\" and \\\"help wanted\\\" is open to whoever wants to implement it.","title":"Fix Bugs"},{"location":"CONTRIBUTING/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with \\\"enhancement\\\" and \\\"help wanted\\\" is open to whoever wants to implement it.","title":"Implement Features"},{"location":"CONTRIBUTING/#write-documentation","text":"nwbmatic could always use more documentation, whether as part of the official nwbmatic docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"CONTRIBUTING/#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/pynapple-org/nwbmatic/issues . If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :)","title":"Submit Feedback"},{"location":"CONTRIBUTING/#get-started","text":"Ready to contribute? Here's how to set up [nwbmatic]{https://github.com/pynapple-org/nwbmatic} for local development. Fork the [nwbmatic]{https://github.com/pynapple-org/nwbmatic} repo on GitHub. Clone your fork locally: $ git clone git@github.com:your_name_here/nwbmatic.git Install your local copy with pip. $ cd nwbmatic/ $ pip install -e . Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"CONTRIBUTING/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.nd. The pull request should work for Python 3.8, 3.9 and 3.10, and for PyPy.","title":"Pull Request Guidelines"},{"location":"io.allends/","text":"Allen Brain Atlas Neuropixel Loading Tutorial Author: Selen Calgin Date created: 13/07/2023 Last edited: 28/07/2023 The beauty of Pynapple's IO and the NWBmatic package is that loaders can be customized for any format of data. NWBmatic has many pre-made loaders, one of which being for the Allen Brain Atlas' Neuropixels dataset. The Neuropixels dataset contains electrophysiological data of the visual cortex and thalamus of mice who are shown passive visual stimuli. For more details about the dataset, please see documentation here: https://allensdk.readthedocs.io/en/latest/visual_coding_neuropixels.html This tutorial will demonstrate how to use NWMmatic's loader for the Allen Neuropixels data in conjunction with Pynapple for various analyses with the data. NWBmatic loads the data into Pynapple core objects, which can then be used for further analysis. Before you begin, make sure you have sufficient storage to download sessions from the database. The session sizes average about ~2gb. Let's get started! First, import libraries. # import libraries import nwbmatic as ntm import pynapple as nap import warnings warnings . filterwarnings ( 'ignore' ) import numpy as np import matplotlib.pyplot as plt % matplotlib inline To load a Neuropixels session, as with all data loading in NWBmatic, call NWBmatic's load_session() function. - path = where the Neuropixels data is downloaded/where it has been downloaded if you've already worked with a specific data session - session_type =\"allends\" indicates that we are loading data from the Allen Institute (DS = dataset). When you run the following cell, a GUI will appear. Using the GUI, choose from the two types of sessions in the Neuropixels database: 1) Brain observatory or 2) Functional connectivity. Then select the session ID you would like to load. Note: If this is your first time loading this session with NWBmatic, the loading will take a while as the data is first downloaded to your local system (to the path you indicated). If you have already loaded this session, loading the data will take about a minute. Additional note: In this tutorial, I am accessing session #715093703 in Brain Observatory. If you would like to follow along, choose the same session ID. All analyses will work with other sessions too, but may have different values than the ones I comment on here. # path to where data will be donwloaded path = r \"C:\\Users\\scalgi\\OneDrive - McGill University\\Peyrache Lab\\Data\\pynapple_test\" # load the data data = ntm . load_session ( path , session_type = \"allends\" ) # type: nap.io.allennp.AllenNP From our data object, we can access the following attributes: - epochs : a dictionary of IntervalSet that holds the start and end times of session epochs. In this dataset, epochs is trivial: there is only one epoch which is the session itself, under the key session . - stimulus_epochs_types : a dictionary of IntervalSet s that holds the start and end times of stimulus epochs, defined by the name of the stimulus type - stimulus_epochs_blocks : a dictionary of IntervalSet s that holds the start and end times of stimulus epoch, defined by the block of the stimulus. Each stimulus block contains one type of stimulus, and the block number indicates the order of when this block of stimulus was shown. Lack of stimulus (i.e. \"spontaneous\") is not given a block number. - stimulus_intervals : a dictionary of IntervalSet s that holds the start and end times of each presentation of of each stimulus type - optogenetic_stimulus_epochs : an IntervalSet that holds the start and end times of optogenetic stimulus epochs. None if session doesn't have it. - spikes : a TsGroup , which holds spike times for each unit and metadata on each unit - metadata : a dictionary of various Pandas Dataframes that holds various types of information: - stimulus_presentations : information about each stimulus presentation - stimulus_conditions : Each distinct stimulus state is called a \"stimulus condition\". Table holds additional information about each unique stimulus - stimulus_parameters : Dictionary of all stimulus parameters as keys and their range of values as values - channels : Information about all channels - probes : Information about all probes used - units : information about each unit identified in the session, including location in the brain - time_support : an IntervalSet containing the global time support based on epochs - stimulus_time_support : an IntervalSet containing the intervals of all stimulus blocks, providing a time support for stimuli Analyzing Neural Activity in the Primary Visual Cortex In this tutorial, we will be analyzing the neural activity of neurons in the primary visual cortex while also understanding how to use NWBmatic's loader for Allen Neuropixel data and how this data can be used in conjunction with Pynapple. Overview of Metadata First, let's get a grasp of the metadata we have access to. # data metadata print ( \"Stimulus presentations:\" ) stimulus_presentations = data . metadata [ \"stimulus_presentations\" ] print ( stimulus_presentations ) print ( \"Stimulus conditions\" ) stimulus_conditions = data . metadata [ \"stimulus_conditions\" ] print ( stimulus_conditions ) print ( \"Stimulus parameters:\" ) stimulus_parameters = data . metadata [ \"stimulus_parameters\" ] print ( stimulus_parameters ) print ( \"Probes:\" ) probes = data . metadata [ \"probes\" ] print ( probes ) print ( \"Channels:\" ) channels = data . metadata [ \"channels\" ] print ( channels ) Stimulus presentations: color contrast frame orientation \\ stimulus_presentation_id 0 null null null null 1 null 0.8 null 45.0 2 null 0.8 null 0.0 3 null 0.8 null 45.0 4 null 0.8 null 0.0 ... ... ... ... ... 70383 null 0.8 null 60.0 70384 null 0.8 null 90.0 70385 null 0.8 null 60.0 70386 null 0.8 null 60.0 70387 null 0.8 null 60.0 phase size \\ stimulus_presentation_id 0 null null 1 [3644.93333333, 3644.93333333] [20.0, 20.0] 2 [3644.93333333, 3644.93333333] [20.0, 20.0] 3 [3644.93333333, 3644.93333333] [20.0, 20.0] 4 [3644.93333333, 3644.93333333] [20.0, 20.0] ... ... ... 70383 0.75 [250.0, 250.0] 70384 0.0 [250.0, 250.0] 70385 0.0 [250.0, 250.0] 70386 0.5 [250.0, 250.0] 70387 0.0 [250.0, 250.0] spatial_frequency start_time stimulus_block \\ stimulus_presentation_id 0 null 13.470683 null 1 0.08 73.537433 0.0 2 0.08 73.770952 0.0 3 0.08 74.021150 0.0 4 0.08 74.271349 0.0 ... ... ... ... 70383 0.04 9133.889309 14.0 70384 0.02 9134.139517 14.0 70385 0.08 9134.389719 14.0 70386 0.32 9134.639920 14.0 70387 0.16 9134.890122 14.0 stimulus_name stop_time temporal_frequency \\ stimulus_presentation_id 0 spontaneous 73.537433 null 1 gabors 73.770952 4.0 2 gabors 74.021150 4.0 3 gabors 74.271349 4.0 4 gabors 74.521547 4.0 ... ... ... ... 70383 static_gratings 9134.139517 null 70384 static_gratings 9134.389719 null 70385 static_gratings 9134.639920 null 70386 static_gratings 9134.890122 null 70387 static_gratings 9135.140323 null x_position y_position duration \\ stimulus_presentation_id 0 null null 60.066750 1 0.0 30.0 0.233519 2 -30.0 -10.0 0.250199 3 10.0 20.0 0.250199 4 -40.0 -40.0 0.250199 ... ... ... ... 70383 null null 0.250209 70384 null null 0.250201 70385 null null 0.250201 70386 null null 0.250201 70387 null null 0.250201 stimulus_condition_id stimulus_presentation_id 0 0 1 1 2 2 3 3 4 4 ... ... 70383 4886 70384 4806 70385 4874 70386 4789 70387 4809 [70388 rows x 16 columns] Stimulus conditions color contrast frame mask opacity orientation \\ stimulus_condition_id 0 null null null null null null 1 null 0.8 null circle True 45.0 2 null 0.8 null circle True 0.0 3 null 0.8 null circle True 45.0 4 null 0.8 null circle True 0.0 ... ... ... ... ... ... ... 5022 null null 35.0 null null null 5023 null null 104.0 null null null 5024 null null 112.0 null null null 5025 null null 48.0 null null null 5026 null null 4.0 null null null phase size \\ stimulus_condition_id 0 null null 1 [3644.93333333, 3644.93333333] [20.0, 20.0] 2 [3644.93333333, 3644.93333333] [20.0, 20.0] 3 [3644.93333333, 3644.93333333] [20.0, 20.0] 4 [3644.93333333, 3644.93333333] [20.0, 20.0] ... ... ... 5022 null null 5023 null null 5024 null null 5025 null null 5026 null null spatial_frequency stimulus_name temporal_frequency \\ stimulus_condition_id 0 null spontaneous null 1 0.08 gabors 4.0 2 0.08 gabors 4.0 3 0.08 gabors 4.0 4 0.08 gabors 4.0 ... ... ... ... 5022 null natural_scenes null 5023 null natural_scenes null 5024 null natural_scenes null 5025 null natural_scenes null 5026 null natural_scenes null units x_position y_position color_triplet stimulus_condition_id 0 null null null null 1 deg 0.0 30.0 [1.0, 1.0, 1.0] 2 deg -30.0 -10.0 [1.0, 1.0, 1.0] 3 deg 10.0 20.0 [1.0, 1.0, 1.0] 4 deg -40.0 -40.0 [1.0, 1.0, 1.0] ... ... ... ... ... 5022 null null null null 5023 null null null null 5024 null null null null 5025 null null null null 5026 null null null null [5027 rows x 15 columns] Stimulus parameters: {'color': array([-1.0, 1.0], dtype=object), 'contrast': array([0.8, 1.0], dtype=object), 'frame': array([0.0, 1.0, 2.0, ..., 3598.0, 3599.0, -1.0], dtype=object), 'orientation': array([45.0, 0.0, 90.0, 315.0, 225.0, 135.0, 270.0, 180.0, 120.0, 150.0, 60.0, 30.0], dtype=object), 'phase': array(['[3644.93333333, 3644.93333333]', '[0.0, 0.0]', '[21211.93333333, 21211.93333333]', '0.5', '0.75', '0.0', '0.25'], dtype=object), 'size': array(['[20.0, 20.0]', '[300.0, 300.0]', '[250.0, 250.0]', '[1920.0, 1080.0]'], dtype=object), 'spatial_frequency': array(['0.08', '[0.0, 0.0]', '0.04', 0.32, 0.08, 0.04, 0.02, 0.16], dtype=object), 'temporal_frequency': array([4.0, 8.0, 2.0, 1.0, 15.0], dtype=object), 'x_position': array([0.0, -30.0, 10.0, -40.0, -10.0, 40.0, 30.0, 20.0, -20.0], dtype=object), 'y_position': array([30.0, -10.0, 20.0, -40.0, -20.0, 0.0, -30.0, 40.0, 10.0], dtype=object)} Probes: description location sampling_rate \\ id 810755797 probeA See electrode locations 29999.954846 810755799 probeB See electrode locations 29999.906318 810755801 probeC See electrode locations 29999.985470 810755803 probeD See electrode locations 29999.908100 810755805 probeE See electrode locations 29999.985679 810755807 probeF See electrode locations 30000.028033 lfp_sampling_rate has_lfp_data id 810755797 1249.998119 True 810755799 1249.996097 True 810755801 1249.999395 True 810755803 1249.996171 True 810755805 1249.999403 True 810755807 1250.001168 True Channels: filtering \\ id 850261194 AP band: 500 Hz high-pass; LFP band: 1000 Hz l... 850261196 AP band: 500 Hz high-pass; LFP band: 1000 Hz l... 850261202 AP band: 500 Hz high-pass; LFP band: 1000 Hz l... 850261206 AP band: 500 Hz high-pass; LFP band: 1000 Hz l... 850261212 AP band: 500 Hz high-pass; LFP band: 1000 Hz l... ... ... 850264894 AP band: 500 Hz high-pass; LFP band: 1000 Hz l... 850264898 AP band: 500 Hz high-pass; LFP band: 1000 Hz l... 850264902 AP band: 500 Hz high-pass; LFP band: 1000 Hz l... 850264908 AP band: 500 Hz high-pass; LFP band: 1000 Hz l... 850264912 AP band: 500 Hz high-pass; LFP band: 1000 Hz l... probe_channel_number probe_horizontal_position probe_id \\ id 850261194 0 43 810755801 850261196 1 11 810755801 850261202 4 43 810755801 850261206 6 59 810755801 850261212 9 11 810755801 ... ... ... ... 850264894 374 59 810755797 850264898 376 43 810755797 850264902 378 59 810755797 850264908 381 11 810755797 850264912 383 27 810755797 probe_vertical_position structure_acronym ecephys_structure_id \\ id 850261194 20 PO 1020.0 850261196 20 PO 1020.0 850261202 60 PO 1020.0 850261206 80 PO 1020.0 850261212 100 PO 1020.0 ... ... ... ... 850264894 3760 None NaN 850264898 3780 None NaN 850264902 3800 None NaN 850264908 3820 None NaN 850264912 3840 None NaN ecephys_structure_acronym anterior_posterior_ccf_coordinate \\ id 850261194 PO 7648.0 850261196 PO 7651.0 850261202 PO 7660.0 850261206 PO 7665.0 850261212 PO 7674.0 ... ... ... 850264894 NaN 7112.0 850264898 NaN 7107.0 850264902 NaN 7102.0 850264908 NaN 7094.0 850264912 NaN 7089.0 dorsal_ventral_ccf_coordinate left_right_ccf_coordinate id 850261194 3645.0 7567.0 850261196 3636.0 7566.0 850261202 3610.0 7564.0 850261206 3592.0 7562.0 850261212 3566.0 7560.0 ... ... ... 850264894 -53.0 7809.0 850264898 -69.0 7813.0 850264902 -85.0 7818.0 850264908 -109.0 7825.0 850264912 -126.0 7829.0 [2219 rows x 11 columns] Accessing Units of Interest (V1) Recall that we want to look at neuronal activity in V1. All unit information is stored within data.spikes , which is a Pynapple object called TsGroup. data.spikes holds spike times for each unit alongside metadata information about each unit, including in which brain structure they are recorded from. Let's see the units we have in our session: spikes = data . spikes print ( spikes . index ) [950910352 950910364 950910371 950910392 950910435 950910463 950910531 950910549 950910558 950910576 950910603 950910651 950910664 950910671 950910727 950910742 950910757 950910778 950910834 950910861 950910889 950910897 950910904 950910941 950911006 950911040 950911088 950911195 950911223 950911266 950911286 950911467 950911563 950911586 950911593 950911601 950911656 950911677 950911684 950911691 950911698 950911704 950911732 950911873 950911880 950911932 950911986 950912018 950912065 950912109 950912164 950912190 950912214 950912226 950912249 950912283 950912293 950912326 950912361 950912384 950912396 950912406 950912417 950912427 950912448 950912460 950912473 950912511 950912601 950912646 950912803 950912814 950912928 950912940 950912952 950913000 950913031 950913096 950913409 950913422 950913456 950913470 950913506 950913517 950913527 950913537 950913547 950913567 950913588 950913652 950913676 950913684 950913766 950913796 950913806 950913850 950913877 950913893 950913901 950913908 950913921 950913929 950913938 950913954 950913976 950913983 950913990 950914026 950914067 950914103 950914130 950914157 950914189 950914197 950914219 950914233 950914294 950914310 950914318 950914348 950914359 950914413 950914424 950914435 950914538 950914625 950914635 950914660 950914683 950914720 950914754 950914766 950914780 950914791 950914812 950914832 950914856 950914868 950914882 950914923 950914940 950914954 950914980 950915023 950915054 950915068 950915101 950915304 950915378 950915441 950915483 950915921 950915947 950915984 950915997 950916009 950916273 950916377 950916395 950916413 950916447 950916519 950916603 950916733 950916754 950916921 950916980 950917063 950917295 950917313 950917332 950917411 950917604 950917669 950917769 950917785 950917849 950917899 950918052 950918175 950918191 950918246 950918261 950918280 950918294 950918310 950918344 950918362 950918381 950918422 950918491 950918570 950918695 950918745 950918802 950918821 950918846 950918902 950918919 950918936 950918955 950919022 950919040 950919054 950919086 950919104 950919120 950919154 950919212 950919249 950919283 950919387 950919456 950919496 950919676 950919748 950919806 950919863 950919900 950919921 950919945 950919993 950920017 950920092 950920151 950920290 950920309 950920434 950920756 950920777 950920827 950920843 950920912 950920929 950920961 950920998 950921013 950921034 950921088 950921163 950921176 950921299 950921442 950921601 950921709 950922041 950922122 950922146 950922174 950922208 950922234 950922258 950922286 950922329 950922351 950922368 950922383 950922466 950922494 950922551 950922576 950922600 950922641 950922659 950922684 950922706 950922725 950922745 950922781 950922841 950922883 950922896 950922929 950922961 950922994 950923011 950923049 950923089 950923121 950923165 950923181 950923217 950923236 950923254 950923348 950923365 950923405 950923464 950923485 950923606 950923669 950923690 950923730 950923754 950923859 950923880 950923897 950923914 950923939 950923958 950923976 950924072 950924089 950924107 950924147 950924173 950924212 950924231 950924272 950924372 950924413 950924434 950924452 950924500 950924519 950924598 950924635 950924693 950924735 950924753 950924800 950924845 950924883 950924899 950925057 950925092 950925112 950925132 950925149 950925166 950925187 950925212 950925267 950925332 950925368 950925383 950925400 950925438 950925455 950925508 950925555 950925626 950925644 950925664 950925707 950925730 950925749 950925786 950925850 950925902 950925925 950925950 950925967 950925983 950926001 950926055 950926095 950926208 950926709 950926787 950926805 950926867 950926886 950926928 950927002 950927024 950927046 950927066 950927229 950927323 950927341 950927745 950927775 950928113 950928160 950928179 950928198 950928255 950928387 950928423 950928440 950928461 950928497 950928518 950928536 950928588 950928686 950928759 950928795 950928855 950928873 950928891 950928911 950928976 950928991 950929009 950929067 950929083 950929101 950929134 950929153 950929171 950929188 950929206 950929227 950929245 950929286 950929377 950929400 950929420 950929495 950929514 950929531 950929570 950929592 950929663 950929697 950929750 950929787 950929804 950929824 950929882 950930105 950930145 950930215 950930237 950930276 950930340 950930358 950930375 950930392 950930407 950930423 950930437 950930454 950930522 950930795 950930866 950930888 950930964 950930985 950931004 950931043 950931118 950931164 950931181 950931236 950931254 950931272 950931315 950931363 950931423 950931458 950931517 950931533 950931565 950931581 950931617 950931656 950931727 950931751 950931770 950931805 950931853 950931878 950931899 950931959 950932032 950932087 950932102 950932445 950932563 950932578 950932696 950932820 950932888 950932943 950932963 950932980 950933012 950933040 950933057 950933173 950933208 950933226 950933426 950933536 950933555 950933606 950933660 950933698 950933732 950933840 950933890 950933907 950933924 950933960 950934028 950934044 950934181 950934229 950934268 950934286 950934304 950934728 950934765 950934843 950934971 950934992 950935070 950935165 950935201 950935269 950935285 950935317 950935402 950935460 950935478 950935499 950935575 950935610 950935658 950935720 950935755 950935907 950935925 950935942 950935975 950936008 950936025 950936093 950936113 950936162 950936177 950936194 950936224 950936272 950936292 950936326 950936345 950936412 950936465 950936482 950936572 950936639 950936656 950936675 950936710 950936727 950936759 950936855 950936870 950936908 950936941 950936979 950936992 950937050 950937068 950937114 950937333 950937929 950937990 950938074 950938091 950938171 950938279 950938344 950938459 950938511 950938598 950938629 950938657 950938700 950938797 950938924 950938960 950938978 950939028 950939079 950939114 950939168 950939257 950939347 950939479 950939509 950939523 950939641 950939678 950939945 950940001 950940023 950940040 950940104 950940121 950940157 950940171 950940185 950940200 950940219 950940237 950940311 950940389 950940437 950940453 950940467 950940507 950940526 950940545 950940615 950940631 950940649 950940671 950940688 950940718 950940859 950940928 950941494 950941529 950941583 950941776 950941795 950942129 950942155 950942199 950942235 950942252 950942304 950942974 950943198 950943517 950943580 950943625 950943695 950943735 950943756 950943795 950943813 950943830 950943877 950944146 950944160 950944212 950944228 950944247 950944259 950944276 950944444 950944566 950944600 950944675 950944706 950944769 950944784 950944815 950944827 950944858 950944872 950944952 950944968 950944989 950945008 950945042 950945060 950945077 950945127 950945180 950945232 950945252 950945295 950945314 950945467 950945518 950945554 950945572 950945588 950945625 950945660 950945768 950945783 950945817 950945986 950946003 950946022 950946068 950946155 950946176 950946192 950946228 950946310 950946327 950946343 950946376 950946437 950946497 950946641 950946658 950946673 950946741 950947140 950947156 950947211 950947224 950947271 950947368 950947412 950947488 950947533 950947555 950947626 950947647 950947665 950947763 950947778 950947798 950947832 950947849 950947868 950947941 950947963 950947990 950948009 950948026 950948063 950948092 950948139 950948174 950948210 950948246 950948281 950948306 950948371 950948396 950948414 950948488 950948527 950948547 950948564 950948631 950948648 950948683 950948716 950948744 950948778 950948793 950948826 950948853 950948867 950948881 950948970 950949089 950949118 950949200 950949248 950949428 950949449 950949628 950949861 950949933 950949961 950949994 950950009 950950031 950950101 950950116 950950160 950950176 950950224 950950270 950950308 950950382 950950433 950950576 950950740 950950756 950950810 950950880 950950919 950951153 950951364 950951511 950951525 950951577 950951613 950951636 950951678 950951702 950951791 950951829 950951840 950951851 950951879 950951891 950951902 950951940 950951950 950951979 950952002 950952213 950952241 950952256 950952312 950952336 950952386 950952629 950952690 950952704 950952719 950952734 950952748 950952845 950952881 950952969 950952985 950953119 950953159 950953187 950953293 950953417 950953703 950954163 950954180 950954195 950954630 950954647 950954737 950954793 950954823 950954838 950954886 950954905 950954922 950954941 950954983 950955053 950955181 950955212 950955361 950955399 950955414 950955543 950955784 950955833 950955844 950955897 950955951 950955965 950955974 950956019 950956030 950956043 950956100 950956148 950956205 950956259 950956297 950956336 950956386 950956399 950956413 950956435 950956493 950956504 950956514 950956527 950956541 950956563 950956592 950956604 950956616 950956630 950956667 950956680 950956778 950956835 950956845 950956870 950956911 950956952 950957053 950957270 950957282 950957295 950957320 950957408] To see what metadata is stored within spikes : print ( spikes . metadata_columns ) ['rate', 'waveform_PT_ratio', 'waveform_amplitude', 'amplitude_cutoff', 'cluster_id', 'cumulative_drift', 'd_prime', 'firing_rate', 'isi_violations', 'isolation_distance', 'L_ratio', 'local_index', 'max_drift', 'nn_hit_rate', 'nn_miss_rate', 'peak_channel_id', 'presence_ratio', 'waveform_recovery_slope', 'waveform_repolarization_slope', 'silhouette_score', 'snr', 'waveform_spread', 'waveform_velocity_above', 'waveform_velocity_below', 'waveform_duration', 'filtering', 'probe_channel_number', 'probe_horizontal_position', 'probe_id', 'probe_vertical_position', 'structure_acronym', 'ecephys_structure_id', 'ecephys_structure_acronym', 'anterior_posterior_ccf_coordinate', 'dorsal_ventral_ccf_coordinate', 'left_right_ccf_coordinate', 'probe_description', 'location', 'probe_sampling_rate', 'probe_lfp_sampling_rate', 'probe_has_lfp_data'] Now, we want to get the units that are in the primary visual cortex (V1), as these are neurons that are most likely to have orientation tuning and tuning to locations in space. The metadata column that corresponds to location in the brain is \"ecephys_structure_acronym\". We can use TsGroup 's functionality to organize the units based on columns in the unit's metadata. This function is called getby_category() . Let's see how we can use it for our purposes: # organize units by structure units_structures = spikes . getby_category ( \"ecephys_structure_acronym\" ) units_structures is a dictionary of TsGroups , with the structure names as keys. Let's look at the names of the structures we are working on by getting the keys of the dictionary. # take a look at the structure names units_structures . keys () dict_keys(['APN', 'CA1', 'CA3', 'DG', 'LGd', 'LP', 'PO', 'PoT', 'VISam', 'VISl', 'VISp', 'VISpm', 'VISrl', 'grey']) To get an overview of how the units are distributed by brain structure, let's quickly count the number of units per brain area and plot it: # getting a list of brain areas brain_areas = units_structures . keys () # counting the length of the TsGroup (i.e. number of units) associated with each brain structure unit_count_by_brain_region = [ len ( units_structures [ key ]) for key in brain_areas ] # plot plt . bar ( brain_areas , unit_count_by_brain_region ) <BarContainer object of 14 artists> Nice! Looks like we have a decent number of units in the primary visual cortex (\"VISp\"). Let's retrieve these units from the dictionary units_structures . # get the TsGroup of units in V1 v1_units = units_structures [ \"VISp\" ] # how many units did we get? print ( \"Number of units in V1: %s \" % len ( v1_units )) Number of units in V1: 60 Now, from the units in V1, we also want to get the units that have a high signal-to-noise (SNR) ratio. If you look above, you can see that this information is also stored in the metadata under \"snr\". We can use the TsGroup 's getby_threshold() function to do this. Rather than a dictionary of TsGroups, this function just returns a new TsGroup with units that obey the given threshold. Let's get units with an SNR > 4: v1_high_snr_units = v1_units . getby_threshold ( \"snr\" , 4 ) # default operator is >, but other ones can be passed via an optional argument # how many units did we get? print ( \"Number of units in V1: %s \" % len ( v1_high_snr_units )) # what are the IDs of the units? v1_unit_IDs = v1_high_snr_units . keys () print ( \"V1 Unit IDs: %s \" % v1_unit_IDs ) Number of units in V1: 6 V1 Unit IDs: [950930985, 950931458, 950931533, 950931727, 950931751, 950932696] Awesome! We can quickly visualize the units' activity over time with TsGroup 's count() function, which counts the number of spikes of each unit within intervals of time. The default intervals of the count() function are based on the global time support of the data. In this database, the global session epoch is the start and end time of the entire session. Let's first use the default interval and specify the bin size in seconds. Here, I'm setting the bin size to 1s. spike_counts = v1_high_snr_units . count ( bin_size = 1 ) spike_counts .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 950930985 950931458 950931533 950931727 950931751 950932696 Time (s) 0.5 0 0 0 0 0 0 1.5 0 0 0 0 0 0 2.5 0 0 0 0 0 0 3.5 0 0 0 0 0 0 4.5 0 0 0 0 0 0 ... ... ... ... ... ... ... 9636.5 0 0 0 0 0 0 9637.5 0 0 0 0 0 0 9638.5 0 0 0 0 0 0 9639.5 0 0 0 0 0 0 9640.5 0 0 0 0 0 0 9641 rows \u00d7 6 columns Nice! With this table, we have everything we need to plot spike counts per time interval. Let's go ahead and plot it for each of our units. This will take a few minutes since the bin number is so high. # Create 1x6 grid of subplots fig , axs = plt . subplots ( 1 , 6 , figsize = ( 15 , 4 )) time_intervals = spike_counts . index # Iterate over each neuron and plot the spike count data for i , ( unit_id , ax ) in enumerate ( zip ( v1_unit_IDs , axs )): ax . bar ( time_intervals , spike_counts [ unit_id ]) ax . set_xlabel ( 'Time (s)' ) ax . set_ylabel ( 'Spike Count' ) ax . set_title ( unit_id ) plt . tight_layout () plt . show () Understanding the Stimuli Now that we've retrieved our units in V1, let's take a look at the kind of stimuli we are working with and how stimulus epochs are organized. First, let's look at stimulus epochs by type and by block. stimulus_epochs_types = data . stimulus_epochs_types stimulus_epochs_blocks = data . stimulus_epochs_blocks print ( \"Stimulus names: %s \" % stimulus_epochs_types . keys ()) print ( \"Stimulus blocks: %s \" % stimulus_epochs_blocks . keys ()) Stimulus names: dict_keys(['drifting_gratings', 'flashes', 'gabors', 'natural_movie_one', 'natural_movie_three', 'natural_scenes', 'spontaneous', 'static_gratings']) Stimulus blocks: dict_keys([0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 'null']) Each block interval provides the beginning and end of the given block of stimulus presentations, stored is stimulus_epochs_blocks Each stimulus blocks contains only one type of stimulus, however, there are multiple blocks of one kind of stimulus. Each IntervalSet in stimulus_epochs_types represents one block of stimulus. Individual stimulus interval of a given stimulus type is stored in stimulus_intervals . To see a visualization of how the stimuli are organized, see here: chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://brainmapportal-live-4cc80a57cd6e400d854-f7fdcae.divio-media.net/filer_public/0f/5d/0f5d22c9-f8f6-428c-9f7a-2983631e72b4/neuropixels_cheat_sheet_nov_2019.pdf For example, from Allen Neuropixels' documentation, we know that drifting gratings are shown in multiple blocks, specifically block 2, 5, and 7. Within these blocks, there are individual stimulus presentations of various conditions. Let's take a look: # Drifting gratings epochs: drifting_gratings = stimulus_epochs_types [ \"drifting_gratings\" ] print ( \"Drifting gratings epochs: \\n %s \\n \" % drifting_gratings ) # Block 2, 5 and 7 epochs print ( \"Block 2 intervals \\n %s : \" % stimulus_epochs_blocks [ 2 ]) print ( \"Block 5 intervals \\n %s : \" % stimulus_epochs_blocks [ 5 ]) print ( \"Block 7 intervals \\n %s : \" % stimulus_epochs_blocks [ 7 ]) Drifting gratings epochs: start end 0 1574.774823 2174.275707 1 3166.137683 3765.638457 2 4697.416823 5380.987797 Block 2 intervals start end 0 1574.774823 2174.275707: Block 5 intervals start end 0 3166.137683 3765.638457: Block 7 intervals start end 0 4697.416823 5380.987797: Notice how the epochs defined by stimulus type and block line up? type and block are two ways of organizing the stimulus epochs, which one you use depends on your analysis goals. Now, let's take a look at the stimulus presentation intervals of all drifting gratings: # get dictionary of stimulus intervals organized by stimulus type stimulus_intervals = data . stimulus_intervals # get stimulus intervals for drifting gratings drifting_gratings_intervals = stimulus_intervals [ 'drifting_gratings' ] drifting_gratings_intervals .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } start end 0 1574.774823 1576.776513 1 1577.777347 1579.779027 2 1580.779833 1582.781563 3 1583.782367 1585.784047 4 1586.784883 1588.786553 ... ... ... 5 5366.976107 5368.977777 6 5369.978603 5371.980283 7 5372.981107 5374.982807 8 5375.983663 5377.985343 9 5378.986127 5380.987797 628 rows \u00d7 2 columns Notice that the first start time and the last end time matches that of the start and end time of the drifting gratings stimulus epochs. To get the stimulus interval for only one of the blocks, you can use IntervalSet 's intersect function, which finds the common times between two IntervalSet s. Let's find the stimulus intervals of drifting gratings in block 2 as an example: block2_stimulus_epochs = stimulus_epochs_blocks [ 2 ] # get block 2 of stimulus epochs # intersect block 2 intervals with drifting gratings intervals to get drifting gratings in block 2 only drifting_gratings_block_2_intervals = drifting_gratings_intervals . intersect ( block2_stimulus_epochs ) # what does this look like? drifting_gratings_block_2_intervals .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } start end 0 1574.774823 1576.776513 1 1577.777347 1579.779027 2 1580.779833 1582.781563 3 1583.782367 1585.784047 4 1586.784883 1588.786553 ... ... ... 5 2160.263967 2162.265657 6 2163.266503 2165.268183 7 2166.269007 2168.270667 8 2169.271543 2171.273213 9 2172.274017 2174.275707 200 rows \u00d7 2 columns To look at the overall set of available parameters of the stimuli, we can access it through the metadata as follows: for key , values in stimulus_parameters . items (): print ( f ' { key } : { values } ' ) color:[-1.0 1.0] contrast:[0.8 1.0] frame:[0.0 1.0 2.0 ... 3598.0 3599.0 -1.0] orientation:[45.0 0.0 90.0 315.0 225.0 135.0 270.0 180.0 120.0 150.0 60.0 30.0] phase:['[3644.93333333, 3644.93333333]' '[0.0, 0.0]' '[21211.93333333, 21211.93333333]' '0.5' '0.75' '0.0' '0.25'] size:['[20.0, 20.0]' '[300.0, 300.0]' '[250.0, 250.0]' '[1920.0, 1080.0]'] spatial_frequency:['0.08' '[0.0, 0.0]' '0.04' 0.32 0.08 0.04 0.02 0.16] temporal_frequency:[4.0 8.0 2.0 1.0 15.0] x_position:[0.0 -30.0 10.0 -40.0 -10.0 40.0 30.0 20.0 -20.0] y_position:[30.0 -10.0 20.0 -40.0 -20.0 0.0 -30.0 40.0 10.0] Great, now that we have a good understanding of the stimuli in this session, we can start examining the activity of the V1 units of interest in relation to the stimuli. Analyzing the Neural Response in V1 to Visual Stimuli Now that we've acquired units from V1 and have explored our data and stimuli, we now do some analysis. Specifically, we're going to look at the response of V1 neurons to visual stimuli. 1D Orientation Tuning Curves This section of the tutorial is adopted from Seigle et al (2021) Dataset Tutorial by Dhruv Mehrotra : https://github.com/PeyracheLab/pynacollada/blob/main/pynacollada/Pynapple%20Paper%20Figures/Siegle%202021/Siegle_dataset.ipynb V1 neurons are known to have orientation specific tuning. In this dataset, we have static grating stimuli which varies in orientation. We are going to compute the tuning curve of V1 neurons to the orientations. To do this, first we need to get information about the static grating stimulus presentations, as follows: drifting_gratings_presentations = stimulus_presentations [ stimulus_presentations [ 'stimulus_name' ] == 'drifting_gratings' ] drifting_gratings_presentations .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } color contrast frame orientation phase size spatial_frequency start_time stimulus_block stimulus_name stop_time temporal_frequency x_position y_position duration stimulus_condition_id stimulus_presentation_id 3798 null 0.8 null 315.0 [21211.93333333, 21211.93333333] [250.0, 250.0] 0.04 1574.774823 2.0 drifting_gratings 1576.776513 4.0 null null 2.00169 246 3799 null 0.8 null 90.0 [21211.93333333, 21211.93333333] [250.0, 250.0] 0.04 1577.777347 2.0 drifting_gratings 1579.779027 8.0 null null 2.00168 247 3800 null 0.8 null 225.0 [21211.93333333, 21211.93333333] [250.0, 250.0] 0.04 1580.779833 2.0 drifting_gratings 1582.781563 2.0 null null 2.00173 248 3801 null 0.8 null 90.0 [21211.93333333, 21211.93333333] [250.0, 250.0] 0.04 1583.782367 2.0 drifting_gratings 1585.784047 2.0 null null 2.00168 249 3802 null 0.8 null 135.0 [21211.93333333, 21211.93333333] [250.0, 250.0] 0.04 1586.784883 2.0 drifting_gratings 1588.786553 8.0 null null 2.00167 250 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 49426 null 0.8 null 135.0 [21211.93333333, 21211.93333333] [250.0, 250.0] 0.04 5366.976107 7.0 drifting_gratings 5368.977777 15.0 null null 2.00167 274 49427 null 0.8 null 180.0 [21211.93333333, 21211.93333333] [250.0, 250.0] 0.04 5369.978603 7.0 drifting_gratings 5371.980283 8.0 null null 2.00168 260 49428 null 0.8 null 0.0 [21211.93333333, 21211.93333333] [250.0, 250.0] 0.04 5372.981107 7.0 drifting_gratings 5374.982807 1.0 null null 2.00170 251 49429 null 0.8 null 180.0 [21211.93333333, 21211.93333333] [250.0, 250.0] 0.04 5375.983663 7.0 drifting_gratings 5377.985343 4.0 null null 2.00168 282 49430 null 0.8 null 270.0 [21211.93333333, 21211.93333333] [250.0, 250.0] 0.04 5378.986127 7.0 drifting_gratings 5380.987797 4.0 null null 2.00167 279 628 rows \u00d7 16 columns Let's see what orientations we have: orientations = drifting_gratings_presentations [ 'orientation' ] . values # get oreintations of the static gratings orientations array([315.0, 90.0, 225.0, 90.0, 135.0, 0.0, 315.0, 315.0, 270.0, 90.0, 0.0, 315.0, 270.0, 'null', 'null', 315.0, 315.0, 315.0, 180.0, 'null', 45.0, 315.0, 90.0, 270.0, 135.0, 90.0, 135.0, 135.0, 135.0, 45.0, 315.0, 0.0, 180.0, 0.0, 315.0, 315.0, 45.0, 90.0, 180.0, 'null', 135.0, 225.0, 0.0, 135.0, 45.0, 315.0, 225.0, 0.0, 45.0, 270.0, 135.0, 180.0, 180.0, 180.0, 90.0, 0.0, 0.0, 90.0, 225.0, 90.0, 'null', 45.0, 'null', 270.0, 45.0, 180.0, 225.0, 225.0, 45.0, 90.0, 315.0, 270.0, 270.0, 270.0, 0.0, 45.0, 270.0, 270.0, 225.0, 270.0, 180.0, 180.0, 315.0, 45.0, 90.0, 270.0, 315.0, 135.0, 315.0, 45.0, 135.0, 'null', 225.0, 180.0, 90.0, 225.0, 180.0, 90.0, 315.0, 270.0, 135.0, 270.0, 180.0, 315.0, 0.0, 315.0, 225.0, 45.0, 90.0, 0.0, 135.0, 0.0, 45.0, 90.0, 90.0, 135.0, 270.0, 270.0, 45.0, 225.0, 45.0, 180.0, 180.0, 90.0, 90.0, 90.0, 270.0, 0.0, 315.0, 225.0, 180.0, 270.0, 45.0, 315.0, 45.0, 135.0, 45.0, 45.0, 45.0, 270.0, 'null', 180.0, 180.0, 90.0, 'null', 315.0, 180.0, 0.0, 'null', 135.0, 315.0, 225.0, 45.0, 315.0, 270.0, 135.0, 'null', 135.0, 270.0, 0.0, 225.0, 0.0, 135.0, 225.0, 225.0, 135.0, 45.0, 90.0, 225.0, 45.0, 45.0, 0.0, 180.0, 180.0, 135.0, 315.0, 180.0, 315.0, 0.0, 315.0, 180.0, 'null', 135.0, 270.0, 225.0, 90.0, 270.0, 135.0, 270.0, 180.0, 225.0, 0.0, 0.0, 0.0, 270.0, 180.0, 270.0, 'null', 'null', 45.0, 0.0, 'null', 135.0, 'null', 315.0, 135.0, 0.0, 90.0, 225.0, 45.0, 315.0, 225.0, 0.0, 180.0, 0.0, 225.0, 0.0, 270.0, 45.0, 225.0, 180.0, 315.0, 90.0, 90.0, 90.0, 270.0, 180.0, 45.0, 0.0, 270.0, 45.0, 90.0, 45.0, 270.0, 0.0, 90.0, 0.0, 90.0, 90.0, 225.0, 180.0, 180.0, 135.0, 135.0, 270.0, 180.0, 135.0, 135.0, 45.0, 315.0, 225.0, 315.0, 90.0, 0.0, 135.0, 'null', 90.0, 135.0, 135.0, 225.0, 45.0, 0.0, 0.0, 135.0, 315.0, 135.0, 225.0, 315.0, 225.0, 270.0, 135.0, 135.0, 225.0, 135.0, 315.0, 'null', 180.0, 315.0, 225.0, 0.0, 225.0, 180.0, 270.0, 45.0, 135.0, 45.0, 270.0, 135.0, 270.0, 90.0, 270.0, 45.0, 45.0, 45.0, 135.0, 'null', 90.0, 45.0, 225.0, 45.0, 225.0, 45.0, 45.0, 0.0, 'null', 45.0, 270.0, 270.0, 45.0, 225.0, 90.0, 135.0, 45.0, 90.0, 180.0, 90.0, 315.0, 0.0, 0.0, 225.0, 315.0, 180.0, 45.0, 90.0, 225.0, 45.0, 180.0, 'null', 135.0, 0.0, 0.0, 135.0, 315.0, 135.0, 180.0, 270.0, 270.0, 315.0, 180.0, 315.0, 90.0, 45.0, 0.0, 315.0, 90.0, 45.0, 135.0, 180.0, 135.0, 135.0, 180.0, 0.0, 270.0, 225.0, 270.0, 45.0, 180.0, 0.0, 225.0, 45.0, 90.0, 270.0, 45.0, 225.0, 270.0, 180.0, 315.0, 225.0, 0.0, 225.0, 90.0, 315.0, 270.0, 45.0, 0.0, 45.0, 90.0, 180.0, 0.0, 90.0, 90.0, 180.0, 45.0, 45.0, 90.0, 135.0, 270.0, 135.0, 225.0, 'null', 315.0, 270.0, 135.0, 315.0, 270.0, 0.0, 270.0, 180.0, 0.0, 270.0, 315.0, 225.0, 180.0, 180.0, 90.0, 315.0, 0.0, 0.0, 45.0, 270.0, 315.0, 180.0, 270.0, 270.0, 0.0, 270.0, 135.0, 270.0, 45.0, 90.0, 'null', 315.0, 90.0, 180.0, 90.0, 225.0, 315.0, 0.0, 315.0, 225.0, 180.0, 0.0, 180.0, 180.0, 180.0, 225.0, 90.0, 45.0, 225.0, 0.0, 0.0, 270.0, 225.0, 0.0, 90.0, 'null', 135.0, 225.0, 315.0, 180.0, 135.0, 90.0, 0.0, 135.0, 180.0, 270.0, 90.0, 0.0, 225.0, 180.0, 180.0, 90.0, 90.0, 90.0, 180.0, 315.0, 180.0, 0.0, 270.0, 135.0, 45.0, 315.0, 'null', 225.0, 90.0, 180.0, 315.0, 'null', 45.0, 90.0, 0.0, 90.0, 225.0, 90.0, 180.0, 0.0, 45.0, 180.0, 270.0, 315.0, 315.0, 225.0, 90.0, 225.0, 90.0, 315.0, 315.0, 0.0, 270.0, 90.0, 225.0, 225.0, 135.0, 45.0, 90.0, 180.0, 45.0, 135.0, 270.0, 225.0, 135.0, 0.0, 0.0, 45.0, 90.0, 315.0, 90.0, 45.0, 45.0, 315.0, 45.0, 90.0, 0.0, 45.0, 180.0, 225.0, 0.0, 315.0, 180.0, 90.0, 225.0, 225.0, 270.0, 45.0, 0.0, 90.0, 0.0, 315.0, 225.0, 270.0, 180.0, 45.0, 135.0, 225.0, 225.0, 225.0, 135.0, 225.0, 270.0, 90.0, 'null', 270.0, 315.0, 135.0, 225.0, 90.0, 180.0, 225.0, 135.0, 270.0, 315.0, 45.0, 180.0, 0.0, 0.0, 225.0, 135.0, 315.0, 135.0, 135.0, 90.0, 225.0, 0.0, 315.0, 270.0, 180.0, 'null', 315.0, 135.0, 45.0, 0.0, 135.0, 45.0, 180.0, 'null', 270.0, 180.0, 270.0, 135.0, 225.0, 225.0, 'null', 270.0, 135.0, 0.0, 135.0, 270.0, 180.0, 315.0, 270.0, 315.0, 225.0, 135.0, 315.0, 315.0, 0.0, 90.0, 180.0, 270.0, 225.0, 135.0, 90.0, 135.0, 45.0, 180.0, 270.0, 315.0, 0.0, 135.0, 315.0, 225.0, 45.0, 225.0, 135.0, 180.0, 0.0, 180.0, 270.0], dtype=object) We can see that there are some null values. Let's convert these values to floats for better handling. orientations [ orientations == 'null' ] = np . nan # replace all null values to NaN orientations = orientations . astype ( float ) # convert to float array angle_range = np . unique ( orientations )[ 0 : - 1 ] # find all unique values excluding NaNs angle_range array([ 0., 45., 90., 135., 180., 225., 270., 315.]) These are the 8 orientations of the driftig gratings, sampling 360 degrees at 45 degree intervals. Now we need to access information from stimulus_presentations metadata and load it as a Pynapple object. Specifically, we need to create a dictionary of IntervalSets for each orientation. First, we need stimulus intervals for drifting gratings to organize by orientation. drifting_gratings_intervals = stimulus_intervals [ 'drifting_gratings' ] drifting_gratings_intervals .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } start end 0 1574.774823 1576.776513 1 1577.777347 1579.779027 2 1580.779833 1582.781563 3 1583.782367 1585.784047 4 1586.784883 1588.786553 ... ... ... 5 5366.976107 5368.977777 6 5369.978603 5371.980283 7 5372.981107 5374.982807 8 5375.983663 5377.985343 9 5378.986127 5380.987797 628 rows \u00d7 2 columns Great. Now let's proceed to sorting these intervals into IntervalSet s by their orientation dict_ori = {} for angle in angle_range : tokeep = [] # list of trials to keep for i in range ( len ( drifting_gratings_presentations )): # loop over all gabors trials if float ( orientations [ i ] == angle ): # find trials with given orientation tokeep . append ( i ) dict_ori [ angle ] = drifting_gratings_intervals . loc [ tokeep ] # make dictionary of IntervalSets dict_ori {0.0: start end 0 1589.787377 1591.789047 1 1604.799993 1606.801573 2 1667.852587 1669.854267 3 1673.857607 1675.859277 4 1700.880223 1702.881873 .. ... ... 69 5237.868283 5239.869913 70 5279.903393 5281.905073 71 5312.930947 5314.932627 72 5348.961047 5350.962727 73 5372.981107 5374.982807 [74 rows x 2 columns], 45.0: start end 0 1634.824993 1636.826683 1 1661.847597 1663.849267 2 1682.865123 1684.866833 3 1706.885213 1708.886883 4 1718.895253 1720.896963 .. ... ... 70 5180.820607 5182.822277 71 5234.865757 5236.867417 72 5243.873283 5245.874953 73 5336.951027 5338.952697 74 5360.971097 5362.972757 [75 rows x 2 columns], 90.0: start end 0 1577.777347 1579.779027 1 1583.782367 1585.784047 2 1601.797417 1603.799097 3 1640.830033 1642.831703 4 1649.837547 1651.839217 .. ... ... 70 5144.790507 5146.792167 71 5162.805557 5164.807227 72 5207.843153 5209.844843 73 5315.933463 5317.935153 74 5330.945997 5332.947677 [75 rows x 2 columns], 135.0: start end 0 1586.784883 1588.786553 1 1646.834993 1648.836703 2 1652.840073 1654.841743 3 1655.842567 1657.844227 4 1658.845063 1660.846763 .. ... ... 69 5303.923423 5305.925113 70 5327.943483 5329.945163 71 5333.948513 5335.950203 72 5351.963563 5353.965203 73 5366.976107 5368.977777 [74 rows x 2 columns], 180.0: start end 0 1628.820023 1630.821703 1 1670.855103 1672.856783 2 1688.870163 1690.871853 3 1727.902757 1729.904427 4 1730.905303 1732.906953 .. ... ... 70 5288.910887 5290.912557 71 5318.935957 5320.937647 72 5339.953523 5341.955183 73 5369.978603 5371.980283 74 5375.983663 5377.985343 [75 rows x 2 columns], 225.0: start end 0 1580.779833 1582.781563 1 1697.877687 1699.879357 2 1712.890213 1714.891913 3 1748.920323 1750.922013 4 1772.940363 1774.942053 .. ... ... 70 5267.893343 5269.895043 71 5300.920927 5302.922597 72 5324.940987 5326.942657 73 5357.968573 5359.970233 74 5363.973593 5365.975283 [75 rows x 2 columns], 270.0: start end 0 1598.794933 1600.796563 1 1610.804943 1612.806633 2 1643.832527 1645.834217 3 1721.897747 1723.899427 4 1763.932867 1765.934547 .. ... ... 70 5285.908373 5287.910063 71 5294.915917 5296.917577 72 5321.938523 5323.940153 73 5342.956047 5344.957707 74 5378.986127 5380.987797 [75 rows x 2 columns], 315.0: start end 0 1574.774823 1576.776513 1 1592.789903 1594.791563 2 1595.792407 1597.794077 3 1607.802437 1609.804107 4 1619.812457 1621.814137 .. ... ... 70 5297.918423 5299.920093 71 5306.925927 5308.927607 72 5309.928463 5311.930133 73 5345.958553 5347.960203 74 5354.966067 5356.967737 [75 rows x 2 columns]} This dictionary has the drifting_gratings_intervals sorted by orientation. We can use these to compute orientation tuning curves. Since the stimuli are presented in discrete orientations (i.e. not spanning all angular values from 0 to 360 degrees), we will be using Pynapple's compute_discrete_tuning_curves . We can calculate tuning curves with one line with Pynapple! # Plot firing rate of V1 units as a function of orientation, i.e. an orientation tuning curve discrete_tuning_curves = nap . compute_discrete_tuning_curves ( v1_high_snr_units , dict_ori ) discrete_tuning_curves .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 950930985 950931458 950931533 950931727 950931751 950932696 0.0 29.995305 10.423757 1.829558 23.304928 2.848980 0.067511 45.0 44.840722 15.482074 3.244307 23.389656 3.430838 0.133236 90.0 33.685112 16.839225 1.825138 14.194774 3.084083 0.006661 135.0 17.431332 16.000099 4.793279 19.017840 3.659094 0.567092 180.0 33.585341 13.109074 1.971690 27.217315 4.336386 0.686095 225.0 45.734898 14.614385 4.782647 22.894093 3.843437 0.293087 270.0 38.394459 14.281353 1.731881 13.681856 2.804314 0.079933 315.0 19.910011 14.840918 4.216473 20.476204 2.930882 0.526226 Each column is a unit, and each row is the orientation of the stimulus in degrees. The values in the table represent the firing rate of the unit in Hz. Let's plot them! plt . figure ( figsize = ( 12 , 9 )) for i in range ( len ( v1_unit_IDs )): # loop over all unit IDs plt . subplot ( 2 , 3 , i + 1 ) # plot tuning curves in 2 rows and 3 columns plt . plot ( discrete_tuning_curves [ v1_unit_IDs [ i ]], 'o-' , color = 'k' , linewidth = 2 ) plt . xlabel ( 'Orientation (deg)' ) plt . ylabel ( 'Firing rate (Hz)' ) plt . title ( \"Unit %d \" % v1_unit_IDs [ i ], fontsize = 10 ) plt . subplots_adjust ( wspace = 0.5 , hspace = 1 , top = 0.4 ) Nice! Looks like some of these neurons have some degree of orientation tuning. 2D Spatial Tuning Curves Neurons in V1 are known to have spatial tuning based on the location of the stimulus in space. The gabors stimuli are shown on various locations on the screen. The x-y location of stimuli can be found in the stimulus_presentations metadata. Here, we will go through how we can access this metadata and use it to calculate 2D spatial tuning curves. Let's first get a table of stimulus presentations for gabors to look at the metadata for this stimulus. # get gabors informatiom from stimulus_presentations metadata gabors_presentations = stimulus_presentations [ stimulus_presentations [ 'stimulus_name' ] == 'gabors' ] gabors_presentations .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } color contrast frame orientation phase size spatial_frequency start_time stimulus_block stimulus_name stop_time temporal_frequency x_position y_position duration stimulus_condition_id stimulus_presentation_id 1 null 0.8 null 45.0 [3644.93333333, 3644.93333333] [20.0, 20.0] 0.08 73.537433 0.0 gabors 73.770952 4.0 0.0 30.0 0.233519 1 2 null 0.8 null 0.0 [3644.93333333, 3644.93333333] [20.0, 20.0] 0.08 73.770952 0.0 gabors 74.021150 4.0 -30.0 -10.0 0.250199 2 3 null 0.8 null 45.0 [3644.93333333, 3644.93333333] [20.0, 20.0] 0.08 74.021150 0.0 gabors 74.271349 4.0 10.0 20.0 0.250199 3 4 null 0.8 null 0.0 [3644.93333333, 3644.93333333] [20.0, 20.0] 0.08 74.271349 0.0 gabors 74.521547 4.0 -40.0 -40.0 0.250199 4 5 null 0.8 null 90.0 [3644.93333333, 3644.93333333] [20.0, 20.0] 0.08 74.521547 0.0 gabors 74.771764 4.0 -10.0 -10.0 0.250216 5 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 3641 null 0.8 null 45.0 [3644.93333333, 3644.93333333] [20.0, 20.0] 0.08 984.281513 0.0 gabors 984.531719 4.0 30.0 -10.0 0.250206 192 3642 null 0.8 null 45.0 [3644.93333333, 3644.93333333] [20.0, 20.0] 0.08 984.531719 0.0 gabors 984.781925 4.0 -20.0 10.0 0.250206 39 3643 null 0.8 null 0.0 [3644.93333333, 3644.93333333] [20.0, 20.0] 0.08 984.781925 0.0 gabors 985.032131 4.0 -30.0 40.0 0.250206 186 3644 null 0.8 null 45.0 [3644.93333333, 3644.93333333] [20.0, 20.0] 0.08 985.032131 0.0 gabors 985.282337 4.0 10.0 -30.0 0.250206 232 3645 null 0.8 null 45.0 [3644.93333333, 3644.93333333] [20.0, 20.0] 0.08 985.282337 0.0 gabors 985.532551 4.0 -40.0 40.0 0.250214 153 3645 rows \u00d7 16 columns Now, we need to extract the x- and y-position from the metadata and load it into a TsdFrame . # extract x- and y-position columns x_positions = np . array ( gabors_presentations [ 'x_position' ]) . tolist () y_positions = np . array ( gabors_presentations [ 'y_position' ]) . tolist () # load into x and y positions into a vstack xy_positions = np . vstack (( x_positions , y_positions )) . T # what does this look like? xy_positions array([[ 0., 30.], [-30., -10.], [ 10., 20.], ..., [-30., 40.], [ 10., -30.], [-40., 40.]]) Now let's load the xy positions into a TsdFrame . Since we will use the absolute start times as the time index (see below), we don't need to provide additional time support. When no time support is passed, the default global time support is used. # get start time as start time of the stimulus presentations time_index = ( gabors_presentations [ 'start_time' ]) . values # load TsdFrame xy_features = nap . TsdFrame ( t = time_index , d = xy_positions , time_units = \"s\" , columns = [ 'x' , 'y' ]) # what does this look like? xy_features .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x y Time (s) 73.537433 0.0 30.0 73.770952 -30.0 -10.0 74.021150 10.0 20.0 74.271349 -40.0 -40.0 74.521547 -10.0 -10.0 ... ... ... 984.281513 30.0 -10.0 984.531719 -20.0 10.0 984.781925 -30.0 40.0 985.032131 10.0 -30.0 985.282337 -40.0 40.0 3645 rows \u00d7 2 columns Great! Almost there. To compute the 2D tuning curve, it just takes one line with Pynapple! # using bin size of 9 since x-y stimulus is shown in a 9x9 grid (-40,-30,-20,-10,0,10,20,30,40) spatial_tuning_curve , binsxy = nap . compute_2d_tuning_curves ( group = v1_high_snr_units , feature = xy_features , nb_bins = 9 ) spatial_tuning_curve {950930985: array([[39.71176567, 44.15379762, 44.7756821 , 42.82118804, 39.00104056, 41.13321589, 43.70959443, 40.60017206, 37.40190905], [35.62509627, 45.04220401, 47.79626383, 45.21988529, 43.44307251, 40.2448095 , 37.57959033, 36.33582138, 40.15596886], [41.31089717, 45.13104465, 46.55249488, 44.15379762, 41.75510037, 43.53191315, 35.26973372, 38.645678 , 39.97828759], [37.49074969, 44.7756821 , 45.39756657, 39.97828759, 41.13321589, 38.29031544, 39.71176567, 40.51133142, 39.35640311], [45.39756657, 39.88944695, 37.31306841, 41.31089717, 36.86886522, 38.11263417, 41.66625973, 39.53408439, 37.49074969], [45.75292913, 38.82335928, 39.00104056, 40.2448095 , 41.75510037, 39.26756247, 39.0898812 , 42.64350676, 45.39756657], [41.93278165, 44.06495698, 44.15379762, 43.35423187, 39.71176567, 35.09205244, 39.44524375, 42.99886932, 38.46799672], [41.04437526, 42.82118804, 44.15379762, 40.60017206, 42.37698484, 36.24698075, 36.42466202, 40.60017206, 39.0898812 ], [40.06712823, 42.91002868, 43.44307251, 41.66625973, 38.2014748 , 36.51350266, 38.82335928, 37.13538714, 39.80060631]]), 950931458: array([[12.08232692, 11.19392052, 11.72696436, 13.32609586, 11.99348628, 11.63812372, 12.88189267, 10.39435477, 12.88189267], [12.70421139, 13.4149365 , 14.21450225, 12.79305203, 12.26000819, 14.48102417, 11.72696436, 14.21450225, 13.68145842], [13.68145842, 15.28058992, 16.52435887, 14.83638673, 15.014068 , 13.14841458, 11.72696436, 12.61537075, 13.14841458], [13.77029906, 17.0574027 , 20.25566571, 21.05523146, 15.4582712 , 14.39218353, 13.32609586, 13.14841458, 11.815805 ], [15.4582712 , 18.92305612, 23.72045063, 27.45175748, 17.32392462, 15.19174928, 11.28276116, 13.59261778, 13.59261778], [12.52653011, 16.34667759, 23.09856616, 22.83204424, 14.56986481, 13.05957394, 10.21667349, 12.88189267, 12.97073331], [12.70421139, 13.77029906, 14.83638673, 13.14841458, 13.23725522, 14.56986481, 11.46044244, 12.43768947, 14.39218353], [14.30334289, 13.68145842, 11.46044244, 13.32609586, 12.43768947, 11.63812372, 14.21450225, 12.52653011, 11.63812372], [12.79305203, 13.14841458, 12.34884883, 12.08232692, 12.61537075, 11.72696436, 12.17116755, 10.74971733, 12.79305203]]), 950931533: array([[1.15492831, 0.97724703, 0.97724703, 1.5991315 , 1.33260959, 1.06608767, 0.88840639, 0.71072511, 1.06608767], [1.06608767, 2.22101598, 2.57637853, 2.22101598, 1.77681278, 0.79956575, 0.97724703, 0.71072511, 0.4442032 ], [1.33260959, 3.19826301, 4.0866694 , 3.90898812, 2.30985662, 1.51029086, 0.4442032 , 1.33260959, 1.24376895], [0.71072511, 2.39869726, 4.61971323, 4.53087259, 1.86565342, 1.15492831, 1.51029086, 1.06608767, 0.88840639], [0.97724703, 1.5991315 , 2.66521917, 3.28710365, 0.53304383, 1.42145023, 0.79956575, 2.30985662, 1.42145023], [0.62188447, 1.15492831, 1.42145023, 1.51029086, 1.42145023, 1.24376895, 1.06608767, 0.71072511, 0.71072511], [1.51029086, 1.42145023, 0.97724703, 1.15492831, 1.15492831, 1.15492831, 1.86565342, 1.33260959, 1.42145023], [0.88840639, 0.71072511, 1.68797214, 1.42145023, 0.88840639, 0.26652192, 1.5991315 , 0.97724703, 0.53304383], [1.06608767, 1.06608767, 1.95449406, 1.51029086, 0.71072511, 1.5991315 , 0.97724703, 0.62188447, 1.24376895]]), 950931727: array([[16.52435887, 15.10290864, 15.99131503, 15.014068 , 14.83638673, 16.25783695, 15.99131503, 14.03682097, 16.16899631], [16.25783695, 14.74754609, 16.79088079, 17.14624334, 18.12349037, 18.92305612, 15.9024744 , 15.10290864, 16.79088079], [16.25783695, 20.16682507, 24.7865383 , 21.5882753 , 19.90030315, 15.36943056, 15.9024744 , 14.21450225, 18.12349037], [16.16899631, 29.49509217, 33.58176157, 27.89596067, 17.32392462, 16.43551823, 16.96856206, 16.16899631, 15.63595248], [20.52218763, 34.02596477, 31.3607456 , 31.89378943, 19.5449406 , 18.74537485, 15.4582712 , 14.65870545, 16.25783695], [21.41059402, 32.33799263, 36.33582138, 35.80277755, 20.78870954, 15.4582712 , 13.59261778, 17.59044654, 20.25566571], [17.94580909, 28.60668578, 33.22639902, 29.76161409, 20.96639082, 17.32392462, 18.03464973, 17.0574027 , 17.32392462], [18.39001229, 21.76595657, 25.05306022, 21.5882753 , 17.85696846, 15.99131503, 18.39001229, 16.61319951, 16.52435887], [17.59044654, 18.39001229, 22.38784105, 21.85479721, 18.30117165, 16.08015567, 16.79088079, 14.39218353, 16.25783695]]), 950931751: array([[3.10942237, 1.06608767, 1.51029086, 1.33260959, 2.22101598, 2.75405981, 1.5991315 , 1.24376895, 1.5991315 ], [2.57637853, 2.66521917, 2.93174109, 2.84290045, 2.84290045, 2.75405981, 2.22101598, 1.5991315 , 2.0433347 ], [1.51029086, 5.06391643, 6.21884474, 3.90898812, 1.86565342, 1.42145023, 1.51029086, 1.68797214, 1.95449406], [1.68797214, 7.37377304, 8.1733388 , 5.33043834, 3.02058173, 1.77681278, 1.42145023, 1.77681278, 2.30985662], [3.37594429, 7.64029496, 8.43986071, 4.88623515, 2.84290045, 4.17551004, 2.13217534, 2.0433347 , 2.57637853], [2.0433347 , 4.97507579, 7.55145432, 2.66521917, 1.95449406, 1.86565342, 1.5991315 , 2.30985662, 2.57637853], [1.95449406, 2.57637853, 3.99782876, 2.57637853, 2.22101598, 2.75405981, 2.13217534, 2.39869726, 1.68797214], [1.86565342, 3.82014748, 2.57637853, 1.86565342, 1.15492831, 1.86565342, 2.93174109, 1.06608767, 2.30985662], [2.75405981, 1.77681278, 1.68797214, 2.0433347 , 2.30985662, 2.0433347 , 1.68797214, 1.95449406, 2.30985662]]), 950932696: array([[0. , 0. , 0. , 0.08884064, 0. , 0. , 0.08884064, 0. , 0.08884064], [0.08884064, 0.08884064, 0.08884064, 0. , 0.17768128, 0. , 0.08884064, 0. , 0.08884064], [0.08884064, 0. , 0. , 0.17768128, 0. , 0. , 0. , 0. , 0.08884064], [0. , 0. , 0.17768128, 0.17768128, 0.08884064, 0. , 0.08884064, 0. , 0.35536256], [0. , 0. , 0.08884064, 0.17768128, 0.08884064, 0. , 0. , 0. , 0.17768128], [0.08884064, 0. , 0.35536256, 0. , 0.17768128, 0.26652192, 0.08884064, 0. , 0.26652192], [0. , 0.08884064, 0.08884064, 0.17768128, 0.26652192, 0. , 0.08884064, 0.08884064, 0.08884064], [0. , 0. , 0. , 0. , 0.26652192, 0.08884064, 0.08884064, 0.26652192, 0. ], [0.08884064, 0.08884064, 0.26652192, 0.17768128, 0.17768128, 0.08884064, 0.17768128, 0. , 0. ]])} Now we have calculated the 2D spatial tuning curve for our neurons in V1! Let's plot it with a heat map. #set the x and y axis limits extent = [ - 40 , 40 , - 40 , 40 ] # Create a 2x3 grid of subplots fig , axs = plt . subplots ( 2 , 3 , figsize = ( 12 , 8 )) # Iterate over each unit and plot the heatmap in a subplot for i , unit in enumerate ( v1_unit_IDs ): values = spatial_tuning_curve [ unit ] # Select the current subplot ax = axs [ i // 3 , i % 3 ] # Create the heatmap using seaborn with adjusted aspect ratio ax . imshow ( spatial_tuning_curve [ unit ], cmap = 'jet' , extent = extent , vmin = np . min ( values ), vmax = np . max ( values )) # Set the title and axis labels ax . set_title ( f 'Unit: { unit } ' ) ax . set_xlabel ( 'X-position' ) ax . set_ylabel ( 'Y-postition' ) # Adjust the spacing between subplots plt . tight_layout () # Display the plot plt . show () Awesome! Looks like some of our units show a great deal of spatial tuning.","title":"AllenSDK"},{"location":"io.allends/#allen-brain-atlas-neuropixel-loading-tutorial","text":"Author: Selen Calgin Date created: 13/07/2023 Last edited: 28/07/2023 The beauty of Pynapple's IO and the NWBmatic package is that loaders can be customized for any format of data. NWBmatic has many pre-made loaders, one of which being for the Allen Brain Atlas' Neuropixels dataset. The Neuropixels dataset contains electrophysiological data of the visual cortex and thalamus of mice who are shown passive visual stimuli. For more details about the dataset, please see documentation here: https://allensdk.readthedocs.io/en/latest/visual_coding_neuropixels.html This tutorial will demonstrate how to use NWMmatic's loader for the Allen Neuropixels data in conjunction with Pynapple for various analyses with the data. NWBmatic loads the data into Pynapple core objects, which can then be used for further analysis. Before you begin, make sure you have sufficient storage to download sessions from the database. The session sizes average about ~2gb. Let's get started! First, import libraries. # import libraries import nwbmatic as ntm import pynapple as nap import warnings warnings . filterwarnings ( 'ignore' ) import numpy as np import matplotlib.pyplot as plt % matplotlib inline To load a Neuropixels session, as with all data loading in NWBmatic, call NWBmatic's load_session() function. - path = where the Neuropixels data is downloaded/where it has been downloaded if you've already worked with a specific data session - session_type =\"allends\" indicates that we are loading data from the Allen Institute (DS = dataset). When you run the following cell, a GUI will appear. Using the GUI, choose from the two types of sessions in the Neuropixels database: 1) Brain observatory or 2) Functional connectivity. Then select the session ID you would like to load. Note: If this is your first time loading this session with NWBmatic, the loading will take a while as the data is first downloaded to your local system (to the path you indicated). If you have already loaded this session, loading the data will take about a minute. Additional note: In this tutorial, I am accessing session #715093703 in Brain Observatory. If you would like to follow along, choose the same session ID. All analyses will work with other sessions too, but may have different values than the ones I comment on here. # path to where data will be donwloaded path = r \"C:\\Users\\scalgi\\OneDrive - McGill University\\Peyrache Lab\\Data\\pynapple_test\" # load the data data = ntm . load_session ( path , session_type = \"allends\" ) # type: nap.io.allennp.AllenNP From our data object, we can access the following attributes: - epochs : a dictionary of IntervalSet that holds the start and end times of session epochs. In this dataset, epochs is trivial: there is only one epoch which is the session itself, under the key session . - stimulus_epochs_types : a dictionary of IntervalSet s that holds the start and end times of stimulus epochs, defined by the name of the stimulus type - stimulus_epochs_blocks : a dictionary of IntervalSet s that holds the start and end times of stimulus epoch, defined by the block of the stimulus. Each stimulus block contains one type of stimulus, and the block number indicates the order of when this block of stimulus was shown. Lack of stimulus (i.e. \"spontaneous\") is not given a block number. - stimulus_intervals : a dictionary of IntervalSet s that holds the start and end times of each presentation of of each stimulus type - optogenetic_stimulus_epochs : an IntervalSet that holds the start and end times of optogenetic stimulus epochs. None if session doesn't have it. - spikes : a TsGroup , which holds spike times for each unit and metadata on each unit - metadata : a dictionary of various Pandas Dataframes that holds various types of information: - stimulus_presentations : information about each stimulus presentation - stimulus_conditions : Each distinct stimulus state is called a \"stimulus condition\". Table holds additional information about each unique stimulus - stimulus_parameters : Dictionary of all stimulus parameters as keys and their range of values as values - channels : Information about all channels - probes : Information about all probes used - units : information about each unit identified in the session, including location in the brain - time_support : an IntervalSet containing the global time support based on epochs - stimulus_time_support : an IntervalSet containing the intervals of all stimulus blocks, providing a time support for stimuli","title":"Allen Brain Atlas Neuropixel Loading Tutorial"},{"location":"io.allends/#analyzing-neural-activity-in-the-primary-visual-cortex","text":"In this tutorial, we will be analyzing the neural activity of neurons in the primary visual cortex while also understanding how to use NWBmatic's loader for Allen Neuropixel data and how this data can be used in conjunction with Pynapple.","title":"Analyzing Neural Activity in the Primary Visual Cortex"},{"location":"io.allends/#overview-of-metadata","text":"First, let's get a grasp of the metadata we have access to. # data metadata print ( \"Stimulus presentations:\" ) stimulus_presentations = data . metadata [ \"stimulus_presentations\" ] print ( stimulus_presentations ) print ( \"Stimulus conditions\" ) stimulus_conditions = data . metadata [ \"stimulus_conditions\" ] print ( stimulus_conditions ) print ( \"Stimulus parameters:\" ) stimulus_parameters = data . metadata [ \"stimulus_parameters\" ] print ( stimulus_parameters ) print ( \"Probes:\" ) probes = data . metadata [ \"probes\" ] print ( probes ) print ( \"Channels:\" ) channels = data . metadata [ \"channels\" ] print ( channels ) Stimulus presentations: color contrast frame orientation \\ stimulus_presentation_id 0 null null null null 1 null 0.8 null 45.0 2 null 0.8 null 0.0 3 null 0.8 null 45.0 4 null 0.8 null 0.0 ... ... ... ... ... 70383 null 0.8 null 60.0 70384 null 0.8 null 90.0 70385 null 0.8 null 60.0 70386 null 0.8 null 60.0 70387 null 0.8 null 60.0 phase size \\ stimulus_presentation_id 0 null null 1 [3644.93333333, 3644.93333333] [20.0, 20.0] 2 [3644.93333333, 3644.93333333] [20.0, 20.0] 3 [3644.93333333, 3644.93333333] [20.0, 20.0] 4 [3644.93333333, 3644.93333333] [20.0, 20.0] ... ... ... 70383 0.75 [250.0, 250.0] 70384 0.0 [250.0, 250.0] 70385 0.0 [250.0, 250.0] 70386 0.5 [250.0, 250.0] 70387 0.0 [250.0, 250.0] spatial_frequency start_time stimulus_block \\ stimulus_presentation_id 0 null 13.470683 null 1 0.08 73.537433 0.0 2 0.08 73.770952 0.0 3 0.08 74.021150 0.0 4 0.08 74.271349 0.0 ... ... ... ... 70383 0.04 9133.889309 14.0 70384 0.02 9134.139517 14.0 70385 0.08 9134.389719 14.0 70386 0.32 9134.639920 14.0 70387 0.16 9134.890122 14.0 stimulus_name stop_time temporal_frequency \\ stimulus_presentation_id 0 spontaneous 73.537433 null 1 gabors 73.770952 4.0 2 gabors 74.021150 4.0 3 gabors 74.271349 4.0 4 gabors 74.521547 4.0 ... ... ... ... 70383 static_gratings 9134.139517 null 70384 static_gratings 9134.389719 null 70385 static_gratings 9134.639920 null 70386 static_gratings 9134.890122 null 70387 static_gratings 9135.140323 null x_position y_position duration \\ stimulus_presentation_id 0 null null 60.066750 1 0.0 30.0 0.233519 2 -30.0 -10.0 0.250199 3 10.0 20.0 0.250199 4 -40.0 -40.0 0.250199 ... ... ... ... 70383 null null 0.250209 70384 null null 0.250201 70385 null null 0.250201 70386 null null 0.250201 70387 null null 0.250201 stimulus_condition_id stimulus_presentation_id 0 0 1 1 2 2 3 3 4 4 ... ... 70383 4886 70384 4806 70385 4874 70386 4789 70387 4809 [70388 rows x 16 columns] Stimulus conditions color contrast frame mask opacity orientation \\ stimulus_condition_id 0 null null null null null null 1 null 0.8 null circle True 45.0 2 null 0.8 null circle True 0.0 3 null 0.8 null circle True 45.0 4 null 0.8 null circle True 0.0 ... ... ... ... ... ... ... 5022 null null 35.0 null null null 5023 null null 104.0 null null null 5024 null null 112.0 null null null 5025 null null 48.0 null null null 5026 null null 4.0 null null null phase size \\ stimulus_condition_id 0 null null 1 [3644.93333333, 3644.93333333] [20.0, 20.0] 2 [3644.93333333, 3644.93333333] [20.0, 20.0] 3 [3644.93333333, 3644.93333333] [20.0, 20.0] 4 [3644.93333333, 3644.93333333] [20.0, 20.0] ... ... ... 5022 null null 5023 null null 5024 null null 5025 null null 5026 null null spatial_frequency stimulus_name temporal_frequency \\ stimulus_condition_id 0 null spontaneous null 1 0.08 gabors 4.0 2 0.08 gabors 4.0 3 0.08 gabors 4.0 4 0.08 gabors 4.0 ... ... ... ... 5022 null natural_scenes null 5023 null natural_scenes null 5024 null natural_scenes null 5025 null natural_scenes null 5026 null natural_scenes null units x_position y_position color_triplet stimulus_condition_id 0 null null null null 1 deg 0.0 30.0 [1.0, 1.0, 1.0] 2 deg -30.0 -10.0 [1.0, 1.0, 1.0] 3 deg 10.0 20.0 [1.0, 1.0, 1.0] 4 deg -40.0 -40.0 [1.0, 1.0, 1.0] ... ... ... ... ... 5022 null null null null 5023 null null null null 5024 null null null null 5025 null null null null 5026 null null null null [5027 rows x 15 columns] Stimulus parameters: {'color': array([-1.0, 1.0], dtype=object), 'contrast': array([0.8, 1.0], dtype=object), 'frame': array([0.0, 1.0, 2.0, ..., 3598.0, 3599.0, -1.0], dtype=object), 'orientation': array([45.0, 0.0, 90.0, 315.0, 225.0, 135.0, 270.0, 180.0, 120.0, 150.0, 60.0, 30.0], dtype=object), 'phase': array(['[3644.93333333, 3644.93333333]', '[0.0, 0.0]', '[21211.93333333, 21211.93333333]', '0.5', '0.75', '0.0', '0.25'], dtype=object), 'size': array(['[20.0, 20.0]', '[300.0, 300.0]', '[250.0, 250.0]', '[1920.0, 1080.0]'], dtype=object), 'spatial_frequency': array(['0.08', '[0.0, 0.0]', '0.04', 0.32, 0.08, 0.04, 0.02, 0.16], dtype=object), 'temporal_frequency': array([4.0, 8.0, 2.0, 1.0, 15.0], dtype=object), 'x_position': array([0.0, -30.0, 10.0, -40.0, -10.0, 40.0, 30.0, 20.0, -20.0], dtype=object), 'y_position': array([30.0, -10.0, 20.0, -40.0, -20.0, 0.0, -30.0, 40.0, 10.0], dtype=object)} Probes: description location sampling_rate \\ id 810755797 probeA See electrode locations 29999.954846 810755799 probeB See electrode locations 29999.906318 810755801 probeC See electrode locations 29999.985470 810755803 probeD See electrode locations 29999.908100 810755805 probeE See electrode locations 29999.985679 810755807 probeF See electrode locations 30000.028033 lfp_sampling_rate has_lfp_data id 810755797 1249.998119 True 810755799 1249.996097 True 810755801 1249.999395 True 810755803 1249.996171 True 810755805 1249.999403 True 810755807 1250.001168 True Channels: filtering \\ id 850261194 AP band: 500 Hz high-pass; LFP band: 1000 Hz l... 850261196 AP band: 500 Hz high-pass; LFP band: 1000 Hz l... 850261202 AP band: 500 Hz high-pass; LFP band: 1000 Hz l... 850261206 AP band: 500 Hz high-pass; LFP band: 1000 Hz l... 850261212 AP band: 500 Hz high-pass; LFP band: 1000 Hz l... ... ... 850264894 AP band: 500 Hz high-pass; LFP band: 1000 Hz l... 850264898 AP band: 500 Hz high-pass; LFP band: 1000 Hz l... 850264902 AP band: 500 Hz high-pass; LFP band: 1000 Hz l... 850264908 AP band: 500 Hz high-pass; LFP band: 1000 Hz l... 850264912 AP band: 500 Hz high-pass; LFP band: 1000 Hz l... probe_channel_number probe_horizontal_position probe_id \\ id 850261194 0 43 810755801 850261196 1 11 810755801 850261202 4 43 810755801 850261206 6 59 810755801 850261212 9 11 810755801 ... ... ... ... 850264894 374 59 810755797 850264898 376 43 810755797 850264902 378 59 810755797 850264908 381 11 810755797 850264912 383 27 810755797 probe_vertical_position structure_acronym ecephys_structure_id \\ id 850261194 20 PO 1020.0 850261196 20 PO 1020.0 850261202 60 PO 1020.0 850261206 80 PO 1020.0 850261212 100 PO 1020.0 ... ... ... ... 850264894 3760 None NaN 850264898 3780 None NaN 850264902 3800 None NaN 850264908 3820 None NaN 850264912 3840 None NaN ecephys_structure_acronym anterior_posterior_ccf_coordinate \\ id 850261194 PO 7648.0 850261196 PO 7651.0 850261202 PO 7660.0 850261206 PO 7665.0 850261212 PO 7674.0 ... ... ... 850264894 NaN 7112.0 850264898 NaN 7107.0 850264902 NaN 7102.0 850264908 NaN 7094.0 850264912 NaN 7089.0 dorsal_ventral_ccf_coordinate left_right_ccf_coordinate id 850261194 3645.0 7567.0 850261196 3636.0 7566.0 850261202 3610.0 7564.0 850261206 3592.0 7562.0 850261212 3566.0 7560.0 ... ... ... 850264894 -53.0 7809.0 850264898 -69.0 7813.0 850264902 -85.0 7818.0 850264908 -109.0 7825.0 850264912 -126.0 7829.0 [2219 rows x 11 columns]","title":"Overview of Metadata"},{"location":"io.allends/#accessing-units-of-interest-v1","text":"Recall that we want to look at neuronal activity in V1. All unit information is stored within data.spikes , which is a Pynapple object called TsGroup. data.spikes holds spike times for each unit alongside metadata information about each unit, including in which brain structure they are recorded from. Let's see the units we have in our session: spikes = data . spikes print ( spikes . index ) [950910352 950910364 950910371 950910392 950910435 950910463 950910531 950910549 950910558 950910576 950910603 950910651 950910664 950910671 950910727 950910742 950910757 950910778 950910834 950910861 950910889 950910897 950910904 950910941 950911006 950911040 950911088 950911195 950911223 950911266 950911286 950911467 950911563 950911586 950911593 950911601 950911656 950911677 950911684 950911691 950911698 950911704 950911732 950911873 950911880 950911932 950911986 950912018 950912065 950912109 950912164 950912190 950912214 950912226 950912249 950912283 950912293 950912326 950912361 950912384 950912396 950912406 950912417 950912427 950912448 950912460 950912473 950912511 950912601 950912646 950912803 950912814 950912928 950912940 950912952 950913000 950913031 950913096 950913409 950913422 950913456 950913470 950913506 950913517 950913527 950913537 950913547 950913567 950913588 950913652 950913676 950913684 950913766 950913796 950913806 950913850 950913877 950913893 950913901 950913908 950913921 950913929 950913938 950913954 950913976 950913983 950913990 950914026 950914067 950914103 950914130 950914157 950914189 950914197 950914219 950914233 950914294 950914310 950914318 950914348 950914359 950914413 950914424 950914435 950914538 950914625 950914635 950914660 950914683 950914720 950914754 950914766 950914780 950914791 950914812 950914832 950914856 950914868 950914882 950914923 950914940 950914954 950914980 950915023 950915054 950915068 950915101 950915304 950915378 950915441 950915483 950915921 950915947 950915984 950915997 950916009 950916273 950916377 950916395 950916413 950916447 950916519 950916603 950916733 950916754 950916921 950916980 950917063 950917295 950917313 950917332 950917411 950917604 950917669 950917769 950917785 950917849 950917899 950918052 950918175 950918191 950918246 950918261 950918280 950918294 950918310 950918344 950918362 950918381 950918422 950918491 950918570 950918695 950918745 950918802 950918821 950918846 950918902 950918919 950918936 950918955 950919022 950919040 950919054 950919086 950919104 950919120 950919154 950919212 950919249 950919283 950919387 950919456 950919496 950919676 950919748 950919806 950919863 950919900 950919921 950919945 950919993 950920017 950920092 950920151 950920290 950920309 950920434 950920756 950920777 950920827 950920843 950920912 950920929 950920961 950920998 950921013 950921034 950921088 950921163 950921176 950921299 950921442 950921601 950921709 950922041 950922122 950922146 950922174 950922208 950922234 950922258 950922286 950922329 950922351 950922368 950922383 950922466 950922494 950922551 950922576 950922600 950922641 950922659 950922684 950922706 950922725 950922745 950922781 950922841 950922883 950922896 950922929 950922961 950922994 950923011 950923049 950923089 950923121 950923165 950923181 950923217 950923236 950923254 950923348 950923365 950923405 950923464 950923485 950923606 950923669 950923690 950923730 950923754 950923859 950923880 950923897 950923914 950923939 950923958 950923976 950924072 950924089 950924107 950924147 950924173 950924212 950924231 950924272 950924372 950924413 950924434 950924452 950924500 950924519 950924598 950924635 950924693 950924735 950924753 950924800 950924845 950924883 950924899 950925057 950925092 950925112 950925132 950925149 950925166 950925187 950925212 950925267 950925332 950925368 950925383 950925400 950925438 950925455 950925508 950925555 950925626 950925644 950925664 950925707 950925730 950925749 950925786 950925850 950925902 950925925 950925950 950925967 950925983 950926001 950926055 950926095 950926208 950926709 950926787 950926805 950926867 950926886 950926928 950927002 950927024 950927046 950927066 950927229 950927323 950927341 950927745 950927775 950928113 950928160 950928179 950928198 950928255 950928387 950928423 950928440 950928461 950928497 950928518 950928536 950928588 950928686 950928759 950928795 950928855 950928873 950928891 950928911 950928976 950928991 950929009 950929067 950929083 950929101 950929134 950929153 950929171 950929188 950929206 950929227 950929245 950929286 950929377 950929400 950929420 950929495 950929514 950929531 950929570 950929592 950929663 950929697 950929750 950929787 950929804 950929824 950929882 950930105 950930145 950930215 950930237 950930276 950930340 950930358 950930375 950930392 950930407 950930423 950930437 950930454 950930522 950930795 950930866 950930888 950930964 950930985 950931004 950931043 950931118 950931164 950931181 950931236 950931254 950931272 950931315 950931363 950931423 950931458 950931517 950931533 950931565 950931581 950931617 950931656 950931727 950931751 950931770 950931805 950931853 950931878 950931899 950931959 950932032 950932087 950932102 950932445 950932563 950932578 950932696 950932820 950932888 950932943 950932963 950932980 950933012 950933040 950933057 950933173 950933208 950933226 950933426 950933536 950933555 950933606 950933660 950933698 950933732 950933840 950933890 950933907 950933924 950933960 950934028 950934044 950934181 950934229 950934268 950934286 950934304 950934728 950934765 950934843 950934971 950934992 950935070 950935165 950935201 950935269 950935285 950935317 950935402 950935460 950935478 950935499 950935575 950935610 950935658 950935720 950935755 950935907 950935925 950935942 950935975 950936008 950936025 950936093 950936113 950936162 950936177 950936194 950936224 950936272 950936292 950936326 950936345 950936412 950936465 950936482 950936572 950936639 950936656 950936675 950936710 950936727 950936759 950936855 950936870 950936908 950936941 950936979 950936992 950937050 950937068 950937114 950937333 950937929 950937990 950938074 950938091 950938171 950938279 950938344 950938459 950938511 950938598 950938629 950938657 950938700 950938797 950938924 950938960 950938978 950939028 950939079 950939114 950939168 950939257 950939347 950939479 950939509 950939523 950939641 950939678 950939945 950940001 950940023 950940040 950940104 950940121 950940157 950940171 950940185 950940200 950940219 950940237 950940311 950940389 950940437 950940453 950940467 950940507 950940526 950940545 950940615 950940631 950940649 950940671 950940688 950940718 950940859 950940928 950941494 950941529 950941583 950941776 950941795 950942129 950942155 950942199 950942235 950942252 950942304 950942974 950943198 950943517 950943580 950943625 950943695 950943735 950943756 950943795 950943813 950943830 950943877 950944146 950944160 950944212 950944228 950944247 950944259 950944276 950944444 950944566 950944600 950944675 950944706 950944769 950944784 950944815 950944827 950944858 950944872 950944952 950944968 950944989 950945008 950945042 950945060 950945077 950945127 950945180 950945232 950945252 950945295 950945314 950945467 950945518 950945554 950945572 950945588 950945625 950945660 950945768 950945783 950945817 950945986 950946003 950946022 950946068 950946155 950946176 950946192 950946228 950946310 950946327 950946343 950946376 950946437 950946497 950946641 950946658 950946673 950946741 950947140 950947156 950947211 950947224 950947271 950947368 950947412 950947488 950947533 950947555 950947626 950947647 950947665 950947763 950947778 950947798 950947832 950947849 950947868 950947941 950947963 950947990 950948009 950948026 950948063 950948092 950948139 950948174 950948210 950948246 950948281 950948306 950948371 950948396 950948414 950948488 950948527 950948547 950948564 950948631 950948648 950948683 950948716 950948744 950948778 950948793 950948826 950948853 950948867 950948881 950948970 950949089 950949118 950949200 950949248 950949428 950949449 950949628 950949861 950949933 950949961 950949994 950950009 950950031 950950101 950950116 950950160 950950176 950950224 950950270 950950308 950950382 950950433 950950576 950950740 950950756 950950810 950950880 950950919 950951153 950951364 950951511 950951525 950951577 950951613 950951636 950951678 950951702 950951791 950951829 950951840 950951851 950951879 950951891 950951902 950951940 950951950 950951979 950952002 950952213 950952241 950952256 950952312 950952336 950952386 950952629 950952690 950952704 950952719 950952734 950952748 950952845 950952881 950952969 950952985 950953119 950953159 950953187 950953293 950953417 950953703 950954163 950954180 950954195 950954630 950954647 950954737 950954793 950954823 950954838 950954886 950954905 950954922 950954941 950954983 950955053 950955181 950955212 950955361 950955399 950955414 950955543 950955784 950955833 950955844 950955897 950955951 950955965 950955974 950956019 950956030 950956043 950956100 950956148 950956205 950956259 950956297 950956336 950956386 950956399 950956413 950956435 950956493 950956504 950956514 950956527 950956541 950956563 950956592 950956604 950956616 950956630 950956667 950956680 950956778 950956835 950956845 950956870 950956911 950956952 950957053 950957270 950957282 950957295 950957320 950957408] To see what metadata is stored within spikes : print ( spikes . metadata_columns ) ['rate', 'waveform_PT_ratio', 'waveform_amplitude', 'amplitude_cutoff', 'cluster_id', 'cumulative_drift', 'd_prime', 'firing_rate', 'isi_violations', 'isolation_distance', 'L_ratio', 'local_index', 'max_drift', 'nn_hit_rate', 'nn_miss_rate', 'peak_channel_id', 'presence_ratio', 'waveform_recovery_slope', 'waveform_repolarization_slope', 'silhouette_score', 'snr', 'waveform_spread', 'waveform_velocity_above', 'waveform_velocity_below', 'waveform_duration', 'filtering', 'probe_channel_number', 'probe_horizontal_position', 'probe_id', 'probe_vertical_position', 'structure_acronym', 'ecephys_structure_id', 'ecephys_structure_acronym', 'anterior_posterior_ccf_coordinate', 'dorsal_ventral_ccf_coordinate', 'left_right_ccf_coordinate', 'probe_description', 'location', 'probe_sampling_rate', 'probe_lfp_sampling_rate', 'probe_has_lfp_data'] Now, we want to get the units that are in the primary visual cortex (V1), as these are neurons that are most likely to have orientation tuning and tuning to locations in space. The metadata column that corresponds to location in the brain is \"ecephys_structure_acronym\". We can use TsGroup 's functionality to organize the units based on columns in the unit's metadata. This function is called getby_category() . Let's see how we can use it for our purposes: # organize units by structure units_structures = spikes . getby_category ( \"ecephys_structure_acronym\" ) units_structures is a dictionary of TsGroups , with the structure names as keys. Let's look at the names of the structures we are working on by getting the keys of the dictionary. # take a look at the structure names units_structures . keys () dict_keys(['APN', 'CA1', 'CA3', 'DG', 'LGd', 'LP', 'PO', 'PoT', 'VISam', 'VISl', 'VISp', 'VISpm', 'VISrl', 'grey']) To get an overview of how the units are distributed by brain structure, let's quickly count the number of units per brain area and plot it: # getting a list of brain areas brain_areas = units_structures . keys () # counting the length of the TsGroup (i.e. number of units) associated with each brain structure unit_count_by_brain_region = [ len ( units_structures [ key ]) for key in brain_areas ] # plot plt . bar ( brain_areas , unit_count_by_brain_region ) <BarContainer object of 14 artists> Nice! Looks like we have a decent number of units in the primary visual cortex (\"VISp\"). Let's retrieve these units from the dictionary units_structures . # get the TsGroup of units in V1 v1_units = units_structures [ \"VISp\" ] # how many units did we get? print ( \"Number of units in V1: %s \" % len ( v1_units )) Number of units in V1: 60 Now, from the units in V1, we also want to get the units that have a high signal-to-noise (SNR) ratio. If you look above, you can see that this information is also stored in the metadata under \"snr\". We can use the TsGroup 's getby_threshold() function to do this. Rather than a dictionary of TsGroups, this function just returns a new TsGroup with units that obey the given threshold. Let's get units with an SNR > 4: v1_high_snr_units = v1_units . getby_threshold ( \"snr\" , 4 ) # default operator is >, but other ones can be passed via an optional argument # how many units did we get? print ( \"Number of units in V1: %s \" % len ( v1_high_snr_units )) # what are the IDs of the units? v1_unit_IDs = v1_high_snr_units . keys () print ( \"V1 Unit IDs: %s \" % v1_unit_IDs ) Number of units in V1: 6 V1 Unit IDs: [950930985, 950931458, 950931533, 950931727, 950931751, 950932696] Awesome! We can quickly visualize the units' activity over time with TsGroup 's count() function, which counts the number of spikes of each unit within intervals of time. The default intervals of the count() function are based on the global time support of the data. In this database, the global session epoch is the start and end time of the entire session. Let's first use the default interval and specify the bin size in seconds. Here, I'm setting the bin size to 1s. spike_counts = v1_high_snr_units . count ( bin_size = 1 ) spike_counts .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 950930985 950931458 950931533 950931727 950931751 950932696 Time (s) 0.5 0 0 0 0 0 0 1.5 0 0 0 0 0 0 2.5 0 0 0 0 0 0 3.5 0 0 0 0 0 0 4.5 0 0 0 0 0 0 ... ... ... ... ... ... ... 9636.5 0 0 0 0 0 0 9637.5 0 0 0 0 0 0 9638.5 0 0 0 0 0 0 9639.5 0 0 0 0 0 0 9640.5 0 0 0 0 0 0 9641 rows \u00d7 6 columns Nice! With this table, we have everything we need to plot spike counts per time interval. Let's go ahead and plot it for each of our units. This will take a few minutes since the bin number is so high. # Create 1x6 grid of subplots fig , axs = plt . subplots ( 1 , 6 , figsize = ( 15 , 4 )) time_intervals = spike_counts . index # Iterate over each neuron and plot the spike count data for i , ( unit_id , ax ) in enumerate ( zip ( v1_unit_IDs , axs )): ax . bar ( time_intervals , spike_counts [ unit_id ]) ax . set_xlabel ( 'Time (s)' ) ax . set_ylabel ( 'Spike Count' ) ax . set_title ( unit_id ) plt . tight_layout () plt . show ()","title":"Accessing Units of Interest (V1)"},{"location":"io.allends/#understanding-the-stimuli","text":"Now that we've retrieved our units in V1, let's take a look at the kind of stimuli we are working with and how stimulus epochs are organized. First, let's look at stimulus epochs by type and by block. stimulus_epochs_types = data . stimulus_epochs_types stimulus_epochs_blocks = data . stimulus_epochs_blocks print ( \"Stimulus names: %s \" % stimulus_epochs_types . keys ()) print ( \"Stimulus blocks: %s \" % stimulus_epochs_blocks . keys ()) Stimulus names: dict_keys(['drifting_gratings', 'flashes', 'gabors', 'natural_movie_one', 'natural_movie_three', 'natural_scenes', 'spontaneous', 'static_gratings']) Stimulus blocks: dict_keys([0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 'null']) Each block interval provides the beginning and end of the given block of stimulus presentations, stored is stimulus_epochs_blocks Each stimulus blocks contains only one type of stimulus, however, there are multiple blocks of one kind of stimulus. Each IntervalSet in stimulus_epochs_types represents one block of stimulus. Individual stimulus interval of a given stimulus type is stored in stimulus_intervals . To see a visualization of how the stimuli are organized, see here: chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://brainmapportal-live-4cc80a57cd6e400d854-f7fdcae.divio-media.net/filer_public/0f/5d/0f5d22c9-f8f6-428c-9f7a-2983631e72b4/neuropixels_cheat_sheet_nov_2019.pdf For example, from Allen Neuropixels' documentation, we know that drifting gratings are shown in multiple blocks, specifically block 2, 5, and 7. Within these blocks, there are individual stimulus presentations of various conditions. Let's take a look: # Drifting gratings epochs: drifting_gratings = stimulus_epochs_types [ \"drifting_gratings\" ] print ( \"Drifting gratings epochs: \\n %s \\n \" % drifting_gratings ) # Block 2, 5 and 7 epochs print ( \"Block 2 intervals \\n %s : \" % stimulus_epochs_blocks [ 2 ]) print ( \"Block 5 intervals \\n %s : \" % stimulus_epochs_blocks [ 5 ]) print ( \"Block 7 intervals \\n %s : \" % stimulus_epochs_blocks [ 7 ]) Drifting gratings epochs: start end 0 1574.774823 2174.275707 1 3166.137683 3765.638457 2 4697.416823 5380.987797 Block 2 intervals start end 0 1574.774823 2174.275707: Block 5 intervals start end 0 3166.137683 3765.638457: Block 7 intervals start end 0 4697.416823 5380.987797: Notice how the epochs defined by stimulus type and block line up? type and block are two ways of organizing the stimulus epochs, which one you use depends on your analysis goals. Now, let's take a look at the stimulus presentation intervals of all drifting gratings: # get dictionary of stimulus intervals organized by stimulus type stimulus_intervals = data . stimulus_intervals # get stimulus intervals for drifting gratings drifting_gratings_intervals = stimulus_intervals [ 'drifting_gratings' ] drifting_gratings_intervals .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } start end 0 1574.774823 1576.776513 1 1577.777347 1579.779027 2 1580.779833 1582.781563 3 1583.782367 1585.784047 4 1586.784883 1588.786553 ... ... ... 5 5366.976107 5368.977777 6 5369.978603 5371.980283 7 5372.981107 5374.982807 8 5375.983663 5377.985343 9 5378.986127 5380.987797 628 rows \u00d7 2 columns Notice that the first start time and the last end time matches that of the start and end time of the drifting gratings stimulus epochs. To get the stimulus interval for only one of the blocks, you can use IntervalSet 's intersect function, which finds the common times between two IntervalSet s. Let's find the stimulus intervals of drifting gratings in block 2 as an example: block2_stimulus_epochs = stimulus_epochs_blocks [ 2 ] # get block 2 of stimulus epochs # intersect block 2 intervals with drifting gratings intervals to get drifting gratings in block 2 only drifting_gratings_block_2_intervals = drifting_gratings_intervals . intersect ( block2_stimulus_epochs ) # what does this look like? drifting_gratings_block_2_intervals .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } start end 0 1574.774823 1576.776513 1 1577.777347 1579.779027 2 1580.779833 1582.781563 3 1583.782367 1585.784047 4 1586.784883 1588.786553 ... ... ... 5 2160.263967 2162.265657 6 2163.266503 2165.268183 7 2166.269007 2168.270667 8 2169.271543 2171.273213 9 2172.274017 2174.275707 200 rows \u00d7 2 columns To look at the overall set of available parameters of the stimuli, we can access it through the metadata as follows: for key , values in stimulus_parameters . items (): print ( f ' { key } : { values } ' ) color:[-1.0 1.0] contrast:[0.8 1.0] frame:[0.0 1.0 2.0 ... 3598.0 3599.0 -1.0] orientation:[45.0 0.0 90.0 315.0 225.0 135.0 270.0 180.0 120.0 150.0 60.0 30.0] phase:['[3644.93333333, 3644.93333333]' '[0.0, 0.0]' '[21211.93333333, 21211.93333333]' '0.5' '0.75' '0.0' '0.25'] size:['[20.0, 20.0]' '[300.0, 300.0]' '[250.0, 250.0]' '[1920.0, 1080.0]'] spatial_frequency:['0.08' '[0.0, 0.0]' '0.04' 0.32 0.08 0.04 0.02 0.16] temporal_frequency:[4.0 8.0 2.0 1.0 15.0] x_position:[0.0 -30.0 10.0 -40.0 -10.0 40.0 30.0 20.0 -20.0] y_position:[30.0 -10.0 20.0 -40.0 -20.0 0.0 -30.0 40.0 10.0] Great, now that we have a good understanding of the stimuli in this session, we can start examining the activity of the V1 units of interest in relation to the stimuli.","title":"Understanding the Stimuli"},{"location":"io.allends/#analyzing-the-neural-response-in-v1-to-visual-stimuli","text":"Now that we've acquired units from V1 and have explored our data and stimuli, we now do some analysis. Specifically, we're going to look at the response of V1 neurons to visual stimuli.","title":"Analyzing the Neural Response in V1 to Visual Stimuli"},{"location":"io.allends/#1d-orientation-tuning-curves","text":"This section of the tutorial is adopted from Seigle et al (2021) Dataset Tutorial by Dhruv Mehrotra : https://github.com/PeyracheLab/pynacollada/blob/main/pynacollada/Pynapple%20Paper%20Figures/Siegle%202021/Siegle_dataset.ipynb V1 neurons are known to have orientation specific tuning. In this dataset, we have static grating stimuli which varies in orientation. We are going to compute the tuning curve of V1 neurons to the orientations. To do this, first we need to get information about the static grating stimulus presentations, as follows: drifting_gratings_presentations = stimulus_presentations [ stimulus_presentations [ 'stimulus_name' ] == 'drifting_gratings' ] drifting_gratings_presentations .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } color contrast frame orientation phase size spatial_frequency start_time stimulus_block stimulus_name stop_time temporal_frequency x_position y_position duration stimulus_condition_id stimulus_presentation_id 3798 null 0.8 null 315.0 [21211.93333333, 21211.93333333] [250.0, 250.0] 0.04 1574.774823 2.0 drifting_gratings 1576.776513 4.0 null null 2.00169 246 3799 null 0.8 null 90.0 [21211.93333333, 21211.93333333] [250.0, 250.0] 0.04 1577.777347 2.0 drifting_gratings 1579.779027 8.0 null null 2.00168 247 3800 null 0.8 null 225.0 [21211.93333333, 21211.93333333] [250.0, 250.0] 0.04 1580.779833 2.0 drifting_gratings 1582.781563 2.0 null null 2.00173 248 3801 null 0.8 null 90.0 [21211.93333333, 21211.93333333] [250.0, 250.0] 0.04 1583.782367 2.0 drifting_gratings 1585.784047 2.0 null null 2.00168 249 3802 null 0.8 null 135.0 [21211.93333333, 21211.93333333] [250.0, 250.0] 0.04 1586.784883 2.0 drifting_gratings 1588.786553 8.0 null null 2.00167 250 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 49426 null 0.8 null 135.0 [21211.93333333, 21211.93333333] [250.0, 250.0] 0.04 5366.976107 7.0 drifting_gratings 5368.977777 15.0 null null 2.00167 274 49427 null 0.8 null 180.0 [21211.93333333, 21211.93333333] [250.0, 250.0] 0.04 5369.978603 7.0 drifting_gratings 5371.980283 8.0 null null 2.00168 260 49428 null 0.8 null 0.0 [21211.93333333, 21211.93333333] [250.0, 250.0] 0.04 5372.981107 7.0 drifting_gratings 5374.982807 1.0 null null 2.00170 251 49429 null 0.8 null 180.0 [21211.93333333, 21211.93333333] [250.0, 250.0] 0.04 5375.983663 7.0 drifting_gratings 5377.985343 4.0 null null 2.00168 282 49430 null 0.8 null 270.0 [21211.93333333, 21211.93333333] [250.0, 250.0] 0.04 5378.986127 7.0 drifting_gratings 5380.987797 4.0 null null 2.00167 279 628 rows \u00d7 16 columns Let's see what orientations we have: orientations = drifting_gratings_presentations [ 'orientation' ] . values # get oreintations of the static gratings orientations array([315.0, 90.0, 225.0, 90.0, 135.0, 0.0, 315.0, 315.0, 270.0, 90.0, 0.0, 315.0, 270.0, 'null', 'null', 315.0, 315.0, 315.0, 180.0, 'null', 45.0, 315.0, 90.0, 270.0, 135.0, 90.0, 135.0, 135.0, 135.0, 45.0, 315.0, 0.0, 180.0, 0.0, 315.0, 315.0, 45.0, 90.0, 180.0, 'null', 135.0, 225.0, 0.0, 135.0, 45.0, 315.0, 225.0, 0.0, 45.0, 270.0, 135.0, 180.0, 180.0, 180.0, 90.0, 0.0, 0.0, 90.0, 225.0, 90.0, 'null', 45.0, 'null', 270.0, 45.0, 180.0, 225.0, 225.0, 45.0, 90.0, 315.0, 270.0, 270.0, 270.0, 0.0, 45.0, 270.0, 270.0, 225.0, 270.0, 180.0, 180.0, 315.0, 45.0, 90.0, 270.0, 315.0, 135.0, 315.0, 45.0, 135.0, 'null', 225.0, 180.0, 90.0, 225.0, 180.0, 90.0, 315.0, 270.0, 135.0, 270.0, 180.0, 315.0, 0.0, 315.0, 225.0, 45.0, 90.0, 0.0, 135.0, 0.0, 45.0, 90.0, 90.0, 135.0, 270.0, 270.0, 45.0, 225.0, 45.0, 180.0, 180.0, 90.0, 90.0, 90.0, 270.0, 0.0, 315.0, 225.0, 180.0, 270.0, 45.0, 315.0, 45.0, 135.0, 45.0, 45.0, 45.0, 270.0, 'null', 180.0, 180.0, 90.0, 'null', 315.0, 180.0, 0.0, 'null', 135.0, 315.0, 225.0, 45.0, 315.0, 270.0, 135.0, 'null', 135.0, 270.0, 0.0, 225.0, 0.0, 135.0, 225.0, 225.0, 135.0, 45.0, 90.0, 225.0, 45.0, 45.0, 0.0, 180.0, 180.0, 135.0, 315.0, 180.0, 315.0, 0.0, 315.0, 180.0, 'null', 135.0, 270.0, 225.0, 90.0, 270.0, 135.0, 270.0, 180.0, 225.0, 0.0, 0.0, 0.0, 270.0, 180.0, 270.0, 'null', 'null', 45.0, 0.0, 'null', 135.0, 'null', 315.0, 135.0, 0.0, 90.0, 225.0, 45.0, 315.0, 225.0, 0.0, 180.0, 0.0, 225.0, 0.0, 270.0, 45.0, 225.0, 180.0, 315.0, 90.0, 90.0, 90.0, 270.0, 180.0, 45.0, 0.0, 270.0, 45.0, 90.0, 45.0, 270.0, 0.0, 90.0, 0.0, 90.0, 90.0, 225.0, 180.0, 180.0, 135.0, 135.0, 270.0, 180.0, 135.0, 135.0, 45.0, 315.0, 225.0, 315.0, 90.0, 0.0, 135.0, 'null', 90.0, 135.0, 135.0, 225.0, 45.0, 0.0, 0.0, 135.0, 315.0, 135.0, 225.0, 315.0, 225.0, 270.0, 135.0, 135.0, 225.0, 135.0, 315.0, 'null', 180.0, 315.0, 225.0, 0.0, 225.0, 180.0, 270.0, 45.0, 135.0, 45.0, 270.0, 135.0, 270.0, 90.0, 270.0, 45.0, 45.0, 45.0, 135.0, 'null', 90.0, 45.0, 225.0, 45.0, 225.0, 45.0, 45.0, 0.0, 'null', 45.0, 270.0, 270.0, 45.0, 225.0, 90.0, 135.0, 45.0, 90.0, 180.0, 90.0, 315.0, 0.0, 0.0, 225.0, 315.0, 180.0, 45.0, 90.0, 225.0, 45.0, 180.0, 'null', 135.0, 0.0, 0.0, 135.0, 315.0, 135.0, 180.0, 270.0, 270.0, 315.0, 180.0, 315.0, 90.0, 45.0, 0.0, 315.0, 90.0, 45.0, 135.0, 180.0, 135.0, 135.0, 180.0, 0.0, 270.0, 225.0, 270.0, 45.0, 180.0, 0.0, 225.0, 45.0, 90.0, 270.0, 45.0, 225.0, 270.0, 180.0, 315.0, 225.0, 0.0, 225.0, 90.0, 315.0, 270.0, 45.0, 0.0, 45.0, 90.0, 180.0, 0.0, 90.0, 90.0, 180.0, 45.0, 45.0, 90.0, 135.0, 270.0, 135.0, 225.0, 'null', 315.0, 270.0, 135.0, 315.0, 270.0, 0.0, 270.0, 180.0, 0.0, 270.0, 315.0, 225.0, 180.0, 180.0, 90.0, 315.0, 0.0, 0.0, 45.0, 270.0, 315.0, 180.0, 270.0, 270.0, 0.0, 270.0, 135.0, 270.0, 45.0, 90.0, 'null', 315.0, 90.0, 180.0, 90.0, 225.0, 315.0, 0.0, 315.0, 225.0, 180.0, 0.0, 180.0, 180.0, 180.0, 225.0, 90.0, 45.0, 225.0, 0.0, 0.0, 270.0, 225.0, 0.0, 90.0, 'null', 135.0, 225.0, 315.0, 180.0, 135.0, 90.0, 0.0, 135.0, 180.0, 270.0, 90.0, 0.0, 225.0, 180.0, 180.0, 90.0, 90.0, 90.0, 180.0, 315.0, 180.0, 0.0, 270.0, 135.0, 45.0, 315.0, 'null', 225.0, 90.0, 180.0, 315.0, 'null', 45.0, 90.0, 0.0, 90.0, 225.0, 90.0, 180.0, 0.0, 45.0, 180.0, 270.0, 315.0, 315.0, 225.0, 90.0, 225.0, 90.0, 315.0, 315.0, 0.0, 270.0, 90.0, 225.0, 225.0, 135.0, 45.0, 90.0, 180.0, 45.0, 135.0, 270.0, 225.0, 135.0, 0.0, 0.0, 45.0, 90.0, 315.0, 90.0, 45.0, 45.0, 315.0, 45.0, 90.0, 0.0, 45.0, 180.0, 225.0, 0.0, 315.0, 180.0, 90.0, 225.0, 225.0, 270.0, 45.0, 0.0, 90.0, 0.0, 315.0, 225.0, 270.0, 180.0, 45.0, 135.0, 225.0, 225.0, 225.0, 135.0, 225.0, 270.0, 90.0, 'null', 270.0, 315.0, 135.0, 225.0, 90.0, 180.0, 225.0, 135.0, 270.0, 315.0, 45.0, 180.0, 0.0, 0.0, 225.0, 135.0, 315.0, 135.0, 135.0, 90.0, 225.0, 0.0, 315.0, 270.0, 180.0, 'null', 315.0, 135.0, 45.0, 0.0, 135.0, 45.0, 180.0, 'null', 270.0, 180.0, 270.0, 135.0, 225.0, 225.0, 'null', 270.0, 135.0, 0.0, 135.0, 270.0, 180.0, 315.0, 270.0, 315.0, 225.0, 135.0, 315.0, 315.0, 0.0, 90.0, 180.0, 270.0, 225.0, 135.0, 90.0, 135.0, 45.0, 180.0, 270.0, 315.0, 0.0, 135.0, 315.0, 225.0, 45.0, 225.0, 135.0, 180.0, 0.0, 180.0, 270.0], dtype=object) We can see that there are some null values. Let's convert these values to floats for better handling. orientations [ orientations == 'null' ] = np . nan # replace all null values to NaN orientations = orientations . astype ( float ) # convert to float array angle_range = np . unique ( orientations )[ 0 : - 1 ] # find all unique values excluding NaNs angle_range array([ 0., 45., 90., 135., 180., 225., 270., 315.]) These are the 8 orientations of the driftig gratings, sampling 360 degrees at 45 degree intervals. Now we need to access information from stimulus_presentations metadata and load it as a Pynapple object. Specifically, we need to create a dictionary of IntervalSets for each orientation. First, we need stimulus intervals for drifting gratings to organize by orientation. drifting_gratings_intervals = stimulus_intervals [ 'drifting_gratings' ] drifting_gratings_intervals .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } start end 0 1574.774823 1576.776513 1 1577.777347 1579.779027 2 1580.779833 1582.781563 3 1583.782367 1585.784047 4 1586.784883 1588.786553 ... ... ... 5 5366.976107 5368.977777 6 5369.978603 5371.980283 7 5372.981107 5374.982807 8 5375.983663 5377.985343 9 5378.986127 5380.987797 628 rows \u00d7 2 columns Great. Now let's proceed to sorting these intervals into IntervalSet s by their orientation dict_ori = {} for angle in angle_range : tokeep = [] # list of trials to keep for i in range ( len ( drifting_gratings_presentations )): # loop over all gabors trials if float ( orientations [ i ] == angle ): # find trials with given orientation tokeep . append ( i ) dict_ori [ angle ] = drifting_gratings_intervals . loc [ tokeep ] # make dictionary of IntervalSets dict_ori {0.0: start end 0 1589.787377 1591.789047 1 1604.799993 1606.801573 2 1667.852587 1669.854267 3 1673.857607 1675.859277 4 1700.880223 1702.881873 .. ... ... 69 5237.868283 5239.869913 70 5279.903393 5281.905073 71 5312.930947 5314.932627 72 5348.961047 5350.962727 73 5372.981107 5374.982807 [74 rows x 2 columns], 45.0: start end 0 1634.824993 1636.826683 1 1661.847597 1663.849267 2 1682.865123 1684.866833 3 1706.885213 1708.886883 4 1718.895253 1720.896963 .. ... ... 70 5180.820607 5182.822277 71 5234.865757 5236.867417 72 5243.873283 5245.874953 73 5336.951027 5338.952697 74 5360.971097 5362.972757 [75 rows x 2 columns], 90.0: start end 0 1577.777347 1579.779027 1 1583.782367 1585.784047 2 1601.797417 1603.799097 3 1640.830033 1642.831703 4 1649.837547 1651.839217 .. ... ... 70 5144.790507 5146.792167 71 5162.805557 5164.807227 72 5207.843153 5209.844843 73 5315.933463 5317.935153 74 5330.945997 5332.947677 [75 rows x 2 columns], 135.0: start end 0 1586.784883 1588.786553 1 1646.834993 1648.836703 2 1652.840073 1654.841743 3 1655.842567 1657.844227 4 1658.845063 1660.846763 .. ... ... 69 5303.923423 5305.925113 70 5327.943483 5329.945163 71 5333.948513 5335.950203 72 5351.963563 5353.965203 73 5366.976107 5368.977777 [74 rows x 2 columns], 180.0: start end 0 1628.820023 1630.821703 1 1670.855103 1672.856783 2 1688.870163 1690.871853 3 1727.902757 1729.904427 4 1730.905303 1732.906953 .. ... ... 70 5288.910887 5290.912557 71 5318.935957 5320.937647 72 5339.953523 5341.955183 73 5369.978603 5371.980283 74 5375.983663 5377.985343 [75 rows x 2 columns], 225.0: start end 0 1580.779833 1582.781563 1 1697.877687 1699.879357 2 1712.890213 1714.891913 3 1748.920323 1750.922013 4 1772.940363 1774.942053 .. ... ... 70 5267.893343 5269.895043 71 5300.920927 5302.922597 72 5324.940987 5326.942657 73 5357.968573 5359.970233 74 5363.973593 5365.975283 [75 rows x 2 columns], 270.0: start end 0 1598.794933 1600.796563 1 1610.804943 1612.806633 2 1643.832527 1645.834217 3 1721.897747 1723.899427 4 1763.932867 1765.934547 .. ... ... 70 5285.908373 5287.910063 71 5294.915917 5296.917577 72 5321.938523 5323.940153 73 5342.956047 5344.957707 74 5378.986127 5380.987797 [75 rows x 2 columns], 315.0: start end 0 1574.774823 1576.776513 1 1592.789903 1594.791563 2 1595.792407 1597.794077 3 1607.802437 1609.804107 4 1619.812457 1621.814137 .. ... ... 70 5297.918423 5299.920093 71 5306.925927 5308.927607 72 5309.928463 5311.930133 73 5345.958553 5347.960203 74 5354.966067 5356.967737 [75 rows x 2 columns]} This dictionary has the drifting_gratings_intervals sorted by orientation. We can use these to compute orientation tuning curves. Since the stimuli are presented in discrete orientations (i.e. not spanning all angular values from 0 to 360 degrees), we will be using Pynapple's compute_discrete_tuning_curves . We can calculate tuning curves with one line with Pynapple! # Plot firing rate of V1 units as a function of orientation, i.e. an orientation tuning curve discrete_tuning_curves = nap . compute_discrete_tuning_curves ( v1_high_snr_units , dict_ori ) discrete_tuning_curves .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 950930985 950931458 950931533 950931727 950931751 950932696 0.0 29.995305 10.423757 1.829558 23.304928 2.848980 0.067511 45.0 44.840722 15.482074 3.244307 23.389656 3.430838 0.133236 90.0 33.685112 16.839225 1.825138 14.194774 3.084083 0.006661 135.0 17.431332 16.000099 4.793279 19.017840 3.659094 0.567092 180.0 33.585341 13.109074 1.971690 27.217315 4.336386 0.686095 225.0 45.734898 14.614385 4.782647 22.894093 3.843437 0.293087 270.0 38.394459 14.281353 1.731881 13.681856 2.804314 0.079933 315.0 19.910011 14.840918 4.216473 20.476204 2.930882 0.526226 Each column is a unit, and each row is the orientation of the stimulus in degrees. The values in the table represent the firing rate of the unit in Hz. Let's plot them! plt . figure ( figsize = ( 12 , 9 )) for i in range ( len ( v1_unit_IDs )): # loop over all unit IDs plt . subplot ( 2 , 3 , i + 1 ) # plot tuning curves in 2 rows and 3 columns plt . plot ( discrete_tuning_curves [ v1_unit_IDs [ i ]], 'o-' , color = 'k' , linewidth = 2 ) plt . xlabel ( 'Orientation (deg)' ) plt . ylabel ( 'Firing rate (Hz)' ) plt . title ( \"Unit %d \" % v1_unit_IDs [ i ], fontsize = 10 ) plt . subplots_adjust ( wspace = 0.5 , hspace = 1 , top = 0.4 ) Nice! Looks like some of these neurons have some degree of orientation tuning.","title":"1D Orientation Tuning Curves"},{"location":"io.allends/#2d-spatial-tuning-curves","text":"Neurons in V1 are known to have spatial tuning based on the location of the stimulus in space. The gabors stimuli are shown on various locations on the screen. The x-y location of stimuli can be found in the stimulus_presentations metadata. Here, we will go through how we can access this metadata and use it to calculate 2D spatial tuning curves. Let's first get a table of stimulus presentations for gabors to look at the metadata for this stimulus. # get gabors informatiom from stimulus_presentations metadata gabors_presentations = stimulus_presentations [ stimulus_presentations [ 'stimulus_name' ] == 'gabors' ] gabors_presentations .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } color contrast frame orientation phase size spatial_frequency start_time stimulus_block stimulus_name stop_time temporal_frequency x_position y_position duration stimulus_condition_id stimulus_presentation_id 1 null 0.8 null 45.0 [3644.93333333, 3644.93333333] [20.0, 20.0] 0.08 73.537433 0.0 gabors 73.770952 4.0 0.0 30.0 0.233519 1 2 null 0.8 null 0.0 [3644.93333333, 3644.93333333] [20.0, 20.0] 0.08 73.770952 0.0 gabors 74.021150 4.0 -30.0 -10.0 0.250199 2 3 null 0.8 null 45.0 [3644.93333333, 3644.93333333] [20.0, 20.0] 0.08 74.021150 0.0 gabors 74.271349 4.0 10.0 20.0 0.250199 3 4 null 0.8 null 0.0 [3644.93333333, 3644.93333333] [20.0, 20.0] 0.08 74.271349 0.0 gabors 74.521547 4.0 -40.0 -40.0 0.250199 4 5 null 0.8 null 90.0 [3644.93333333, 3644.93333333] [20.0, 20.0] 0.08 74.521547 0.0 gabors 74.771764 4.0 -10.0 -10.0 0.250216 5 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 3641 null 0.8 null 45.0 [3644.93333333, 3644.93333333] [20.0, 20.0] 0.08 984.281513 0.0 gabors 984.531719 4.0 30.0 -10.0 0.250206 192 3642 null 0.8 null 45.0 [3644.93333333, 3644.93333333] [20.0, 20.0] 0.08 984.531719 0.0 gabors 984.781925 4.0 -20.0 10.0 0.250206 39 3643 null 0.8 null 0.0 [3644.93333333, 3644.93333333] [20.0, 20.0] 0.08 984.781925 0.0 gabors 985.032131 4.0 -30.0 40.0 0.250206 186 3644 null 0.8 null 45.0 [3644.93333333, 3644.93333333] [20.0, 20.0] 0.08 985.032131 0.0 gabors 985.282337 4.0 10.0 -30.0 0.250206 232 3645 null 0.8 null 45.0 [3644.93333333, 3644.93333333] [20.0, 20.0] 0.08 985.282337 0.0 gabors 985.532551 4.0 -40.0 40.0 0.250214 153 3645 rows \u00d7 16 columns Now, we need to extract the x- and y-position from the metadata and load it into a TsdFrame . # extract x- and y-position columns x_positions = np . array ( gabors_presentations [ 'x_position' ]) . tolist () y_positions = np . array ( gabors_presentations [ 'y_position' ]) . tolist () # load into x and y positions into a vstack xy_positions = np . vstack (( x_positions , y_positions )) . T # what does this look like? xy_positions array([[ 0., 30.], [-30., -10.], [ 10., 20.], ..., [-30., 40.], [ 10., -30.], [-40., 40.]]) Now let's load the xy positions into a TsdFrame . Since we will use the absolute start times as the time index (see below), we don't need to provide additional time support. When no time support is passed, the default global time support is used. # get start time as start time of the stimulus presentations time_index = ( gabors_presentations [ 'start_time' ]) . values # load TsdFrame xy_features = nap . TsdFrame ( t = time_index , d = xy_positions , time_units = \"s\" , columns = [ 'x' , 'y' ]) # what does this look like? xy_features .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x y Time (s) 73.537433 0.0 30.0 73.770952 -30.0 -10.0 74.021150 10.0 20.0 74.271349 -40.0 -40.0 74.521547 -10.0 -10.0 ... ... ... 984.281513 30.0 -10.0 984.531719 -20.0 10.0 984.781925 -30.0 40.0 985.032131 10.0 -30.0 985.282337 -40.0 40.0 3645 rows \u00d7 2 columns Great! Almost there. To compute the 2D tuning curve, it just takes one line with Pynapple! # using bin size of 9 since x-y stimulus is shown in a 9x9 grid (-40,-30,-20,-10,0,10,20,30,40) spatial_tuning_curve , binsxy = nap . compute_2d_tuning_curves ( group = v1_high_snr_units , feature = xy_features , nb_bins = 9 ) spatial_tuning_curve {950930985: array([[39.71176567, 44.15379762, 44.7756821 , 42.82118804, 39.00104056, 41.13321589, 43.70959443, 40.60017206, 37.40190905], [35.62509627, 45.04220401, 47.79626383, 45.21988529, 43.44307251, 40.2448095 , 37.57959033, 36.33582138, 40.15596886], [41.31089717, 45.13104465, 46.55249488, 44.15379762, 41.75510037, 43.53191315, 35.26973372, 38.645678 , 39.97828759], [37.49074969, 44.7756821 , 45.39756657, 39.97828759, 41.13321589, 38.29031544, 39.71176567, 40.51133142, 39.35640311], [45.39756657, 39.88944695, 37.31306841, 41.31089717, 36.86886522, 38.11263417, 41.66625973, 39.53408439, 37.49074969], [45.75292913, 38.82335928, 39.00104056, 40.2448095 , 41.75510037, 39.26756247, 39.0898812 , 42.64350676, 45.39756657], [41.93278165, 44.06495698, 44.15379762, 43.35423187, 39.71176567, 35.09205244, 39.44524375, 42.99886932, 38.46799672], [41.04437526, 42.82118804, 44.15379762, 40.60017206, 42.37698484, 36.24698075, 36.42466202, 40.60017206, 39.0898812 ], [40.06712823, 42.91002868, 43.44307251, 41.66625973, 38.2014748 , 36.51350266, 38.82335928, 37.13538714, 39.80060631]]), 950931458: array([[12.08232692, 11.19392052, 11.72696436, 13.32609586, 11.99348628, 11.63812372, 12.88189267, 10.39435477, 12.88189267], [12.70421139, 13.4149365 , 14.21450225, 12.79305203, 12.26000819, 14.48102417, 11.72696436, 14.21450225, 13.68145842], [13.68145842, 15.28058992, 16.52435887, 14.83638673, 15.014068 , 13.14841458, 11.72696436, 12.61537075, 13.14841458], [13.77029906, 17.0574027 , 20.25566571, 21.05523146, 15.4582712 , 14.39218353, 13.32609586, 13.14841458, 11.815805 ], [15.4582712 , 18.92305612, 23.72045063, 27.45175748, 17.32392462, 15.19174928, 11.28276116, 13.59261778, 13.59261778], [12.52653011, 16.34667759, 23.09856616, 22.83204424, 14.56986481, 13.05957394, 10.21667349, 12.88189267, 12.97073331], [12.70421139, 13.77029906, 14.83638673, 13.14841458, 13.23725522, 14.56986481, 11.46044244, 12.43768947, 14.39218353], [14.30334289, 13.68145842, 11.46044244, 13.32609586, 12.43768947, 11.63812372, 14.21450225, 12.52653011, 11.63812372], [12.79305203, 13.14841458, 12.34884883, 12.08232692, 12.61537075, 11.72696436, 12.17116755, 10.74971733, 12.79305203]]), 950931533: array([[1.15492831, 0.97724703, 0.97724703, 1.5991315 , 1.33260959, 1.06608767, 0.88840639, 0.71072511, 1.06608767], [1.06608767, 2.22101598, 2.57637853, 2.22101598, 1.77681278, 0.79956575, 0.97724703, 0.71072511, 0.4442032 ], [1.33260959, 3.19826301, 4.0866694 , 3.90898812, 2.30985662, 1.51029086, 0.4442032 , 1.33260959, 1.24376895], [0.71072511, 2.39869726, 4.61971323, 4.53087259, 1.86565342, 1.15492831, 1.51029086, 1.06608767, 0.88840639], [0.97724703, 1.5991315 , 2.66521917, 3.28710365, 0.53304383, 1.42145023, 0.79956575, 2.30985662, 1.42145023], [0.62188447, 1.15492831, 1.42145023, 1.51029086, 1.42145023, 1.24376895, 1.06608767, 0.71072511, 0.71072511], [1.51029086, 1.42145023, 0.97724703, 1.15492831, 1.15492831, 1.15492831, 1.86565342, 1.33260959, 1.42145023], [0.88840639, 0.71072511, 1.68797214, 1.42145023, 0.88840639, 0.26652192, 1.5991315 , 0.97724703, 0.53304383], [1.06608767, 1.06608767, 1.95449406, 1.51029086, 0.71072511, 1.5991315 , 0.97724703, 0.62188447, 1.24376895]]), 950931727: array([[16.52435887, 15.10290864, 15.99131503, 15.014068 , 14.83638673, 16.25783695, 15.99131503, 14.03682097, 16.16899631], [16.25783695, 14.74754609, 16.79088079, 17.14624334, 18.12349037, 18.92305612, 15.9024744 , 15.10290864, 16.79088079], [16.25783695, 20.16682507, 24.7865383 , 21.5882753 , 19.90030315, 15.36943056, 15.9024744 , 14.21450225, 18.12349037], [16.16899631, 29.49509217, 33.58176157, 27.89596067, 17.32392462, 16.43551823, 16.96856206, 16.16899631, 15.63595248], [20.52218763, 34.02596477, 31.3607456 , 31.89378943, 19.5449406 , 18.74537485, 15.4582712 , 14.65870545, 16.25783695], [21.41059402, 32.33799263, 36.33582138, 35.80277755, 20.78870954, 15.4582712 , 13.59261778, 17.59044654, 20.25566571], [17.94580909, 28.60668578, 33.22639902, 29.76161409, 20.96639082, 17.32392462, 18.03464973, 17.0574027 , 17.32392462], [18.39001229, 21.76595657, 25.05306022, 21.5882753 , 17.85696846, 15.99131503, 18.39001229, 16.61319951, 16.52435887], [17.59044654, 18.39001229, 22.38784105, 21.85479721, 18.30117165, 16.08015567, 16.79088079, 14.39218353, 16.25783695]]), 950931751: array([[3.10942237, 1.06608767, 1.51029086, 1.33260959, 2.22101598, 2.75405981, 1.5991315 , 1.24376895, 1.5991315 ], [2.57637853, 2.66521917, 2.93174109, 2.84290045, 2.84290045, 2.75405981, 2.22101598, 1.5991315 , 2.0433347 ], [1.51029086, 5.06391643, 6.21884474, 3.90898812, 1.86565342, 1.42145023, 1.51029086, 1.68797214, 1.95449406], [1.68797214, 7.37377304, 8.1733388 , 5.33043834, 3.02058173, 1.77681278, 1.42145023, 1.77681278, 2.30985662], [3.37594429, 7.64029496, 8.43986071, 4.88623515, 2.84290045, 4.17551004, 2.13217534, 2.0433347 , 2.57637853], [2.0433347 , 4.97507579, 7.55145432, 2.66521917, 1.95449406, 1.86565342, 1.5991315 , 2.30985662, 2.57637853], [1.95449406, 2.57637853, 3.99782876, 2.57637853, 2.22101598, 2.75405981, 2.13217534, 2.39869726, 1.68797214], [1.86565342, 3.82014748, 2.57637853, 1.86565342, 1.15492831, 1.86565342, 2.93174109, 1.06608767, 2.30985662], [2.75405981, 1.77681278, 1.68797214, 2.0433347 , 2.30985662, 2.0433347 , 1.68797214, 1.95449406, 2.30985662]]), 950932696: array([[0. , 0. , 0. , 0.08884064, 0. , 0. , 0.08884064, 0. , 0.08884064], [0.08884064, 0.08884064, 0.08884064, 0. , 0.17768128, 0. , 0.08884064, 0. , 0.08884064], [0.08884064, 0. , 0. , 0.17768128, 0. , 0. , 0. , 0. , 0.08884064], [0. , 0. , 0.17768128, 0.17768128, 0.08884064, 0. , 0.08884064, 0. , 0.35536256], [0. , 0. , 0.08884064, 0.17768128, 0.08884064, 0. , 0. , 0. , 0.17768128], [0.08884064, 0. , 0.35536256, 0. , 0.17768128, 0.26652192, 0.08884064, 0. , 0.26652192], [0. , 0.08884064, 0.08884064, 0.17768128, 0.26652192, 0. , 0.08884064, 0.08884064, 0.08884064], [0. , 0. , 0. , 0. , 0.26652192, 0.08884064, 0.08884064, 0.26652192, 0. ], [0.08884064, 0.08884064, 0.26652192, 0.17768128, 0.17768128, 0.08884064, 0.17768128, 0. , 0. ]])} Now we have calculated the 2D spatial tuning curve for our neurons in V1! Let's plot it with a heat map. #set the x and y axis limits extent = [ - 40 , 40 , - 40 , 40 ] # Create a 2x3 grid of subplots fig , axs = plt . subplots ( 2 , 3 , figsize = ( 12 , 8 )) # Iterate over each unit and plot the heatmap in a subplot for i , unit in enumerate ( v1_unit_IDs ): values = spatial_tuning_curve [ unit ] # Select the current subplot ax = axs [ i // 3 , i % 3 ] # Create the heatmap using seaborn with adjusted aspect ratio ax . imshow ( spatial_tuning_curve [ unit ], cmap = 'jet' , extent = extent , vmin = np . min ( values ), vmax = np . max ( values )) # Set the title and axis labels ax . set_title ( f 'Unit: { unit } ' ) ax . set_xlabel ( 'X-position' ) ax . set_ylabel ( 'Y-postition' ) # Adjust the spacing between subplots plt . tight_layout () # Display the plot plt . show () Awesome! Looks like some of our units show a great deal of spatial tuning.","title":"2D Spatial Tuning Curves"},{"location":"io.cnmfe/","text":"Loaders for calcium imaging data with miniscope. Support CNMF-E in matlab, inscopix-cnmfe and minian. CNMF_E ( BaseLoader ) Loader for data processed with matlab CNMF-E(https://github.com/zhoupc/CNMF_E). The path folder should contain a file ending in .mat when calling Source2d.save_neurons Attributes: Name Type Description A numpy.ndarray Spatial footprints C TsdFrame The calcium transients sampling_rate float Sampling rate of the data (default is 30 Hz). Source code in pynapple/io/cnmfe.py class CNMF_E ( BaseLoader ): \"\"\"Loader for data processed with matlab CNMF-E(https://github.com/zhoupc/CNMF_E). The path folder should contain a file ending in .mat when calling Source2d.save_neurons Attributes ---------- A : numpy.ndarray Spatial footprints C : TsdFrame The calcium transients sampling_rate : float Sampling rate of the data (default is 30 Hz). \"\"\" def __init__ ( self , path ): \"\"\" Parameters ---------- path : str The path to the data. \"\"\" self . basename = os . path . basename ( path ) super () . __init__ ( path ) # Need to check if nwb file exists and if data are there loading_my_data = True if self . path is not None : nwb_path = os . path . join ( self . path , \"pynapplenwb\" ) if os . path . exists ( nwb_path ): files = os . listdir ( nwb_path ) if len ([ f for f in files if f . endswith ( \".nwb\" )]): success = self . load_cnmfe_nwb ( path ) if success : loading_my_data = False # Bypass if data have already been transfered to nwb if loading_my_data : app = App () window = OphysGUI ( app , path = path ) app . mainloop () try : app . update () except Exception : pass if window . status : self . ophys_information = window . ophys_information self . load_cnmf_e ( path ) self . save_cnmfe_nwb ( path ) def load_cnmf_e ( self , path ): \"\"\" Load the calcium transients and the spatial footprints. Parameters ---------- path : str Path to the session \"\"\" files = os . listdir ( path ) matfiles = [ f for f in files if f . endswith ( \".mat\" )] if len ( matfiles ): data = loadmat ( os . path . join ( path , matfiles [ 0 ]), struct_as_record = False ) else : raise RuntimeError ( \"No mat file found in {} \" . format ( path )) self . struct = data [ \"neuron_results\" ][ 0 ][ 0 ] C = self . struct . C . T self . A = self . struct . A . T self . sampling_rate = float ( self . ophys_information [ \"ImagingPlane\" ][ \"imaging_rate\" ] ) time_index = np . arange ( 0 , len ( C )) / self . sampling_rate self . C = nap . TsdFrame ( t = time_index , d = C ) return None def save_cnmfe_nwb ( self , path ): \"\"\" Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field. Parameters ---------- path : TYPE Description \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () device_info = self . ophys_information [ \"device\" ] device = nwbfile . create_device ( name = device_info [ \"name\" ], description = device_info [ \"description\" ], manufacturer = device_info [ \"manufacturer\" ], ) optical_info = self . ophys_information [ \"OpticalChannel\" ] optical_info [ \"emission_lambda\" ] = float ( optical_info [ \"emission_lambda\" ]) optical_channel = OpticalChannel ( name = optical_info [ \"name\" ], description = optical_info [ \"description\" ], emission_lambda = optical_info [ \"emission_lambda\" ], ) imaging_info = self . ophys_information [ \"ImagingPlane\" ] imaging_info [ \"excitation_lambda\" ] = float ( imaging_info [ \"excitation_lambda\" ]) imaging_plane = nwbfile . create_imaging_plane ( name = imaging_info [ \"name\" ], optical_channel = optical_channel , imaging_rate = self . sampling_rate , description = imaging_info [ \"description\" ], device = device , excitation_lambda = imaging_info [ \"excitation_lambda\" ], indicator = imaging_info [ \"indicator\" ], location = imaging_info [ \"location\" ], ) ophys_module = nwbfile . create_processing_module ( name = \"ophys\" , description = \"optical physiology processed data\" ) seg_info = self . ophys_information [ \"PlaneSegmentation\" ] img_seg = ImageSegmentation () ps = img_seg . create_plane_segmentation ( name = seg_info [ \"name\" ], description = seg_info [ \"description\" ], imaging_plane = imaging_plane , ) for i in range ( self . C . shape [ 1 ]): image_mask = np . atleast_2d ( self . A [ i ]) # add image mask to plane segmentation ps . add_roi ( image_mask = image_mask ) ophys_module . add ( img_seg ) rt_region = ps . create_roi_table_region ( region = list ( np . arange ( self . C . shape [ 1 ])), description = \"ROIs\" ) roi_resp_series = RoiResponseSeries ( name = \"RoiResponseSeries\" , data = self . C . values , rois = rt_region , unit = \"lumens\" , timestamps = self . C . index . values , ) fl = Fluorescence ( roi_response_series = roi_resp_series ) ophys_module . add ( fl ) io . write ( nwbfile ) io . close () return def load_cnmfe_nwb ( self , path ): \"\"\" Load the calcium transient and spatial footprint from nwb Parameters ---------- path : str Path to the session \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r\" ) nwbfile = io . read () if \"ophys\" in nwbfile . processing . keys (): data = nwbfile . processing [ \"ophys\" ][ \"Fluorescence\" ][ \"RoiResponseSeries\" ] . data [:] t = nwbfile . processing [ \"ophys\" ][ \"Fluorescence\" ][ \"RoiResponseSeries\" ] . timestamps [:] self . C = nap . TsdFrame ( t = t , d = data ) self . A = nwbfile . processing [ \"ophys\" ][ \"ImageSegmentation\" ][ \"PlaneSegmentation\" ][ \"image_mask\" ] . data [:] io . close () return True else : io . close () return False __init__ ( self , path ) special Parameters: Name Type Description Default path str The path to the data. required Source code in pynapple/io/cnmfe.py def __init__ ( self , path ): \"\"\" Parameters ---------- path : str The path to the data. \"\"\" self . basename = os . path . basename ( path ) super () . __init__ ( path ) # Need to check if nwb file exists and if data are there loading_my_data = True if self . path is not None : nwb_path = os . path . join ( self . path , \"pynapplenwb\" ) if os . path . exists ( nwb_path ): files = os . listdir ( nwb_path ) if len ([ f for f in files if f . endswith ( \".nwb\" )]): success = self . load_cnmfe_nwb ( path ) if success : loading_my_data = False # Bypass if data have already been transfered to nwb if loading_my_data : app = App () window = OphysGUI ( app , path = path ) app . mainloop () try : app . update () except Exception : pass if window . status : self . ophys_information = window . ophys_information self . load_cnmf_e ( path ) self . save_cnmfe_nwb ( path ) load_cnmf_e ( self , path ) Load the calcium transients and the spatial footprints. Parameters: Name Type Description Default path str Path to the session required Source code in pynapple/io/cnmfe.py def load_cnmf_e ( self , path ): \"\"\" Load the calcium transients and the spatial footprints. Parameters ---------- path : str Path to the session \"\"\" files = os . listdir ( path ) matfiles = [ f for f in files if f . endswith ( \".mat\" )] if len ( matfiles ): data = loadmat ( os . path . join ( path , matfiles [ 0 ]), struct_as_record = False ) else : raise RuntimeError ( \"No mat file found in {} \" . format ( path )) self . struct = data [ \"neuron_results\" ][ 0 ][ 0 ] C = self . struct . C . T self . A = self . struct . A . T self . sampling_rate = float ( self . ophys_information [ \"ImagingPlane\" ][ \"imaging_rate\" ] ) time_index = np . arange ( 0 , len ( C )) / self . sampling_rate self . C = nap . TsdFrame ( t = time_index , d = C ) return None save_cnmfe_nwb ( self , path ) Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field. Parameters: Name Type Description Default path TYPE Description required Source code in pynapple/io/cnmfe.py def save_cnmfe_nwb ( self , path ): \"\"\" Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field. Parameters ---------- path : TYPE Description \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () device_info = self . ophys_information [ \"device\" ] device = nwbfile . create_device ( name = device_info [ \"name\" ], description = device_info [ \"description\" ], manufacturer = device_info [ \"manufacturer\" ], ) optical_info = self . ophys_information [ \"OpticalChannel\" ] optical_info [ \"emission_lambda\" ] = float ( optical_info [ \"emission_lambda\" ]) optical_channel = OpticalChannel ( name = optical_info [ \"name\" ], description = optical_info [ \"description\" ], emission_lambda = optical_info [ \"emission_lambda\" ], ) imaging_info = self . ophys_information [ \"ImagingPlane\" ] imaging_info [ \"excitation_lambda\" ] = float ( imaging_info [ \"excitation_lambda\" ]) imaging_plane = nwbfile . create_imaging_plane ( name = imaging_info [ \"name\" ], optical_channel = optical_channel , imaging_rate = self . sampling_rate , description = imaging_info [ \"description\" ], device = device , excitation_lambda = imaging_info [ \"excitation_lambda\" ], indicator = imaging_info [ \"indicator\" ], location = imaging_info [ \"location\" ], ) ophys_module = nwbfile . create_processing_module ( name = \"ophys\" , description = \"optical physiology processed data\" ) seg_info = self . ophys_information [ \"PlaneSegmentation\" ] img_seg = ImageSegmentation () ps = img_seg . create_plane_segmentation ( name = seg_info [ \"name\" ], description = seg_info [ \"description\" ], imaging_plane = imaging_plane , ) for i in range ( self . C . shape [ 1 ]): image_mask = np . atleast_2d ( self . A [ i ]) # add image mask to plane segmentation ps . add_roi ( image_mask = image_mask ) ophys_module . add ( img_seg ) rt_region = ps . create_roi_table_region ( region = list ( np . arange ( self . C . shape [ 1 ])), description = \"ROIs\" ) roi_resp_series = RoiResponseSeries ( name = \"RoiResponseSeries\" , data = self . C . values , rois = rt_region , unit = \"lumens\" , timestamps = self . C . index . values , ) fl = Fluorescence ( roi_response_series = roi_resp_series ) ophys_module . add ( fl ) io . write ( nwbfile ) io . close () return load_cnmfe_nwb ( self , path ) Load the calcium transient and spatial footprint from nwb Parameters: Name Type Description Default path str Path to the session required Source code in pynapple/io/cnmfe.py def load_cnmfe_nwb ( self , path ): \"\"\" Load the calcium transient and spatial footprint from nwb Parameters ---------- path : str Path to the session \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r\" ) nwbfile = io . read () if \"ophys\" in nwbfile . processing . keys (): data = nwbfile . processing [ \"ophys\" ][ \"Fluorescence\" ][ \"RoiResponseSeries\" ] . data [:] t = nwbfile . processing [ \"ophys\" ][ \"Fluorescence\" ][ \"RoiResponseSeries\" ] . timestamps [:] self . C = nap . TsdFrame ( t = t , d = data ) self . A = nwbfile . processing [ \"ophys\" ][ \"ImageSegmentation\" ][ \"PlaneSegmentation\" ][ \"image_mask\" ] . data [:] io . close () return True else : io . close () return False Minian ( BaseLoader ) Loader for data processed with Minian (https://github.com/denisecailab/minian). The path folder should contain a subfolder name minian. Attributes: Name Type Description A numpy.ndarray Spatial footprints C TsdFrame The calcium transients sampling_rate float Sampling rate of the data (default is 30 Hz). Source code in pynapple/io/cnmfe.py class Minian ( BaseLoader ): \"\"\"Loader for data processed with Minian (https://github.com/denisecailab/minian). The path folder should contain a subfolder name minian. Attributes ---------- A : numpy.ndarray Spatial footprints C : TsdFrame The calcium transients sampling_rate : float Sampling rate of the data (default is 30 Hz). \"\"\" def __init__ ( self , path ): \"\"\" Parameters ---------- path : str The path to the data. \"\"\" self . basename = os . path . basename ( path ) super () . __init__ ( path ) # Need to check if nwb file exists and if data are there loading_my_data = True if self . path is not None : nwb_path = os . path . join ( self . path , \"pynapplenwb\" ) if os . path . exists ( nwb_path ): files = os . listdir ( nwb_path ) if len ([ f for f in files if f . endswith ( \".nwb\" )]): success = self . load_cnmfe_nwb ( path ) if success : loading_my_data = False # Bypass if data have already been transfered to nwb if loading_my_data : app = App () window = OphysGUI ( app , path = path ) app . mainloop () try : app . update () except Exception : pass if window . status : self . ophys_information = window . ophys_information self . load_minian ( path ) self . save_cnmfe_nwb ( path ) def load_minian ( self , path ): \"\"\" Load the calcium transients and the spatial footprints. Parameters ---------- path : str Path to the session \"\"\" minian_folder = os . path . join ( path , \"minian\" ) if not os . path . exists ( minian_folder ): raise RuntimeError ( \"Path {} does not contain a minian folder\" . format ( path )) try : import zarr except ImportError as ie : print ( \"Please install module zarr for loading minian data\" , ie ) sys . exit () data = zarr . open ( minian_folder , \"r\" ) C = data [ \"C.zarr\" ][ \"C\" ][:] C = C . T self . sampling_rate = float ( self . ophys_information [ \"ImagingPlane\" ][ \"imaging_rate\" ] ) time_index = np . arange ( 0 , len ( C )) / self . sampling_rate self . C = nap . TsdFrame ( t = time_index , d = C ) self . A = data [ \"A.zarr\" ][ \"A\" ][:] return None def save_cnmfe_nwb ( self , path ): \"\"\" Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field. Parameters ---------- path : TYPE Description \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () device_info = self . ophys_information [ \"device\" ] device = nwbfile . create_device ( name = device_info [ \"name\" ], description = device_info [ \"description\" ], manufacturer = device_info [ \"manufacturer\" ], ) optical_info = self . ophys_information [ \"OpticalChannel\" ] optical_info [ \"emission_lambda\" ] = float ( optical_info [ \"emission_lambda\" ]) optical_channel = OpticalChannel ( name = optical_info [ \"name\" ], description = optical_info [ \"description\" ], emission_lambda = optical_info [ \"emission_lambda\" ], ) imaging_info = self . ophys_information [ \"ImagingPlane\" ] imaging_info [ \"excitation_lambda\" ] = float ( imaging_info [ \"excitation_lambda\" ]) imaging_plane = nwbfile . create_imaging_plane ( name = imaging_info [ \"name\" ], optical_channel = optical_channel , imaging_rate = self . sampling_rate , description = imaging_info [ \"description\" ], device = device , excitation_lambda = imaging_info [ \"excitation_lambda\" ], indicator = imaging_info [ \"indicator\" ], location = imaging_info [ \"location\" ], ) ophys_module = nwbfile . create_processing_module ( name = \"ophys\" , description = \"optical physiology processed data\" ) seg_info = self . ophys_information [ \"PlaneSegmentation\" ] img_seg = ImageSegmentation () ps = img_seg . create_plane_segmentation ( name = seg_info [ \"name\" ], description = seg_info [ \"description\" ], imaging_plane = imaging_plane , ) for i in range ( self . C . shape [ 1 ]): image_mask = self . A [ i ] # add image mask to plane segmentation ps . add_roi ( image_mask = image_mask ) ophys_module . add ( img_seg ) rt_region = ps . create_roi_table_region ( region = list ( np . arange ( self . C . shape [ 1 ])), description = \"ROIs\" ) roi_resp_series = RoiResponseSeries ( name = \"RoiResponseSeries\" , data = self . C . values , rois = rt_region , unit = \"lumens\" , timestamps = self . C . index . values , ) fl = Fluorescence ( roi_response_series = roi_resp_series ) ophys_module . add ( fl ) io . write ( nwbfile ) io . close () return def load_cnmfe_nwb ( self , path ): \"\"\" Load the calcium transient and spatial footprint from nwb Parameters ---------- path : str Path to the session \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r\" ) nwbfile = io . read () if \"ophys\" in nwbfile . processing . keys (): data = nwbfile . processing [ \"ophys\" ][ \"Fluorescence\" ][ \"RoiResponseSeries\" ] . data [:] t = nwbfile . processing [ \"ophys\" ][ \"Fluorescence\" ][ \"RoiResponseSeries\" ] . timestamps [:] self . C = nap . TsdFrame ( t = t , d = data ) self . A = nwbfile . processing [ \"ophys\" ][ \"ImageSegmentation\" ][ \"PlaneSegmentation\" ][ \"image_mask\" ] . data [:] io . close () return True else : io . close () return False __init__ ( self , path ) special Parameters: Name Type Description Default path str The path to the data. required Source code in pynapple/io/cnmfe.py def __init__ ( self , path ): \"\"\" Parameters ---------- path : str The path to the data. \"\"\" self . basename = os . path . basename ( path ) super () . __init__ ( path ) # Need to check if nwb file exists and if data are there loading_my_data = True if self . path is not None : nwb_path = os . path . join ( self . path , \"pynapplenwb\" ) if os . path . exists ( nwb_path ): files = os . listdir ( nwb_path ) if len ([ f for f in files if f . endswith ( \".nwb\" )]): success = self . load_cnmfe_nwb ( path ) if success : loading_my_data = False # Bypass if data have already been transfered to nwb if loading_my_data : app = App () window = OphysGUI ( app , path = path ) app . mainloop () try : app . update () except Exception : pass if window . status : self . ophys_information = window . ophys_information self . load_minian ( path ) self . save_cnmfe_nwb ( path ) load_minian ( self , path ) Load the calcium transients and the spatial footprints. Parameters: Name Type Description Default path str Path to the session required Source code in pynapple/io/cnmfe.py def load_minian ( self , path ): \"\"\" Load the calcium transients and the spatial footprints. Parameters ---------- path : str Path to the session \"\"\" minian_folder = os . path . join ( path , \"minian\" ) if not os . path . exists ( minian_folder ): raise RuntimeError ( \"Path {} does not contain a minian folder\" . format ( path )) try : import zarr except ImportError as ie : print ( \"Please install module zarr for loading minian data\" , ie ) sys . exit () data = zarr . open ( minian_folder , \"r\" ) C = data [ \"C.zarr\" ][ \"C\" ][:] C = C . T self . sampling_rate = float ( self . ophys_information [ \"ImagingPlane\" ][ \"imaging_rate\" ] ) time_index = np . arange ( 0 , len ( C )) / self . sampling_rate self . C = nap . TsdFrame ( t = time_index , d = C ) self . A = data [ \"A.zarr\" ][ \"A\" ][:] return None save_cnmfe_nwb ( self , path ) Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field. Parameters: Name Type Description Default path TYPE Description required Source code in pynapple/io/cnmfe.py def save_cnmfe_nwb ( self , path ): \"\"\" Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field. Parameters ---------- path : TYPE Description \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () device_info = self . ophys_information [ \"device\" ] device = nwbfile . create_device ( name = device_info [ \"name\" ], description = device_info [ \"description\" ], manufacturer = device_info [ \"manufacturer\" ], ) optical_info = self . ophys_information [ \"OpticalChannel\" ] optical_info [ \"emission_lambda\" ] = float ( optical_info [ \"emission_lambda\" ]) optical_channel = OpticalChannel ( name = optical_info [ \"name\" ], description = optical_info [ \"description\" ], emission_lambda = optical_info [ \"emission_lambda\" ], ) imaging_info = self . ophys_information [ \"ImagingPlane\" ] imaging_info [ \"excitation_lambda\" ] = float ( imaging_info [ \"excitation_lambda\" ]) imaging_plane = nwbfile . create_imaging_plane ( name = imaging_info [ \"name\" ], optical_channel = optical_channel , imaging_rate = self . sampling_rate , description = imaging_info [ \"description\" ], device = device , excitation_lambda = imaging_info [ \"excitation_lambda\" ], indicator = imaging_info [ \"indicator\" ], location = imaging_info [ \"location\" ], ) ophys_module = nwbfile . create_processing_module ( name = \"ophys\" , description = \"optical physiology processed data\" ) seg_info = self . ophys_information [ \"PlaneSegmentation\" ] img_seg = ImageSegmentation () ps = img_seg . create_plane_segmentation ( name = seg_info [ \"name\" ], description = seg_info [ \"description\" ], imaging_plane = imaging_plane , ) for i in range ( self . C . shape [ 1 ]): image_mask = self . A [ i ] # add image mask to plane segmentation ps . add_roi ( image_mask = image_mask ) ophys_module . add ( img_seg ) rt_region = ps . create_roi_table_region ( region = list ( np . arange ( self . C . shape [ 1 ])), description = \"ROIs\" ) roi_resp_series = RoiResponseSeries ( name = \"RoiResponseSeries\" , data = self . C . values , rois = rt_region , unit = \"lumens\" , timestamps = self . C . index . values , ) fl = Fluorescence ( roi_response_series = roi_resp_series ) ophys_module . add ( fl ) io . write ( nwbfile ) io . close () return load_cnmfe_nwb ( self , path ) Load the calcium transient and spatial footprint from nwb Parameters: Name Type Description Default path str Path to the session required Source code in pynapple/io/cnmfe.py def load_cnmfe_nwb ( self , path ): \"\"\" Load the calcium transient and spatial footprint from nwb Parameters ---------- path : str Path to the session \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r\" ) nwbfile = io . read () if \"ophys\" in nwbfile . processing . keys (): data = nwbfile . processing [ \"ophys\" ][ \"Fluorescence\" ][ \"RoiResponseSeries\" ] . data [:] t = nwbfile . processing [ \"ophys\" ][ \"Fluorescence\" ][ \"RoiResponseSeries\" ] . timestamps [:] self . C = nap . TsdFrame ( t = t , d = data ) self . A = nwbfile . processing [ \"ophys\" ][ \"ImageSegmentation\" ][ \"PlaneSegmentation\" ][ \"image_mask\" ] . data [:] io . close () return True else : io . close () return False InscopixCNMFE ( BaseLoader ) Loader for Inscopix-cnmfe (https://github.com/inscopix/inscopix-cnmfe). The folder should contain a file ending with '_traces.csv' and a tiff file for spatial footprints. Attributes: Name Type Description A np.ndarray The spatial footprints C TsdFrame The calcium transients sampling_rate float Sampling rate of the data (default is 30 Hz). Source code in pynapple/io/cnmfe.py class InscopixCNMFE ( BaseLoader ): \"\"\"Loader for Inscopix-cnmfe (https://github.com/inscopix/inscopix-cnmfe). The folder should contain a file ending with '_traces.csv' and a tiff file for spatial footprints. Attributes ---------- A : np.ndarray The spatial footprints C : TsdFrame The calcium transients sampling_rate : float Sampling rate of the data (default is 30 Hz). \"\"\" def __init__ ( self , path ): \"\"\" Parameters ---------- path : str The path to the data. \"\"\" self . basename = os . path . basename ( path ) super () . __init__ ( path ) # Need to check if nwb file exists and if data are there loading_my_data = True if self . path is not None : nwb_path = os . path . join ( self . path , \"pynapplenwb\" ) if os . path . exists ( nwb_path ): files = os . listdir ( nwb_path ) if len ([ f for f in files if f . endswith ( \".nwb\" )]): success = self . load_cnmfe_nwb ( path ) if success : loading_my_data = False # Bypass if data have already been transfered to nwb if loading_my_data : app = App () window = OphysGUI ( app , path = path ) app . mainloop () try : app . update () except Exception : pass if window . status : self . ophys_information = window . ophys_information self . load_inscopix_cnmfe ( path ) self . save_cnmfe_nwb ( path ) def load_inscopix_cnmfe ( self , path ): \"\"\" Load the calcium transients and the spatial footprints. Parameters ---------- path : str Path to the session \"\"\" files = os . listdir ( path ) tracefile = [ f for f in files if f . endswith ( \"_traces.csv\" )] if len ( tracefile ): C = pd . read_csv ( os . path . join ( path , tracefile [ 0 ]), index_col = 0 ) else : raise RuntimeError ( \"Path {} does not contain the file {} \" . format ( path , \"*_traces.csv\" ) ) self . sampling_rate = float ( self . ophys_information [ \"ImagingPlane\" ][ \"imaging_rate\" ] ) time_index = np . arange ( 0 , len ( C )) / self . sampling_rate self . C = nap . TsdFrame ( t = time_index , d = C . values ) try : import tifffile as tiff except ImportError as ie : print ( \"Please install module tifffile for loading inscopix-cnmfe data\" , ie ) sys . exit () tifffile = [ f for f in files if f . endswith ( \".tiff\" )] if len ( tifffile ): self . A = tiff . imread ( os . path . join ( path , tifffile [ 0 ])) else : raise RuntimeError ( \"Path {} does not contain the file {} \" . format ( path , \"*.tiff\" ) ) return None def save_cnmfe_nwb ( self , path ): \"\"\" Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field. Parameters ---------- path : TYPE Description \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () device_info = self . ophys_information [ \"device\" ] device = nwbfile . create_device ( name = device_info [ \"name\" ], description = device_info [ \"description\" ], manufacturer = device_info [ \"manufacturer\" ], ) optical_info = self . ophys_information [ \"OpticalChannel\" ] optical_info [ \"emission_lambda\" ] = float ( optical_info [ \"emission_lambda\" ]) optical_channel = OpticalChannel ( name = optical_info [ \"name\" ], description = optical_info [ \"description\" ], emission_lambda = optical_info [ \"emission_lambda\" ], ) imaging_info = self . ophys_information [ \"ImagingPlane\" ] imaging_info [ \"excitation_lambda\" ] = float ( imaging_info [ \"excitation_lambda\" ]) imaging_plane = nwbfile . create_imaging_plane ( name = imaging_info [ \"name\" ], optical_channel = optical_channel , imaging_rate = self . sampling_rate , description = imaging_info [ \"description\" ], device = device , excitation_lambda = imaging_info [ \"excitation_lambda\" ], indicator = imaging_info [ \"indicator\" ], location = imaging_info [ \"location\" ], ) ophys_module = nwbfile . create_processing_module ( name = \"ophys\" , description = \"optical physiology processed data\" ) seg_info = self . ophys_information [ \"PlaneSegmentation\" ] img_seg = ImageSegmentation () ps = img_seg . create_plane_segmentation ( name = seg_info [ \"name\" ], description = seg_info [ \"description\" ], imaging_plane = imaging_plane , ) for i in range ( self . C . shape [ 1 ]): image_mask = self . A [ i ] # add image mask to plane segmentation ps . add_roi ( image_mask = image_mask ) ophys_module . add ( img_seg ) rt_region = ps . create_roi_table_region ( region = list ( np . arange ( self . C . shape [ 1 ])), description = \"ROIs\" ) roi_resp_series = RoiResponseSeries ( name = \"RoiResponseSeries\" , data = self . C . values , rois = rt_region , unit = \"lumens\" , timestamps = self . C . index . values , ) fl = Fluorescence ( roi_response_series = roi_resp_series ) ophys_module . add ( fl ) io . write ( nwbfile ) io . close () return def load_cnmfe_nwb ( self , path ): \"\"\" Load the calcium transient and spatial footprint from nwb Parameters ---------- path : str Path to the session \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r\" ) nwbfile = io . read () if \"ophys\" in nwbfile . processing . keys (): data = nwbfile . processing [ \"ophys\" ][ \"Fluorescence\" ][ \"RoiResponseSeries\" ] . data [:] t = nwbfile . processing [ \"ophys\" ][ \"Fluorescence\" ][ \"RoiResponseSeries\" ] . timestamps [:] self . C = nap . TsdFrame ( t = t , d = data ) self . A = nwbfile . processing [ \"ophys\" ][ \"ImageSegmentation\" ][ \"PlaneSegmentation\" ][ \"image_mask\" ] . data [:] io . close () return True else : io . close () return False __init__ ( self , path ) special Parameters: Name Type Description Default path str The path to the data. required Source code in pynapple/io/cnmfe.py def __init__ ( self , path ): \"\"\" Parameters ---------- path : str The path to the data. \"\"\" self . basename = os . path . basename ( path ) super () . __init__ ( path ) # Need to check if nwb file exists and if data are there loading_my_data = True if self . path is not None : nwb_path = os . path . join ( self . path , \"pynapplenwb\" ) if os . path . exists ( nwb_path ): files = os . listdir ( nwb_path ) if len ([ f for f in files if f . endswith ( \".nwb\" )]): success = self . load_cnmfe_nwb ( path ) if success : loading_my_data = False # Bypass if data have already been transfered to nwb if loading_my_data : app = App () window = OphysGUI ( app , path = path ) app . mainloop () try : app . update () except Exception : pass if window . status : self . ophys_information = window . ophys_information self . load_inscopix_cnmfe ( path ) self . save_cnmfe_nwb ( path ) load_inscopix_cnmfe ( self , path ) Load the calcium transients and the spatial footprints. Parameters: Name Type Description Default path str Path to the session required Source code in pynapple/io/cnmfe.py def load_inscopix_cnmfe ( self , path ): \"\"\" Load the calcium transients and the spatial footprints. Parameters ---------- path : str Path to the session \"\"\" files = os . listdir ( path ) tracefile = [ f for f in files if f . endswith ( \"_traces.csv\" )] if len ( tracefile ): C = pd . read_csv ( os . path . join ( path , tracefile [ 0 ]), index_col = 0 ) else : raise RuntimeError ( \"Path {} does not contain the file {} \" . format ( path , \"*_traces.csv\" ) ) self . sampling_rate = float ( self . ophys_information [ \"ImagingPlane\" ][ \"imaging_rate\" ] ) time_index = np . arange ( 0 , len ( C )) / self . sampling_rate self . C = nap . TsdFrame ( t = time_index , d = C . values ) try : import tifffile as tiff except ImportError as ie : print ( \"Please install module tifffile for loading inscopix-cnmfe data\" , ie ) sys . exit () tifffile = [ f for f in files if f . endswith ( \".tiff\" )] if len ( tifffile ): self . A = tiff . imread ( os . path . join ( path , tifffile [ 0 ])) else : raise RuntimeError ( \"Path {} does not contain the file {} \" . format ( path , \"*.tiff\" ) ) return None save_cnmfe_nwb ( self , path ) Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field. Parameters: Name Type Description Default path TYPE Description required Source code in pynapple/io/cnmfe.py def save_cnmfe_nwb ( self , path ): \"\"\" Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field. Parameters ---------- path : TYPE Description \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () device_info = self . ophys_information [ \"device\" ] device = nwbfile . create_device ( name = device_info [ \"name\" ], description = device_info [ \"description\" ], manufacturer = device_info [ \"manufacturer\" ], ) optical_info = self . ophys_information [ \"OpticalChannel\" ] optical_info [ \"emission_lambda\" ] = float ( optical_info [ \"emission_lambda\" ]) optical_channel = OpticalChannel ( name = optical_info [ \"name\" ], description = optical_info [ \"description\" ], emission_lambda = optical_info [ \"emission_lambda\" ], ) imaging_info = self . ophys_information [ \"ImagingPlane\" ] imaging_info [ \"excitation_lambda\" ] = float ( imaging_info [ \"excitation_lambda\" ]) imaging_plane = nwbfile . create_imaging_plane ( name = imaging_info [ \"name\" ], optical_channel = optical_channel , imaging_rate = self . sampling_rate , description = imaging_info [ \"description\" ], device = device , excitation_lambda = imaging_info [ \"excitation_lambda\" ], indicator = imaging_info [ \"indicator\" ], location = imaging_info [ \"location\" ], ) ophys_module = nwbfile . create_processing_module ( name = \"ophys\" , description = \"optical physiology processed data\" ) seg_info = self . ophys_information [ \"PlaneSegmentation\" ] img_seg = ImageSegmentation () ps = img_seg . create_plane_segmentation ( name = seg_info [ \"name\" ], description = seg_info [ \"description\" ], imaging_plane = imaging_plane , ) for i in range ( self . C . shape [ 1 ]): image_mask = self . A [ i ] # add image mask to plane segmentation ps . add_roi ( image_mask = image_mask ) ophys_module . add ( img_seg ) rt_region = ps . create_roi_table_region ( region = list ( np . arange ( self . C . shape [ 1 ])), description = \"ROIs\" ) roi_resp_series = RoiResponseSeries ( name = \"RoiResponseSeries\" , data = self . C . values , rois = rt_region , unit = \"lumens\" , timestamps = self . C . index . values , ) fl = Fluorescence ( roi_response_series = roi_resp_series ) ophys_module . add ( fl ) io . write ( nwbfile ) io . close () return load_cnmfe_nwb ( self , path ) Load the calcium transient and spatial footprint from nwb Parameters: Name Type Description Default path str Path to the session required Source code in pynapple/io/cnmfe.py def load_cnmfe_nwb ( self , path ): \"\"\" Load the calcium transient and spatial footprint from nwb Parameters ---------- path : str Path to the session \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r\" ) nwbfile = io . read () if \"ophys\" in nwbfile . processing . keys (): data = nwbfile . processing [ \"ophys\" ][ \"Fluorescence\" ][ \"RoiResponseSeries\" ] . data [:] t = nwbfile . processing [ \"ophys\" ][ \"Fluorescence\" ][ \"RoiResponseSeries\" ] . timestamps [:] self . C = nap . TsdFrame ( t = t , d = data ) self . A = nwbfile . processing [ \"ophys\" ][ \"ImageSegmentation\" ][ \"PlaneSegmentation\" ][ \"image_mask\" ] . data [:] io . close () return True else : io . close () return False","title":"CNMF-E"},{"location":"io.cnmfe/#pynapple.io.cnmfe.CNMF_E","text":"Loader for data processed with matlab CNMF-E(https://github.com/zhoupc/CNMF_E). The path folder should contain a file ending in .mat when calling Source2d.save_neurons Attributes: Name Type Description A numpy.ndarray Spatial footprints C TsdFrame The calcium transients sampling_rate float Sampling rate of the data (default is 30 Hz). Source code in pynapple/io/cnmfe.py class CNMF_E ( BaseLoader ): \"\"\"Loader for data processed with matlab CNMF-E(https://github.com/zhoupc/CNMF_E). The path folder should contain a file ending in .mat when calling Source2d.save_neurons Attributes ---------- A : numpy.ndarray Spatial footprints C : TsdFrame The calcium transients sampling_rate : float Sampling rate of the data (default is 30 Hz). \"\"\" def __init__ ( self , path ): \"\"\" Parameters ---------- path : str The path to the data. \"\"\" self . basename = os . path . basename ( path ) super () . __init__ ( path ) # Need to check if nwb file exists and if data are there loading_my_data = True if self . path is not None : nwb_path = os . path . join ( self . path , \"pynapplenwb\" ) if os . path . exists ( nwb_path ): files = os . listdir ( nwb_path ) if len ([ f for f in files if f . endswith ( \".nwb\" )]): success = self . load_cnmfe_nwb ( path ) if success : loading_my_data = False # Bypass if data have already been transfered to nwb if loading_my_data : app = App () window = OphysGUI ( app , path = path ) app . mainloop () try : app . update () except Exception : pass if window . status : self . ophys_information = window . ophys_information self . load_cnmf_e ( path ) self . save_cnmfe_nwb ( path ) def load_cnmf_e ( self , path ): \"\"\" Load the calcium transients and the spatial footprints. Parameters ---------- path : str Path to the session \"\"\" files = os . listdir ( path ) matfiles = [ f for f in files if f . endswith ( \".mat\" )] if len ( matfiles ): data = loadmat ( os . path . join ( path , matfiles [ 0 ]), struct_as_record = False ) else : raise RuntimeError ( \"No mat file found in {} \" . format ( path )) self . struct = data [ \"neuron_results\" ][ 0 ][ 0 ] C = self . struct . C . T self . A = self . struct . A . T self . sampling_rate = float ( self . ophys_information [ \"ImagingPlane\" ][ \"imaging_rate\" ] ) time_index = np . arange ( 0 , len ( C )) / self . sampling_rate self . C = nap . TsdFrame ( t = time_index , d = C ) return None def save_cnmfe_nwb ( self , path ): \"\"\" Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field. Parameters ---------- path : TYPE Description \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () device_info = self . ophys_information [ \"device\" ] device = nwbfile . create_device ( name = device_info [ \"name\" ], description = device_info [ \"description\" ], manufacturer = device_info [ \"manufacturer\" ], ) optical_info = self . ophys_information [ \"OpticalChannel\" ] optical_info [ \"emission_lambda\" ] = float ( optical_info [ \"emission_lambda\" ]) optical_channel = OpticalChannel ( name = optical_info [ \"name\" ], description = optical_info [ \"description\" ], emission_lambda = optical_info [ \"emission_lambda\" ], ) imaging_info = self . ophys_information [ \"ImagingPlane\" ] imaging_info [ \"excitation_lambda\" ] = float ( imaging_info [ \"excitation_lambda\" ]) imaging_plane = nwbfile . create_imaging_plane ( name = imaging_info [ \"name\" ], optical_channel = optical_channel , imaging_rate = self . sampling_rate , description = imaging_info [ \"description\" ], device = device , excitation_lambda = imaging_info [ \"excitation_lambda\" ], indicator = imaging_info [ \"indicator\" ], location = imaging_info [ \"location\" ], ) ophys_module = nwbfile . create_processing_module ( name = \"ophys\" , description = \"optical physiology processed data\" ) seg_info = self . ophys_information [ \"PlaneSegmentation\" ] img_seg = ImageSegmentation () ps = img_seg . create_plane_segmentation ( name = seg_info [ \"name\" ], description = seg_info [ \"description\" ], imaging_plane = imaging_plane , ) for i in range ( self . C . shape [ 1 ]): image_mask = np . atleast_2d ( self . A [ i ]) # add image mask to plane segmentation ps . add_roi ( image_mask = image_mask ) ophys_module . add ( img_seg ) rt_region = ps . create_roi_table_region ( region = list ( np . arange ( self . C . shape [ 1 ])), description = \"ROIs\" ) roi_resp_series = RoiResponseSeries ( name = \"RoiResponseSeries\" , data = self . C . values , rois = rt_region , unit = \"lumens\" , timestamps = self . C . index . values , ) fl = Fluorescence ( roi_response_series = roi_resp_series ) ophys_module . add ( fl ) io . write ( nwbfile ) io . close () return def load_cnmfe_nwb ( self , path ): \"\"\" Load the calcium transient and spatial footprint from nwb Parameters ---------- path : str Path to the session \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r\" ) nwbfile = io . read () if \"ophys\" in nwbfile . processing . keys (): data = nwbfile . processing [ \"ophys\" ][ \"Fluorescence\" ][ \"RoiResponseSeries\" ] . data [:] t = nwbfile . processing [ \"ophys\" ][ \"Fluorescence\" ][ \"RoiResponseSeries\" ] . timestamps [:] self . C = nap . TsdFrame ( t = t , d = data ) self . A = nwbfile . processing [ \"ophys\" ][ \"ImageSegmentation\" ][ \"PlaneSegmentation\" ][ \"image_mask\" ] . data [:] io . close () return True else : io . close () return False","title":"CNMF_E"},{"location":"io.cnmfe/#pynapple.io.cnmfe.CNMF_E.__init__","text":"Parameters: Name Type Description Default path str The path to the data. required Source code in pynapple/io/cnmfe.py def __init__ ( self , path ): \"\"\" Parameters ---------- path : str The path to the data. \"\"\" self . basename = os . path . basename ( path ) super () . __init__ ( path ) # Need to check if nwb file exists and if data are there loading_my_data = True if self . path is not None : nwb_path = os . path . join ( self . path , \"pynapplenwb\" ) if os . path . exists ( nwb_path ): files = os . listdir ( nwb_path ) if len ([ f for f in files if f . endswith ( \".nwb\" )]): success = self . load_cnmfe_nwb ( path ) if success : loading_my_data = False # Bypass if data have already been transfered to nwb if loading_my_data : app = App () window = OphysGUI ( app , path = path ) app . mainloop () try : app . update () except Exception : pass if window . status : self . ophys_information = window . ophys_information self . load_cnmf_e ( path ) self . save_cnmfe_nwb ( path )","title":"__init__()"},{"location":"io.cnmfe/#pynapple.io.cnmfe.CNMF_E.load_cnmf_e","text":"Load the calcium transients and the spatial footprints. Parameters: Name Type Description Default path str Path to the session required Source code in pynapple/io/cnmfe.py def load_cnmf_e ( self , path ): \"\"\" Load the calcium transients and the spatial footprints. Parameters ---------- path : str Path to the session \"\"\" files = os . listdir ( path ) matfiles = [ f for f in files if f . endswith ( \".mat\" )] if len ( matfiles ): data = loadmat ( os . path . join ( path , matfiles [ 0 ]), struct_as_record = False ) else : raise RuntimeError ( \"No mat file found in {} \" . format ( path )) self . struct = data [ \"neuron_results\" ][ 0 ][ 0 ] C = self . struct . C . T self . A = self . struct . A . T self . sampling_rate = float ( self . ophys_information [ \"ImagingPlane\" ][ \"imaging_rate\" ] ) time_index = np . arange ( 0 , len ( C )) / self . sampling_rate self . C = nap . TsdFrame ( t = time_index , d = C ) return None","title":"load_cnmf_e()"},{"location":"io.cnmfe/#pynapple.io.cnmfe.CNMF_E.save_cnmfe_nwb","text":"Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field. Parameters: Name Type Description Default path TYPE Description required Source code in pynapple/io/cnmfe.py def save_cnmfe_nwb ( self , path ): \"\"\" Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field. Parameters ---------- path : TYPE Description \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () device_info = self . ophys_information [ \"device\" ] device = nwbfile . create_device ( name = device_info [ \"name\" ], description = device_info [ \"description\" ], manufacturer = device_info [ \"manufacturer\" ], ) optical_info = self . ophys_information [ \"OpticalChannel\" ] optical_info [ \"emission_lambda\" ] = float ( optical_info [ \"emission_lambda\" ]) optical_channel = OpticalChannel ( name = optical_info [ \"name\" ], description = optical_info [ \"description\" ], emission_lambda = optical_info [ \"emission_lambda\" ], ) imaging_info = self . ophys_information [ \"ImagingPlane\" ] imaging_info [ \"excitation_lambda\" ] = float ( imaging_info [ \"excitation_lambda\" ]) imaging_plane = nwbfile . create_imaging_plane ( name = imaging_info [ \"name\" ], optical_channel = optical_channel , imaging_rate = self . sampling_rate , description = imaging_info [ \"description\" ], device = device , excitation_lambda = imaging_info [ \"excitation_lambda\" ], indicator = imaging_info [ \"indicator\" ], location = imaging_info [ \"location\" ], ) ophys_module = nwbfile . create_processing_module ( name = \"ophys\" , description = \"optical physiology processed data\" ) seg_info = self . ophys_information [ \"PlaneSegmentation\" ] img_seg = ImageSegmentation () ps = img_seg . create_plane_segmentation ( name = seg_info [ \"name\" ], description = seg_info [ \"description\" ], imaging_plane = imaging_plane , ) for i in range ( self . C . shape [ 1 ]): image_mask = np . atleast_2d ( self . A [ i ]) # add image mask to plane segmentation ps . add_roi ( image_mask = image_mask ) ophys_module . add ( img_seg ) rt_region = ps . create_roi_table_region ( region = list ( np . arange ( self . C . shape [ 1 ])), description = \"ROIs\" ) roi_resp_series = RoiResponseSeries ( name = \"RoiResponseSeries\" , data = self . C . values , rois = rt_region , unit = \"lumens\" , timestamps = self . C . index . values , ) fl = Fluorescence ( roi_response_series = roi_resp_series ) ophys_module . add ( fl ) io . write ( nwbfile ) io . close () return","title":"save_cnmfe_nwb()"},{"location":"io.cnmfe/#pynapple.io.cnmfe.CNMF_E.load_cnmfe_nwb","text":"Load the calcium transient and spatial footprint from nwb Parameters: Name Type Description Default path str Path to the session required Source code in pynapple/io/cnmfe.py def load_cnmfe_nwb ( self , path ): \"\"\" Load the calcium transient and spatial footprint from nwb Parameters ---------- path : str Path to the session \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r\" ) nwbfile = io . read () if \"ophys\" in nwbfile . processing . keys (): data = nwbfile . processing [ \"ophys\" ][ \"Fluorescence\" ][ \"RoiResponseSeries\" ] . data [:] t = nwbfile . processing [ \"ophys\" ][ \"Fluorescence\" ][ \"RoiResponseSeries\" ] . timestamps [:] self . C = nap . TsdFrame ( t = t , d = data ) self . A = nwbfile . processing [ \"ophys\" ][ \"ImageSegmentation\" ][ \"PlaneSegmentation\" ][ \"image_mask\" ] . data [:] io . close () return True else : io . close () return False","title":"load_cnmfe_nwb()"},{"location":"io.cnmfe/#pynapple.io.cnmfe.Minian","text":"Loader for data processed with Minian (https://github.com/denisecailab/minian). The path folder should contain a subfolder name minian. Attributes: Name Type Description A numpy.ndarray Spatial footprints C TsdFrame The calcium transients sampling_rate float Sampling rate of the data (default is 30 Hz). Source code in pynapple/io/cnmfe.py class Minian ( BaseLoader ): \"\"\"Loader for data processed with Minian (https://github.com/denisecailab/minian). The path folder should contain a subfolder name minian. Attributes ---------- A : numpy.ndarray Spatial footprints C : TsdFrame The calcium transients sampling_rate : float Sampling rate of the data (default is 30 Hz). \"\"\" def __init__ ( self , path ): \"\"\" Parameters ---------- path : str The path to the data. \"\"\" self . basename = os . path . basename ( path ) super () . __init__ ( path ) # Need to check if nwb file exists and if data are there loading_my_data = True if self . path is not None : nwb_path = os . path . join ( self . path , \"pynapplenwb\" ) if os . path . exists ( nwb_path ): files = os . listdir ( nwb_path ) if len ([ f for f in files if f . endswith ( \".nwb\" )]): success = self . load_cnmfe_nwb ( path ) if success : loading_my_data = False # Bypass if data have already been transfered to nwb if loading_my_data : app = App () window = OphysGUI ( app , path = path ) app . mainloop () try : app . update () except Exception : pass if window . status : self . ophys_information = window . ophys_information self . load_minian ( path ) self . save_cnmfe_nwb ( path ) def load_minian ( self , path ): \"\"\" Load the calcium transients and the spatial footprints. Parameters ---------- path : str Path to the session \"\"\" minian_folder = os . path . join ( path , \"minian\" ) if not os . path . exists ( minian_folder ): raise RuntimeError ( \"Path {} does not contain a minian folder\" . format ( path )) try : import zarr except ImportError as ie : print ( \"Please install module zarr for loading minian data\" , ie ) sys . exit () data = zarr . open ( minian_folder , \"r\" ) C = data [ \"C.zarr\" ][ \"C\" ][:] C = C . T self . sampling_rate = float ( self . ophys_information [ \"ImagingPlane\" ][ \"imaging_rate\" ] ) time_index = np . arange ( 0 , len ( C )) / self . sampling_rate self . C = nap . TsdFrame ( t = time_index , d = C ) self . A = data [ \"A.zarr\" ][ \"A\" ][:] return None def save_cnmfe_nwb ( self , path ): \"\"\" Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field. Parameters ---------- path : TYPE Description \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () device_info = self . ophys_information [ \"device\" ] device = nwbfile . create_device ( name = device_info [ \"name\" ], description = device_info [ \"description\" ], manufacturer = device_info [ \"manufacturer\" ], ) optical_info = self . ophys_information [ \"OpticalChannel\" ] optical_info [ \"emission_lambda\" ] = float ( optical_info [ \"emission_lambda\" ]) optical_channel = OpticalChannel ( name = optical_info [ \"name\" ], description = optical_info [ \"description\" ], emission_lambda = optical_info [ \"emission_lambda\" ], ) imaging_info = self . ophys_information [ \"ImagingPlane\" ] imaging_info [ \"excitation_lambda\" ] = float ( imaging_info [ \"excitation_lambda\" ]) imaging_plane = nwbfile . create_imaging_plane ( name = imaging_info [ \"name\" ], optical_channel = optical_channel , imaging_rate = self . sampling_rate , description = imaging_info [ \"description\" ], device = device , excitation_lambda = imaging_info [ \"excitation_lambda\" ], indicator = imaging_info [ \"indicator\" ], location = imaging_info [ \"location\" ], ) ophys_module = nwbfile . create_processing_module ( name = \"ophys\" , description = \"optical physiology processed data\" ) seg_info = self . ophys_information [ \"PlaneSegmentation\" ] img_seg = ImageSegmentation () ps = img_seg . create_plane_segmentation ( name = seg_info [ \"name\" ], description = seg_info [ \"description\" ], imaging_plane = imaging_plane , ) for i in range ( self . C . shape [ 1 ]): image_mask = self . A [ i ] # add image mask to plane segmentation ps . add_roi ( image_mask = image_mask ) ophys_module . add ( img_seg ) rt_region = ps . create_roi_table_region ( region = list ( np . arange ( self . C . shape [ 1 ])), description = \"ROIs\" ) roi_resp_series = RoiResponseSeries ( name = \"RoiResponseSeries\" , data = self . C . values , rois = rt_region , unit = \"lumens\" , timestamps = self . C . index . values , ) fl = Fluorescence ( roi_response_series = roi_resp_series ) ophys_module . add ( fl ) io . write ( nwbfile ) io . close () return def load_cnmfe_nwb ( self , path ): \"\"\" Load the calcium transient and spatial footprint from nwb Parameters ---------- path : str Path to the session \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r\" ) nwbfile = io . read () if \"ophys\" in nwbfile . processing . keys (): data = nwbfile . processing [ \"ophys\" ][ \"Fluorescence\" ][ \"RoiResponseSeries\" ] . data [:] t = nwbfile . processing [ \"ophys\" ][ \"Fluorescence\" ][ \"RoiResponseSeries\" ] . timestamps [:] self . C = nap . TsdFrame ( t = t , d = data ) self . A = nwbfile . processing [ \"ophys\" ][ \"ImageSegmentation\" ][ \"PlaneSegmentation\" ][ \"image_mask\" ] . data [:] io . close () return True else : io . close () return False","title":"Minian"},{"location":"io.cnmfe/#pynapple.io.cnmfe.Minian.__init__","text":"Parameters: Name Type Description Default path str The path to the data. required Source code in pynapple/io/cnmfe.py def __init__ ( self , path ): \"\"\" Parameters ---------- path : str The path to the data. \"\"\" self . basename = os . path . basename ( path ) super () . __init__ ( path ) # Need to check if nwb file exists and if data are there loading_my_data = True if self . path is not None : nwb_path = os . path . join ( self . path , \"pynapplenwb\" ) if os . path . exists ( nwb_path ): files = os . listdir ( nwb_path ) if len ([ f for f in files if f . endswith ( \".nwb\" )]): success = self . load_cnmfe_nwb ( path ) if success : loading_my_data = False # Bypass if data have already been transfered to nwb if loading_my_data : app = App () window = OphysGUI ( app , path = path ) app . mainloop () try : app . update () except Exception : pass if window . status : self . ophys_information = window . ophys_information self . load_minian ( path ) self . save_cnmfe_nwb ( path )","title":"__init__()"},{"location":"io.cnmfe/#pynapple.io.cnmfe.Minian.load_minian","text":"Load the calcium transients and the spatial footprints. Parameters: Name Type Description Default path str Path to the session required Source code in pynapple/io/cnmfe.py def load_minian ( self , path ): \"\"\" Load the calcium transients and the spatial footprints. Parameters ---------- path : str Path to the session \"\"\" minian_folder = os . path . join ( path , \"minian\" ) if not os . path . exists ( minian_folder ): raise RuntimeError ( \"Path {} does not contain a minian folder\" . format ( path )) try : import zarr except ImportError as ie : print ( \"Please install module zarr for loading minian data\" , ie ) sys . exit () data = zarr . open ( minian_folder , \"r\" ) C = data [ \"C.zarr\" ][ \"C\" ][:] C = C . T self . sampling_rate = float ( self . ophys_information [ \"ImagingPlane\" ][ \"imaging_rate\" ] ) time_index = np . arange ( 0 , len ( C )) / self . sampling_rate self . C = nap . TsdFrame ( t = time_index , d = C ) self . A = data [ \"A.zarr\" ][ \"A\" ][:] return None","title":"load_minian()"},{"location":"io.cnmfe/#pynapple.io.cnmfe.Minian.save_cnmfe_nwb","text":"Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field. Parameters: Name Type Description Default path TYPE Description required Source code in pynapple/io/cnmfe.py def save_cnmfe_nwb ( self , path ): \"\"\" Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field. Parameters ---------- path : TYPE Description \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () device_info = self . ophys_information [ \"device\" ] device = nwbfile . create_device ( name = device_info [ \"name\" ], description = device_info [ \"description\" ], manufacturer = device_info [ \"manufacturer\" ], ) optical_info = self . ophys_information [ \"OpticalChannel\" ] optical_info [ \"emission_lambda\" ] = float ( optical_info [ \"emission_lambda\" ]) optical_channel = OpticalChannel ( name = optical_info [ \"name\" ], description = optical_info [ \"description\" ], emission_lambda = optical_info [ \"emission_lambda\" ], ) imaging_info = self . ophys_information [ \"ImagingPlane\" ] imaging_info [ \"excitation_lambda\" ] = float ( imaging_info [ \"excitation_lambda\" ]) imaging_plane = nwbfile . create_imaging_plane ( name = imaging_info [ \"name\" ], optical_channel = optical_channel , imaging_rate = self . sampling_rate , description = imaging_info [ \"description\" ], device = device , excitation_lambda = imaging_info [ \"excitation_lambda\" ], indicator = imaging_info [ \"indicator\" ], location = imaging_info [ \"location\" ], ) ophys_module = nwbfile . create_processing_module ( name = \"ophys\" , description = \"optical physiology processed data\" ) seg_info = self . ophys_information [ \"PlaneSegmentation\" ] img_seg = ImageSegmentation () ps = img_seg . create_plane_segmentation ( name = seg_info [ \"name\" ], description = seg_info [ \"description\" ], imaging_plane = imaging_plane , ) for i in range ( self . C . shape [ 1 ]): image_mask = self . A [ i ] # add image mask to plane segmentation ps . add_roi ( image_mask = image_mask ) ophys_module . add ( img_seg ) rt_region = ps . create_roi_table_region ( region = list ( np . arange ( self . C . shape [ 1 ])), description = \"ROIs\" ) roi_resp_series = RoiResponseSeries ( name = \"RoiResponseSeries\" , data = self . C . values , rois = rt_region , unit = \"lumens\" , timestamps = self . C . index . values , ) fl = Fluorescence ( roi_response_series = roi_resp_series ) ophys_module . add ( fl ) io . write ( nwbfile ) io . close () return","title":"save_cnmfe_nwb()"},{"location":"io.cnmfe/#pynapple.io.cnmfe.Minian.load_cnmfe_nwb","text":"Load the calcium transient and spatial footprint from nwb Parameters: Name Type Description Default path str Path to the session required Source code in pynapple/io/cnmfe.py def load_cnmfe_nwb ( self , path ): \"\"\" Load the calcium transient and spatial footprint from nwb Parameters ---------- path : str Path to the session \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r\" ) nwbfile = io . read () if \"ophys\" in nwbfile . processing . keys (): data = nwbfile . processing [ \"ophys\" ][ \"Fluorescence\" ][ \"RoiResponseSeries\" ] . data [:] t = nwbfile . processing [ \"ophys\" ][ \"Fluorescence\" ][ \"RoiResponseSeries\" ] . timestamps [:] self . C = nap . TsdFrame ( t = t , d = data ) self . A = nwbfile . processing [ \"ophys\" ][ \"ImageSegmentation\" ][ \"PlaneSegmentation\" ][ \"image_mask\" ] . data [:] io . close () return True else : io . close () return False","title":"load_cnmfe_nwb()"},{"location":"io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE","text":"Loader for Inscopix-cnmfe (https://github.com/inscopix/inscopix-cnmfe). The folder should contain a file ending with '_traces.csv' and a tiff file for spatial footprints. Attributes: Name Type Description A np.ndarray The spatial footprints C TsdFrame The calcium transients sampling_rate float Sampling rate of the data (default is 30 Hz). Source code in pynapple/io/cnmfe.py class InscopixCNMFE ( BaseLoader ): \"\"\"Loader for Inscopix-cnmfe (https://github.com/inscopix/inscopix-cnmfe). The folder should contain a file ending with '_traces.csv' and a tiff file for spatial footprints. Attributes ---------- A : np.ndarray The spatial footprints C : TsdFrame The calcium transients sampling_rate : float Sampling rate of the data (default is 30 Hz). \"\"\" def __init__ ( self , path ): \"\"\" Parameters ---------- path : str The path to the data. \"\"\" self . basename = os . path . basename ( path ) super () . __init__ ( path ) # Need to check if nwb file exists and if data are there loading_my_data = True if self . path is not None : nwb_path = os . path . join ( self . path , \"pynapplenwb\" ) if os . path . exists ( nwb_path ): files = os . listdir ( nwb_path ) if len ([ f for f in files if f . endswith ( \".nwb\" )]): success = self . load_cnmfe_nwb ( path ) if success : loading_my_data = False # Bypass if data have already been transfered to nwb if loading_my_data : app = App () window = OphysGUI ( app , path = path ) app . mainloop () try : app . update () except Exception : pass if window . status : self . ophys_information = window . ophys_information self . load_inscopix_cnmfe ( path ) self . save_cnmfe_nwb ( path ) def load_inscopix_cnmfe ( self , path ): \"\"\" Load the calcium transients and the spatial footprints. Parameters ---------- path : str Path to the session \"\"\" files = os . listdir ( path ) tracefile = [ f for f in files if f . endswith ( \"_traces.csv\" )] if len ( tracefile ): C = pd . read_csv ( os . path . join ( path , tracefile [ 0 ]), index_col = 0 ) else : raise RuntimeError ( \"Path {} does not contain the file {} \" . format ( path , \"*_traces.csv\" ) ) self . sampling_rate = float ( self . ophys_information [ \"ImagingPlane\" ][ \"imaging_rate\" ] ) time_index = np . arange ( 0 , len ( C )) / self . sampling_rate self . C = nap . TsdFrame ( t = time_index , d = C . values ) try : import tifffile as tiff except ImportError as ie : print ( \"Please install module tifffile for loading inscopix-cnmfe data\" , ie ) sys . exit () tifffile = [ f for f in files if f . endswith ( \".tiff\" )] if len ( tifffile ): self . A = tiff . imread ( os . path . join ( path , tifffile [ 0 ])) else : raise RuntimeError ( \"Path {} does not contain the file {} \" . format ( path , \"*.tiff\" ) ) return None def save_cnmfe_nwb ( self , path ): \"\"\" Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field. Parameters ---------- path : TYPE Description \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () device_info = self . ophys_information [ \"device\" ] device = nwbfile . create_device ( name = device_info [ \"name\" ], description = device_info [ \"description\" ], manufacturer = device_info [ \"manufacturer\" ], ) optical_info = self . ophys_information [ \"OpticalChannel\" ] optical_info [ \"emission_lambda\" ] = float ( optical_info [ \"emission_lambda\" ]) optical_channel = OpticalChannel ( name = optical_info [ \"name\" ], description = optical_info [ \"description\" ], emission_lambda = optical_info [ \"emission_lambda\" ], ) imaging_info = self . ophys_information [ \"ImagingPlane\" ] imaging_info [ \"excitation_lambda\" ] = float ( imaging_info [ \"excitation_lambda\" ]) imaging_plane = nwbfile . create_imaging_plane ( name = imaging_info [ \"name\" ], optical_channel = optical_channel , imaging_rate = self . sampling_rate , description = imaging_info [ \"description\" ], device = device , excitation_lambda = imaging_info [ \"excitation_lambda\" ], indicator = imaging_info [ \"indicator\" ], location = imaging_info [ \"location\" ], ) ophys_module = nwbfile . create_processing_module ( name = \"ophys\" , description = \"optical physiology processed data\" ) seg_info = self . ophys_information [ \"PlaneSegmentation\" ] img_seg = ImageSegmentation () ps = img_seg . create_plane_segmentation ( name = seg_info [ \"name\" ], description = seg_info [ \"description\" ], imaging_plane = imaging_plane , ) for i in range ( self . C . shape [ 1 ]): image_mask = self . A [ i ] # add image mask to plane segmentation ps . add_roi ( image_mask = image_mask ) ophys_module . add ( img_seg ) rt_region = ps . create_roi_table_region ( region = list ( np . arange ( self . C . shape [ 1 ])), description = \"ROIs\" ) roi_resp_series = RoiResponseSeries ( name = \"RoiResponseSeries\" , data = self . C . values , rois = rt_region , unit = \"lumens\" , timestamps = self . C . index . values , ) fl = Fluorescence ( roi_response_series = roi_resp_series ) ophys_module . add ( fl ) io . write ( nwbfile ) io . close () return def load_cnmfe_nwb ( self , path ): \"\"\" Load the calcium transient and spatial footprint from nwb Parameters ---------- path : str Path to the session \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r\" ) nwbfile = io . read () if \"ophys\" in nwbfile . processing . keys (): data = nwbfile . processing [ \"ophys\" ][ \"Fluorescence\" ][ \"RoiResponseSeries\" ] . data [:] t = nwbfile . processing [ \"ophys\" ][ \"Fluorescence\" ][ \"RoiResponseSeries\" ] . timestamps [:] self . C = nap . TsdFrame ( t = t , d = data ) self . A = nwbfile . processing [ \"ophys\" ][ \"ImageSegmentation\" ][ \"PlaneSegmentation\" ][ \"image_mask\" ] . data [:] io . close () return True else : io . close () return False","title":"InscopixCNMFE"},{"location":"io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.__init__","text":"Parameters: Name Type Description Default path str The path to the data. required Source code in pynapple/io/cnmfe.py def __init__ ( self , path ): \"\"\" Parameters ---------- path : str The path to the data. \"\"\" self . basename = os . path . basename ( path ) super () . __init__ ( path ) # Need to check if nwb file exists and if data are there loading_my_data = True if self . path is not None : nwb_path = os . path . join ( self . path , \"pynapplenwb\" ) if os . path . exists ( nwb_path ): files = os . listdir ( nwb_path ) if len ([ f for f in files if f . endswith ( \".nwb\" )]): success = self . load_cnmfe_nwb ( path ) if success : loading_my_data = False # Bypass if data have already been transfered to nwb if loading_my_data : app = App () window = OphysGUI ( app , path = path ) app . mainloop () try : app . update () except Exception : pass if window . status : self . ophys_information = window . ophys_information self . load_inscopix_cnmfe ( path ) self . save_cnmfe_nwb ( path )","title":"__init__()"},{"location":"io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.load_inscopix_cnmfe","text":"Load the calcium transients and the spatial footprints. Parameters: Name Type Description Default path str Path to the session required Source code in pynapple/io/cnmfe.py def load_inscopix_cnmfe ( self , path ): \"\"\" Load the calcium transients and the spatial footprints. Parameters ---------- path : str Path to the session \"\"\" files = os . listdir ( path ) tracefile = [ f for f in files if f . endswith ( \"_traces.csv\" )] if len ( tracefile ): C = pd . read_csv ( os . path . join ( path , tracefile [ 0 ]), index_col = 0 ) else : raise RuntimeError ( \"Path {} does not contain the file {} \" . format ( path , \"*_traces.csv\" ) ) self . sampling_rate = float ( self . ophys_information [ \"ImagingPlane\" ][ \"imaging_rate\" ] ) time_index = np . arange ( 0 , len ( C )) / self . sampling_rate self . C = nap . TsdFrame ( t = time_index , d = C . values ) try : import tifffile as tiff except ImportError as ie : print ( \"Please install module tifffile for loading inscopix-cnmfe data\" , ie ) sys . exit () tifffile = [ f for f in files if f . endswith ( \".tiff\" )] if len ( tifffile ): self . A = tiff . imread ( os . path . join ( path , tifffile [ 0 ])) else : raise RuntimeError ( \"Path {} does not contain the file {} \" . format ( path , \"*.tiff\" ) ) return None","title":"load_inscopix_cnmfe()"},{"location":"io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.save_cnmfe_nwb","text":"Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field. Parameters: Name Type Description Default path TYPE Description required Source code in pynapple/io/cnmfe.py def save_cnmfe_nwb ( self , path ): \"\"\" Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field. Parameters ---------- path : TYPE Description \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () device_info = self . ophys_information [ \"device\" ] device = nwbfile . create_device ( name = device_info [ \"name\" ], description = device_info [ \"description\" ], manufacturer = device_info [ \"manufacturer\" ], ) optical_info = self . ophys_information [ \"OpticalChannel\" ] optical_info [ \"emission_lambda\" ] = float ( optical_info [ \"emission_lambda\" ]) optical_channel = OpticalChannel ( name = optical_info [ \"name\" ], description = optical_info [ \"description\" ], emission_lambda = optical_info [ \"emission_lambda\" ], ) imaging_info = self . ophys_information [ \"ImagingPlane\" ] imaging_info [ \"excitation_lambda\" ] = float ( imaging_info [ \"excitation_lambda\" ]) imaging_plane = nwbfile . create_imaging_plane ( name = imaging_info [ \"name\" ], optical_channel = optical_channel , imaging_rate = self . sampling_rate , description = imaging_info [ \"description\" ], device = device , excitation_lambda = imaging_info [ \"excitation_lambda\" ], indicator = imaging_info [ \"indicator\" ], location = imaging_info [ \"location\" ], ) ophys_module = nwbfile . create_processing_module ( name = \"ophys\" , description = \"optical physiology processed data\" ) seg_info = self . ophys_information [ \"PlaneSegmentation\" ] img_seg = ImageSegmentation () ps = img_seg . create_plane_segmentation ( name = seg_info [ \"name\" ], description = seg_info [ \"description\" ], imaging_plane = imaging_plane , ) for i in range ( self . C . shape [ 1 ]): image_mask = self . A [ i ] # add image mask to plane segmentation ps . add_roi ( image_mask = image_mask ) ophys_module . add ( img_seg ) rt_region = ps . create_roi_table_region ( region = list ( np . arange ( self . C . shape [ 1 ])), description = \"ROIs\" ) roi_resp_series = RoiResponseSeries ( name = \"RoiResponseSeries\" , data = self . C . values , rois = rt_region , unit = \"lumens\" , timestamps = self . C . index . values , ) fl = Fluorescence ( roi_response_series = roi_resp_series ) ophys_module . add ( fl ) io . write ( nwbfile ) io . close () return","title":"save_cnmfe_nwb()"},{"location":"io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.load_cnmfe_nwb","text":"Load the calcium transient and spatial footprint from nwb Parameters: Name Type Description Default path str Path to the session required Source code in pynapple/io/cnmfe.py def load_cnmfe_nwb ( self , path ): \"\"\" Load the calcium transient and spatial footprint from nwb Parameters ---------- path : str Path to the session \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r\" ) nwbfile = io . read () if \"ophys\" in nwbfile . processing . keys (): data = nwbfile . processing [ \"ophys\" ][ \"Fluorescence\" ][ \"RoiResponseSeries\" ] . data [:] t = nwbfile . processing [ \"ophys\" ][ \"Fluorescence\" ][ \"RoiResponseSeries\" ] . timestamps [:] self . C = nap . TsdFrame ( t = t , d = data ) self . A = nwbfile . processing [ \"ophys\" ][ \"ImageSegmentation\" ][ \"PlaneSegmentation\" ][ \"image_mask\" ] . data [:] io . close () return True else : io . close () return False","title":"load_cnmfe_nwb()"},{"location":"io.loader/","text":"BaseLoader is the general class for loading session with pynapple. BaseLoader General loader for epochs and tracking data Source code in pynapple/io/loader.py class BaseLoader ( object ): \"\"\" General loader for epochs and tracking data \"\"\" def __init__ ( self , path = None ): self . path = path start_gui = True # Check if a pynapplenwb folder exist to bypass GUI if self . path is not None : nwb_path = os . path . join ( self . path , \"pynapplenwb\" ) if os . path . exists ( nwb_path ): files = os . listdir ( nwb_path ) if len ([ f for f in files if f . endswith ( \".nwb\" )]): start_gui = False self . load_data ( path ) # Starting the GUI if start_gui : warnings . warn ( get_deprecation_text (), category = DeprecationWarning , stacklevel = 2 ) app = App () window = BaseLoaderGUI ( app , path = path ) app . mainloop () try : app . update () except Exception : pass # Extracting all the information from gui loader if window . status : self . session_information = window . session_information self . subject_information = window . subject_information self . name = self . session_information [ \"name\" ] self . tracking_frequency = window . tracking_frequency self . position = self . _make_position ( window . tracking_parameters , window . tracking_method , window . tracking_frequency , window . epochs , window . time_units_epochs , window . tracking_alignment , ) self . epochs = self . _make_epochs ( window . epochs , window . time_units_epochs ) self . time_support = self . _join_epochs ( window . epochs , window . time_units_epochs ) # Save the data self . create_nwb_file ( path ) def load_default_csv ( self , csv_file ): \"\"\" Load tracking data. The default csv should have the time index in the first column in seconds. If no header is provided, the column names will be the column index. Parameters ---------- csv_file : str path to the csv file Returns ------- pandas.DataFrame _ \"\"\" position = pd . read_csv ( csv_file , header = [ 0 ], index_col = 0 ) position = position [ ~ position . index . duplicated ( keep = \"first\" )] return position def load_optitrack_csv ( self , csv_file ): \"\"\" Load tracking data exported with Optitrack. By default, the function reads rows 4 and 5 to build the column names. Parameters ---------- csv_file : str path to the csv file Raises ------ RuntimeError If header names are unknown. Should be 'Position' and 'Rotation' Returns ------- pandas.DataFrame _ \"\"\" position = pd . read_csv ( csv_file , header = [ 4 , 5 ], index_col = 1 ) if 1 in position . columns : position = position . drop ( labels = 1 , axis = 1 ) position = position [ ~ position . index . duplicated ( keep = \"first\" )] order = [] cols = [] for n in position . columns : if n [ 0 ] == \"Rotation\" : order . append ( \"r\" + n [ 1 ] . lower ()) cols . append ( n ) elif n [ 0 ] == \"Position\" : order . append ( n [ 1 ] . lower ()) cols . append ( n ) if len ( order ) == 0 : raise RuntimeError ( \"Unknow tracking format for csv file {} \" . format ( csv_file ) ) position = position [ cols ] position . columns = order return position def load_dlc_csv ( self , csv_file ): \"\"\" Load tracking data exported with DeepLabCut Parameters ---------- csv_file : str path to the csv file Returns ------- pandas.DataFrame _ \"\"\" position = pd . read_csv ( csv_file , header = [ 1 , 2 ], index_col = 0 ) position = position [ ~ position . index . duplicated ( keep = \"first\" )] position . columns = list ( map ( lambda x : \"_\" . join ( x ), position . columns . values )) return position def load_ttl_pulse ( self , ttl_file , tracking_frequency , n_channels = 1 , channel = 0 , bytes_size = 2 , fs = 20000.0 , threshold = 0.3 , ): \"\"\" Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames. Parameters ---------- ttl_file : str File name n_channels : int, optional The number of channels in the binary file. channel : int, optional Which channel contains the TTL bytes_size : int, optional Bytes size of the binary file. fs : float, optional Sampling frequency of the binary file Returns ------- pd.Series A series containing the time index of the TTL. \"\"\" f = open ( ttl_file , \"rb\" ) startoffile = f . seek ( 0 , 0 ) endoffile = f . seek ( 0 , 2 ) n_samples = int (( endoffile - startoffile ) / n_channels / bytes_size ) f . close () with open ( ttl_file , \"rb\" ) as f : data = np . fromfile ( f , np . uint16 ) . reshape (( n_samples , n_channels )) if n_channels == 1 : data = data . flatten () . astype ( np . int32 ) else : data = data [:, channel ] . flatten () . astype ( np . int32 ) data = data / data . max () peaks , _ = scipy . signal . find_peaks ( np . diff ( data ), height = threshold , distance = int ( fs / ( tracking_frequency * 2 )) ) timestep = np . arange ( 0 , len ( data )) / fs peaks += 1 ttl = pd . Series ( index = timestep [ peaks ], data = data [ peaks ]) return ttl def _make_position ( self , parameters , method , frequency , epochs , time_units , alignment ): \"\"\" Make the position TSDFrame with the parameters extracted from the GUI. \"\"\" if len ( parameters . index ) == 0 : return None else : if len ( epochs ) == 0 : epochs . loc [ 0 , \"start\" ] = 0.0 frames = [] time_supports_starts = [] time_support_ends = [] for i in range ( len ( parameters )): if method . lower () == \"optitrack\" : position = self . load_optitrack_csv ( parameters . loc [ i , \"csv\" ]) elif method . lower () == \"deep lab cut\" : position = self . load_dlc_csv ( parameters . loc [ i , \"csv\" ]) elif method . lower () == \"default\" : position = self . load_default_csv ( parameters . loc [ i , \"csv\" ]) if alignment . lower () == \"local\" : start_epoch = nap . format_timestamps ( epochs . loc [ int ( parameters . loc [ i , \"epoch\" ]), \"start\" ], time_units ) end_epoch = nap . format_timestamps ( epochs . loc [ int ( parameters . loc [ i , \"epoch\" ]), \"end\" ], time_units ) timestamps = ( position . index . values + nap . return_timestamps ( start_epoch , \"s\" )[ 0 ] ) # Make sure timestamps are within the epochs idx = np . where ( timestamps < end_epoch )[ 0 ] position = position . iloc [ idx ] position . index = pd . Index ( timestamps [ idx ]) if alignment . lower () == \"ttl\" : ttl = self . load_ttl_pulse ( ttl_file = parameters . loc [ i , \"ttl\" ], tracking_frequency = frequency , n_channels = int ( parameters . loc [ i , \"n_channels\" ]), channel = int ( parameters . loc [ i , \"tracking_channel\" ]), bytes_size = int ( parameters . loc [ i , \"bytes_size\" ]), fs = float ( parameters . loc [ i , \"fs\" ]), threshold = float ( parameters . loc [ i , \"threshold\" ]), ) if len ( ttl ): length = np . minimum ( len ( ttl ), len ( position )) ttl = ttl . iloc [ 0 : length ] position = position . iloc [ 0 : length ] else : raise RuntimeError ( \"No ttl detected for {} \" . format ( parameters . loc [ i , \"ttl\" ]) ) # Make sure start epochs in seconds # start_epoch = format_timestamp( # epochs.loc[parameters.loc[f, \"epoch\"], \"start\"], time_units # ) start_epoch = nap . format_timestamps ( epochs . loc [ int ( parameters . loc [ i , \"epoch\" ]), \"start\" ], time_units ) timestamps = start_epoch + ttl . index . values position . index = pd . Index ( timestamps ) frames . append ( position ) time_supports_starts . append ( position . index [ 0 ]) time_support_ends . append ( position . index [ - 1 ]) position = pd . concat ( frames ) time_supports = nap . IntervalSet ( start = time_supports_starts , end = time_support_ends , time_units = \"s\" ) # Specific to optitrACK if set ([ \"rx\" , \"ry\" , \"rz\" ]) . issubset ( position . columns ): position [[ \"ry\" , \"rx\" , \"rz\" ]] *= np . pi / 180 position [[ \"ry\" , \"rx\" , \"rz\" ]] += 2 * np . pi position [[ \"ry\" , \"rx\" , \"rz\" ]] %= 2 * np . pi position = nap . TsdFrame ( t = position . index . values , d = position . values , columns = position . columns . values , time_support = time_supports , time_units = \"s\" , ) return position def _make_epochs ( self , epochs , time_units = \"s\" ): \"\"\" Split GUI epochs into dict of epochs \"\"\" labels = epochs . groupby ( \"label\" ) . groups isets = {} for lbs in labels . keys (): tmp = epochs . loc [ labels [ lbs ]] isets [ lbs ] = nap . IntervalSet ( start = tmp [ \"start\" ], end = tmp [ \"end\" ], time_units = time_units ) return isets def _join_epochs ( self , epochs , time_units = \"s\" ): \"\"\" To create the global time support of the data \"\"\" with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" ) isets = nap . IntervalSet ( start = epochs [ \"start\" ] . sort_values (), end = epochs [ \"end\" ] . sort_values (), time_units = time_units , ) iset = isets . merge_close_intervals ( 1 , time_units = \"us\" ) if len ( iset ): return iset else : return None def create_nwb_file ( self , path ): \"\"\" Initialize the NWB file in the folder pynapplenwb within the data folder. Parameters ---------- path : str The path to save the data \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): os . makedirs ( self . nwb_path ) self . nwbfilepath = os . path . join ( self . nwb_path , self . session_information [ \"name\" ] + \".nwb\" ) self . subject_information [ \"date_of_birth\" ] = None nwbfile = NWBFile ( session_description = self . session_information [ \"description\" ], identifier = self . session_information [ \"name\" ], session_start_time = datetime . datetime . now ( datetime . timezone . utc ), experimenter = self . session_information [ \"experimenter\" ], lab = self . session_information [ \"lab\" ], institution = self . session_information [ \"institution\" ], subject = Subject ( ** self . subject_information ), ) # Tracking if self . position is not None : data = self . position . as_units ( \"s\" ) # specific to optitrack if set ([ \"x\" , \"y\" , \"z\" , \"rx\" , \"ry\" , \"rz\" ]) . issubset ( data . columns ): position = Position () for c in [ \"x\" , \"y\" , \"z\" ]: tmp = SpatialSeries ( name = c , data = data [ c ] . values , timestamps = data . index . values , unit = \"\" , reference_frame = \"\" , ) position . add_spatial_series ( tmp ) direction = CompassDirection () for c in [ \"rx\" , \"ry\" , \"rz\" ]: tmp = SpatialSeries ( name = c , data = data [ c ] . values , timestamps = data . index . values , unit = \"radian\" , reference_frame = \"\" , ) direction . add_spatial_series ( tmp ) nwbfile . add_acquisition ( position ) nwbfile . add_acquisition ( direction ) # Other types else : position = Position () for c in data . columns : tmp = SpatialSeries ( name = c , data = data [ c ] . values , timestamps = data . index . values , unit = \"\" , reference_frame = \"\" , ) position . add_spatial_series ( tmp ) nwbfile . add_acquisition ( position ) # Adding time support of position as TimeIntervals epochs = self . position . time_support . as_units ( \"s\" ) position_time_support = TimeIntervals ( name = \"position_time_support\" , description = \"The time support of the position i.e the real start and end of the tracking\" , ) for i in self . position . time_support . index : position_time_support . add_interval ( start_time = epochs . loc [ i , \"start\" ], stop_time = epochs . loc [ i , \"end\" ], tags = str ( i ), ) nwbfile . add_time_intervals ( position_time_support ) # Epochs for ep in self . epochs . keys (): epochs = self . epochs [ ep ] . as_units ( \"s\" ) for i in self . epochs [ ep ] . index : nwbfile . add_epoch ( start_time = epochs . loc [ i , \"start\" ], stop_time = epochs . loc [ i , \"end\" ], tags = [ ep ], # This is stupid nwb who tries to parse the string ) with NWBHDF5IO ( self . nwbfilepath , \"w\" ) as io : io . write ( nwbfile ) return def load_data ( self , path ): \"\"\" Load NWB data save with pynapple in the pynapplenwb folder Parameters ---------- path : str Path to the session folder \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () position = {} acq_keys = nwbfile . acquisition . keys () if \"CompassDirection\" in acq_keys : compass = nwbfile . acquisition [ \"CompassDirection\" ] for k in compass . spatial_series . keys (): position [ k ] = pd . Series ( index = compass . get_spatial_series ( k ) . timestamps [:], data = compass . get_spatial_series ( k ) . data [:], ) if \"Position\" in acq_keys : tracking = nwbfile . acquisition [ \"Position\" ] for k in tracking . spatial_series . keys (): position [ k ] = pd . Series ( index = tracking . get_spatial_series ( k ) . timestamps [:], data = tracking . get_spatial_series ( k ) . data [:], ) if len ( position ): position = pd . DataFrame . from_dict ( position ) # retrieveing time support position if in epochs if \"position_time_support\" in nwbfile . intervals . keys (): epochs = nwbfile . intervals [ \"position_time_support\" ] . to_dataframe () time_support = nap . IntervalSet ( start = epochs [ \"start_time\" ], end = epochs [ \"stop_time\" ], time_units = \"s\" ) self . position = nap . TsdFrame ( position , time_units = \"s\" , time_support = time_support ) if nwbfile . epochs is not None : epochs = nwbfile . epochs . to_dataframe () # NWB is dumb and cannot take a single string for labels epochs [ \"label\" ] = [ epochs . loc [ i , \"tags\" ][ 0 ] for i in epochs . index ] epochs = epochs . drop ( labels = \"tags\" , axis = 1 ) epochs = epochs . rename ( columns = { \"start_time\" : \"start\" , \"stop_time\" : \"end\" }) self . epochs = self . _make_epochs ( epochs ) self . time_support = self . _join_epochs ( epochs , \"s\" ) io . close () return def save_nwb_intervals ( self , iset , name , description = \"\" ): \"\"\" Add epochs to the NWB file (e.g. ripples epochs) See pynwb.epoch.TimeIntervals Parameters ---------- iset : IntervalSet The intervalSet to save name : str The name in the nwb file \"\"\" io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () epochs = iset . as_units ( \"s\" ) time_intervals = TimeIntervals ( name = name , description = description ) for i in epochs . index : time_intervals . add_interval ( start_time = epochs . loc [ i , \"start\" ], stop_time = epochs . loc [ i , \"end\" ], tags = str ( i ), ) nwbfile . add_time_intervals ( time_intervals ) io . write ( nwbfile ) io . close () return def save_nwb_timeseries ( self , tsd , name , description = \"\" ): \"\"\" Save timestamps in the NWB file (e.g. ripples time) with the time support. See pynwb.base.TimeSeries Parameters ---------- tsd : TsdFrame _ name : str _ description : str, optional _ \"\"\" io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () ts = TimeSeries ( name = name , unit = \"s\" , data = tsd . values , timestamps = tsd . as_units ( \"s\" ) . index . values , ) time_support = TimeIntervals ( name = name + \"_timesupport\" , description = \"The time support of the object\" ) epochs = tsd . time_support . as_units ( \"s\" ) for i in epochs . index : time_support . add_interval ( start_time = epochs . loc [ i , \"start\" ], stop_time = epochs . loc [ i , \"end\" ], tags = str ( i ), ) nwbfile . add_time_intervals ( time_support ) nwbfile . add_acquisition ( ts ) io . write ( nwbfile ) io . close () return def load_nwb_intervals ( self , name ): \"\"\" Load epochs from the NWB file (e.g. 'ripples') Parameters ---------- name : str The name in the nwb file \"\"\" io = NWBHDF5IO ( self . nwbfilepath , \"r\" ) nwbfile = io . read () if name in nwbfile . intervals . keys (): epochs = nwbfile . intervals [ name ] . to_dataframe () isets = nap . IntervalSet ( start = epochs [ \"start_time\" ], end = epochs [ \"stop_time\" ], time_units = \"s\" ) io . close () return isets else : io . close () return def load_nwb_timeseries ( self , name ): \"\"\" Load timestamps in the NWB file (e.g. ripples time) Parameters ---------- name : str _ Returns ------- Tsd _ \"\"\" io = NWBHDF5IO ( self . nwbfilepath , \"r\" ) nwbfile = io . read () ts = nwbfile . acquisition [ name ] time_support = self . load_nwb_intervals ( name + \"_timesupport\" ) tsd = nap . Tsd ( t = ts . timestamps [:], d = ts . data [:], time_units = \"s\" , time_support = time_support ) io . close () return tsd load_default_csv ( self , csv_file ) Load tracking data. The default csv should have the time index in the first column in seconds. If no header is provided, the column names will be the column index. Parameters: Name Type Description Default csv_file str path to the csv file required Returns: Type Description pandas.DataFrame _ Source code in pynapple/io/loader.py def load_default_csv ( self , csv_file ): \"\"\" Load tracking data. The default csv should have the time index in the first column in seconds. If no header is provided, the column names will be the column index. Parameters ---------- csv_file : str path to the csv file Returns ------- pandas.DataFrame _ \"\"\" position = pd . read_csv ( csv_file , header = [ 0 ], index_col = 0 ) position = position [ ~ position . index . duplicated ( keep = \"first\" )] return position load_optitrack_csv ( self , csv_file ) Load tracking data exported with Optitrack. By default, the function reads rows 4 and 5 to build the column names. Parameters: Name Type Description Default csv_file str path to the csv file required Exceptions: Type Description RuntimeError If header names are unknown. Should be 'Position' and 'Rotation' Returns: Type Description pandas.DataFrame _ Source code in pynapple/io/loader.py def load_optitrack_csv ( self , csv_file ): \"\"\" Load tracking data exported with Optitrack. By default, the function reads rows 4 and 5 to build the column names. Parameters ---------- csv_file : str path to the csv file Raises ------ RuntimeError If header names are unknown. Should be 'Position' and 'Rotation' Returns ------- pandas.DataFrame _ \"\"\" position = pd . read_csv ( csv_file , header = [ 4 , 5 ], index_col = 1 ) if 1 in position . columns : position = position . drop ( labels = 1 , axis = 1 ) position = position [ ~ position . index . duplicated ( keep = \"first\" )] order = [] cols = [] for n in position . columns : if n [ 0 ] == \"Rotation\" : order . append ( \"r\" + n [ 1 ] . lower ()) cols . append ( n ) elif n [ 0 ] == \"Position\" : order . append ( n [ 1 ] . lower ()) cols . append ( n ) if len ( order ) == 0 : raise RuntimeError ( \"Unknow tracking format for csv file {} \" . format ( csv_file ) ) position = position [ cols ] position . columns = order return position load_dlc_csv ( self , csv_file ) Load tracking data exported with DeepLabCut Parameters: Name Type Description Default csv_file str path to the csv file required Returns: Type Description pandas.DataFrame _ Source code in pynapple/io/loader.py def load_dlc_csv ( self , csv_file ): \"\"\" Load tracking data exported with DeepLabCut Parameters ---------- csv_file : str path to the csv file Returns ------- pandas.DataFrame _ \"\"\" position = pd . read_csv ( csv_file , header = [ 1 , 2 ], index_col = 0 ) position = position [ ~ position . index . duplicated ( keep = \"first\" )] position . columns = list ( map ( lambda x : \"_\" . join ( x ), position . columns . values )) return position load_ttl_pulse ( self , ttl_file , tracking_frequency , n_channels = 1 , channel = 0 , bytes_size = 2 , fs = 20000.0 , threshold = 0.3 ) Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames. Parameters: Name Type Description Default ttl_file str File name required n_channels int The number of channels in the binary file. 1 channel int Which channel contains the TTL 0 bytes_size int Bytes size of the binary file. 2 fs float Sampling frequency of the binary file 20000.0 Returns: Type Description pd.Series A series containing the time index of the TTL. Source code in pynapple/io/loader.py def load_ttl_pulse ( self , ttl_file , tracking_frequency , n_channels = 1 , channel = 0 , bytes_size = 2 , fs = 20000.0 , threshold = 0.3 , ): \"\"\" Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames. Parameters ---------- ttl_file : str File name n_channels : int, optional The number of channels in the binary file. channel : int, optional Which channel contains the TTL bytes_size : int, optional Bytes size of the binary file. fs : float, optional Sampling frequency of the binary file Returns ------- pd.Series A series containing the time index of the TTL. \"\"\" f = open ( ttl_file , \"rb\" ) startoffile = f . seek ( 0 , 0 ) endoffile = f . seek ( 0 , 2 ) n_samples = int (( endoffile - startoffile ) / n_channels / bytes_size ) f . close () with open ( ttl_file , \"rb\" ) as f : data = np . fromfile ( f , np . uint16 ) . reshape (( n_samples , n_channels )) if n_channels == 1 : data = data . flatten () . astype ( np . int32 ) else : data = data [:, channel ] . flatten () . astype ( np . int32 ) data = data / data . max () peaks , _ = scipy . signal . find_peaks ( np . diff ( data ), height = threshold , distance = int ( fs / ( tracking_frequency * 2 )) ) timestep = np . arange ( 0 , len ( data )) / fs peaks += 1 ttl = pd . Series ( index = timestep [ peaks ], data = data [ peaks ]) return ttl create_nwb_file ( self , path ) Initialize the NWB file in the folder pynapplenwb within the data folder. Parameters: Name Type Description Default path str The path to save the data required Source code in pynapple/io/loader.py def create_nwb_file ( self , path ): \"\"\" Initialize the NWB file in the folder pynapplenwb within the data folder. Parameters ---------- path : str The path to save the data \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): os . makedirs ( self . nwb_path ) self . nwbfilepath = os . path . join ( self . nwb_path , self . session_information [ \"name\" ] + \".nwb\" ) self . subject_information [ \"date_of_birth\" ] = None nwbfile = NWBFile ( session_description = self . session_information [ \"description\" ], identifier = self . session_information [ \"name\" ], session_start_time = datetime . datetime . now ( datetime . timezone . utc ), experimenter = self . session_information [ \"experimenter\" ], lab = self . session_information [ \"lab\" ], institution = self . session_information [ \"institution\" ], subject = Subject ( ** self . subject_information ), ) # Tracking if self . position is not None : data = self . position . as_units ( \"s\" ) # specific to optitrack if set ([ \"x\" , \"y\" , \"z\" , \"rx\" , \"ry\" , \"rz\" ]) . issubset ( data . columns ): position = Position () for c in [ \"x\" , \"y\" , \"z\" ]: tmp = SpatialSeries ( name = c , data = data [ c ] . values , timestamps = data . index . values , unit = \"\" , reference_frame = \"\" , ) position . add_spatial_series ( tmp ) direction = CompassDirection () for c in [ \"rx\" , \"ry\" , \"rz\" ]: tmp = SpatialSeries ( name = c , data = data [ c ] . values , timestamps = data . index . values , unit = \"radian\" , reference_frame = \"\" , ) direction . add_spatial_series ( tmp ) nwbfile . add_acquisition ( position ) nwbfile . add_acquisition ( direction ) # Other types else : position = Position () for c in data . columns : tmp = SpatialSeries ( name = c , data = data [ c ] . values , timestamps = data . index . values , unit = \"\" , reference_frame = \"\" , ) position . add_spatial_series ( tmp ) nwbfile . add_acquisition ( position ) # Adding time support of position as TimeIntervals epochs = self . position . time_support . as_units ( \"s\" ) position_time_support = TimeIntervals ( name = \"position_time_support\" , description = \"The time support of the position i.e the real start and end of the tracking\" , ) for i in self . position . time_support . index : position_time_support . add_interval ( start_time = epochs . loc [ i , \"start\" ], stop_time = epochs . loc [ i , \"end\" ], tags = str ( i ), ) nwbfile . add_time_intervals ( position_time_support ) # Epochs for ep in self . epochs . keys (): epochs = self . epochs [ ep ] . as_units ( \"s\" ) for i in self . epochs [ ep ] . index : nwbfile . add_epoch ( start_time = epochs . loc [ i , \"start\" ], stop_time = epochs . loc [ i , \"end\" ], tags = [ ep ], # This is stupid nwb who tries to parse the string ) with NWBHDF5IO ( self . nwbfilepath , \"w\" ) as io : io . write ( nwbfile ) return load_data ( self , path ) Load NWB data save with pynapple in the pynapplenwb folder Parameters: Name Type Description Default path str Path to the session folder required Source code in pynapple/io/loader.py def load_data ( self , path ): \"\"\" Load NWB data save with pynapple in the pynapplenwb folder Parameters ---------- path : str Path to the session folder \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () position = {} acq_keys = nwbfile . acquisition . keys () if \"CompassDirection\" in acq_keys : compass = nwbfile . acquisition [ \"CompassDirection\" ] for k in compass . spatial_series . keys (): position [ k ] = pd . Series ( index = compass . get_spatial_series ( k ) . timestamps [:], data = compass . get_spatial_series ( k ) . data [:], ) if \"Position\" in acq_keys : tracking = nwbfile . acquisition [ \"Position\" ] for k in tracking . spatial_series . keys (): position [ k ] = pd . Series ( index = tracking . get_spatial_series ( k ) . timestamps [:], data = tracking . get_spatial_series ( k ) . data [:], ) if len ( position ): position = pd . DataFrame . from_dict ( position ) # retrieveing time support position if in epochs if \"position_time_support\" in nwbfile . intervals . keys (): epochs = nwbfile . intervals [ \"position_time_support\" ] . to_dataframe () time_support = nap . IntervalSet ( start = epochs [ \"start_time\" ], end = epochs [ \"stop_time\" ], time_units = \"s\" ) self . position = nap . TsdFrame ( position , time_units = \"s\" , time_support = time_support ) if nwbfile . epochs is not None : epochs = nwbfile . epochs . to_dataframe () # NWB is dumb and cannot take a single string for labels epochs [ \"label\" ] = [ epochs . loc [ i , \"tags\" ][ 0 ] for i in epochs . index ] epochs = epochs . drop ( labels = \"tags\" , axis = 1 ) epochs = epochs . rename ( columns = { \"start_time\" : \"start\" , \"stop_time\" : \"end\" }) self . epochs = self . _make_epochs ( epochs ) self . time_support = self . _join_epochs ( epochs , \"s\" ) io . close () return save_nwb_intervals ( self , iset , name , description = '' ) Add epochs to the NWB file (e.g. ripples epochs) See pynwb.epoch.TimeIntervals Parameters: Name Type Description Default iset IntervalSet The intervalSet to save required name str The name in the nwb file required Source code in pynapple/io/loader.py def save_nwb_intervals ( self , iset , name , description = \"\" ): \"\"\" Add epochs to the NWB file (e.g. ripples epochs) See pynwb.epoch.TimeIntervals Parameters ---------- iset : IntervalSet The intervalSet to save name : str The name in the nwb file \"\"\" io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () epochs = iset . as_units ( \"s\" ) time_intervals = TimeIntervals ( name = name , description = description ) for i in epochs . index : time_intervals . add_interval ( start_time = epochs . loc [ i , \"start\" ], stop_time = epochs . loc [ i , \"end\" ], tags = str ( i ), ) nwbfile . add_time_intervals ( time_intervals ) io . write ( nwbfile ) io . close () return save_nwb_timeseries ( self , tsd , name , description = '' ) Save timestamps in the NWB file (e.g. ripples time) with the time support. See pynwb.base.TimeSeries Parameters: Name Type Description Default tsd TsdFrame _ required name str _ required description str _ '' Source code in pynapple/io/loader.py def save_nwb_timeseries ( self , tsd , name , description = \"\" ): \"\"\" Save timestamps in the NWB file (e.g. ripples time) with the time support. See pynwb.base.TimeSeries Parameters ---------- tsd : TsdFrame _ name : str _ description : str, optional _ \"\"\" io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () ts = TimeSeries ( name = name , unit = \"s\" , data = tsd . values , timestamps = tsd . as_units ( \"s\" ) . index . values , ) time_support = TimeIntervals ( name = name + \"_timesupport\" , description = \"The time support of the object\" ) epochs = tsd . time_support . as_units ( \"s\" ) for i in epochs . index : time_support . add_interval ( start_time = epochs . loc [ i , \"start\" ], stop_time = epochs . loc [ i , \"end\" ], tags = str ( i ), ) nwbfile . add_time_intervals ( time_support ) nwbfile . add_acquisition ( ts ) io . write ( nwbfile ) io . close () return load_nwb_intervals ( self , name ) Load epochs from the NWB file (e.g. 'ripples') Parameters: Name Type Description Default name str The name in the nwb file required Source code in pynapple/io/loader.py def load_nwb_intervals ( self , name ): \"\"\" Load epochs from the NWB file (e.g. 'ripples') Parameters ---------- name : str The name in the nwb file \"\"\" io = NWBHDF5IO ( self . nwbfilepath , \"r\" ) nwbfile = io . read () if name in nwbfile . intervals . keys (): epochs = nwbfile . intervals [ name ] . to_dataframe () isets = nap . IntervalSet ( start = epochs [ \"start_time\" ], end = epochs [ \"stop_time\" ], time_units = \"s\" ) io . close () return isets else : io . close () return load_nwb_timeseries ( self , name ) Load timestamps in the NWB file (e.g. ripples time) Parameters: Name Type Description Default name str _ required Returns: Type Description Tsd _ Source code in pynapple/io/loader.py def load_nwb_timeseries ( self , name ): \"\"\" Load timestamps in the NWB file (e.g. ripples time) Parameters ---------- name : str _ Returns ------- Tsd _ \"\"\" io = NWBHDF5IO ( self . nwbfilepath , \"r\" ) nwbfile = io . read () ts = nwbfile . acquisition [ name ] time_support = self . load_nwb_intervals ( name + \"_timesupport\" ) tsd = nap . Tsd ( t = ts . timestamps [:], d = ts . data [:], time_units = \"s\" , time_support = time_support ) io . close () return tsd","title":"Basic IO"},{"location":"io.loader/#pynapple.io.loader.BaseLoader","text":"General loader for epochs and tracking data Source code in pynapple/io/loader.py class BaseLoader ( object ): \"\"\" General loader for epochs and tracking data \"\"\" def __init__ ( self , path = None ): self . path = path start_gui = True # Check if a pynapplenwb folder exist to bypass GUI if self . path is not None : nwb_path = os . path . join ( self . path , \"pynapplenwb\" ) if os . path . exists ( nwb_path ): files = os . listdir ( nwb_path ) if len ([ f for f in files if f . endswith ( \".nwb\" )]): start_gui = False self . load_data ( path ) # Starting the GUI if start_gui : warnings . warn ( get_deprecation_text (), category = DeprecationWarning , stacklevel = 2 ) app = App () window = BaseLoaderGUI ( app , path = path ) app . mainloop () try : app . update () except Exception : pass # Extracting all the information from gui loader if window . status : self . session_information = window . session_information self . subject_information = window . subject_information self . name = self . session_information [ \"name\" ] self . tracking_frequency = window . tracking_frequency self . position = self . _make_position ( window . tracking_parameters , window . tracking_method , window . tracking_frequency , window . epochs , window . time_units_epochs , window . tracking_alignment , ) self . epochs = self . _make_epochs ( window . epochs , window . time_units_epochs ) self . time_support = self . _join_epochs ( window . epochs , window . time_units_epochs ) # Save the data self . create_nwb_file ( path ) def load_default_csv ( self , csv_file ): \"\"\" Load tracking data. The default csv should have the time index in the first column in seconds. If no header is provided, the column names will be the column index. Parameters ---------- csv_file : str path to the csv file Returns ------- pandas.DataFrame _ \"\"\" position = pd . read_csv ( csv_file , header = [ 0 ], index_col = 0 ) position = position [ ~ position . index . duplicated ( keep = \"first\" )] return position def load_optitrack_csv ( self , csv_file ): \"\"\" Load tracking data exported with Optitrack. By default, the function reads rows 4 and 5 to build the column names. Parameters ---------- csv_file : str path to the csv file Raises ------ RuntimeError If header names are unknown. Should be 'Position' and 'Rotation' Returns ------- pandas.DataFrame _ \"\"\" position = pd . read_csv ( csv_file , header = [ 4 , 5 ], index_col = 1 ) if 1 in position . columns : position = position . drop ( labels = 1 , axis = 1 ) position = position [ ~ position . index . duplicated ( keep = \"first\" )] order = [] cols = [] for n in position . columns : if n [ 0 ] == \"Rotation\" : order . append ( \"r\" + n [ 1 ] . lower ()) cols . append ( n ) elif n [ 0 ] == \"Position\" : order . append ( n [ 1 ] . lower ()) cols . append ( n ) if len ( order ) == 0 : raise RuntimeError ( \"Unknow tracking format for csv file {} \" . format ( csv_file ) ) position = position [ cols ] position . columns = order return position def load_dlc_csv ( self , csv_file ): \"\"\" Load tracking data exported with DeepLabCut Parameters ---------- csv_file : str path to the csv file Returns ------- pandas.DataFrame _ \"\"\" position = pd . read_csv ( csv_file , header = [ 1 , 2 ], index_col = 0 ) position = position [ ~ position . index . duplicated ( keep = \"first\" )] position . columns = list ( map ( lambda x : \"_\" . join ( x ), position . columns . values )) return position def load_ttl_pulse ( self , ttl_file , tracking_frequency , n_channels = 1 , channel = 0 , bytes_size = 2 , fs = 20000.0 , threshold = 0.3 , ): \"\"\" Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames. Parameters ---------- ttl_file : str File name n_channels : int, optional The number of channels in the binary file. channel : int, optional Which channel contains the TTL bytes_size : int, optional Bytes size of the binary file. fs : float, optional Sampling frequency of the binary file Returns ------- pd.Series A series containing the time index of the TTL. \"\"\" f = open ( ttl_file , \"rb\" ) startoffile = f . seek ( 0 , 0 ) endoffile = f . seek ( 0 , 2 ) n_samples = int (( endoffile - startoffile ) / n_channels / bytes_size ) f . close () with open ( ttl_file , \"rb\" ) as f : data = np . fromfile ( f , np . uint16 ) . reshape (( n_samples , n_channels )) if n_channels == 1 : data = data . flatten () . astype ( np . int32 ) else : data = data [:, channel ] . flatten () . astype ( np . int32 ) data = data / data . max () peaks , _ = scipy . signal . find_peaks ( np . diff ( data ), height = threshold , distance = int ( fs / ( tracking_frequency * 2 )) ) timestep = np . arange ( 0 , len ( data )) / fs peaks += 1 ttl = pd . Series ( index = timestep [ peaks ], data = data [ peaks ]) return ttl def _make_position ( self , parameters , method , frequency , epochs , time_units , alignment ): \"\"\" Make the position TSDFrame with the parameters extracted from the GUI. \"\"\" if len ( parameters . index ) == 0 : return None else : if len ( epochs ) == 0 : epochs . loc [ 0 , \"start\" ] = 0.0 frames = [] time_supports_starts = [] time_support_ends = [] for i in range ( len ( parameters )): if method . lower () == \"optitrack\" : position = self . load_optitrack_csv ( parameters . loc [ i , \"csv\" ]) elif method . lower () == \"deep lab cut\" : position = self . load_dlc_csv ( parameters . loc [ i , \"csv\" ]) elif method . lower () == \"default\" : position = self . load_default_csv ( parameters . loc [ i , \"csv\" ]) if alignment . lower () == \"local\" : start_epoch = nap . format_timestamps ( epochs . loc [ int ( parameters . loc [ i , \"epoch\" ]), \"start\" ], time_units ) end_epoch = nap . format_timestamps ( epochs . loc [ int ( parameters . loc [ i , \"epoch\" ]), \"end\" ], time_units ) timestamps = ( position . index . values + nap . return_timestamps ( start_epoch , \"s\" )[ 0 ] ) # Make sure timestamps are within the epochs idx = np . where ( timestamps < end_epoch )[ 0 ] position = position . iloc [ idx ] position . index = pd . Index ( timestamps [ idx ]) if alignment . lower () == \"ttl\" : ttl = self . load_ttl_pulse ( ttl_file = parameters . loc [ i , \"ttl\" ], tracking_frequency = frequency , n_channels = int ( parameters . loc [ i , \"n_channels\" ]), channel = int ( parameters . loc [ i , \"tracking_channel\" ]), bytes_size = int ( parameters . loc [ i , \"bytes_size\" ]), fs = float ( parameters . loc [ i , \"fs\" ]), threshold = float ( parameters . loc [ i , \"threshold\" ]), ) if len ( ttl ): length = np . minimum ( len ( ttl ), len ( position )) ttl = ttl . iloc [ 0 : length ] position = position . iloc [ 0 : length ] else : raise RuntimeError ( \"No ttl detected for {} \" . format ( parameters . loc [ i , \"ttl\" ]) ) # Make sure start epochs in seconds # start_epoch = format_timestamp( # epochs.loc[parameters.loc[f, \"epoch\"], \"start\"], time_units # ) start_epoch = nap . format_timestamps ( epochs . loc [ int ( parameters . loc [ i , \"epoch\" ]), \"start\" ], time_units ) timestamps = start_epoch + ttl . index . values position . index = pd . Index ( timestamps ) frames . append ( position ) time_supports_starts . append ( position . index [ 0 ]) time_support_ends . append ( position . index [ - 1 ]) position = pd . concat ( frames ) time_supports = nap . IntervalSet ( start = time_supports_starts , end = time_support_ends , time_units = \"s\" ) # Specific to optitrACK if set ([ \"rx\" , \"ry\" , \"rz\" ]) . issubset ( position . columns ): position [[ \"ry\" , \"rx\" , \"rz\" ]] *= np . pi / 180 position [[ \"ry\" , \"rx\" , \"rz\" ]] += 2 * np . pi position [[ \"ry\" , \"rx\" , \"rz\" ]] %= 2 * np . pi position = nap . TsdFrame ( t = position . index . values , d = position . values , columns = position . columns . values , time_support = time_supports , time_units = \"s\" , ) return position def _make_epochs ( self , epochs , time_units = \"s\" ): \"\"\" Split GUI epochs into dict of epochs \"\"\" labels = epochs . groupby ( \"label\" ) . groups isets = {} for lbs in labels . keys (): tmp = epochs . loc [ labels [ lbs ]] isets [ lbs ] = nap . IntervalSet ( start = tmp [ \"start\" ], end = tmp [ \"end\" ], time_units = time_units ) return isets def _join_epochs ( self , epochs , time_units = \"s\" ): \"\"\" To create the global time support of the data \"\"\" with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" ) isets = nap . IntervalSet ( start = epochs [ \"start\" ] . sort_values (), end = epochs [ \"end\" ] . sort_values (), time_units = time_units , ) iset = isets . merge_close_intervals ( 1 , time_units = \"us\" ) if len ( iset ): return iset else : return None def create_nwb_file ( self , path ): \"\"\" Initialize the NWB file in the folder pynapplenwb within the data folder. Parameters ---------- path : str The path to save the data \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): os . makedirs ( self . nwb_path ) self . nwbfilepath = os . path . join ( self . nwb_path , self . session_information [ \"name\" ] + \".nwb\" ) self . subject_information [ \"date_of_birth\" ] = None nwbfile = NWBFile ( session_description = self . session_information [ \"description\" ], identifier = self . session_information [ \"name\" ], session_start_time = datetime . datetime . now ( datetime . timezone . utc ), experimenter = self . session_information [ \"experimenter\" ], lab = self . session_information [ \"lab\" ], institution = self . session_information [ \"institution\" ], subject = Subject ( ** self . subject_information ), ) # Tracking if self . position is not None : data = self . position . as_units ( \"s\" ) # specific to optitrack if set ([ \"x\" , \"y\" , \"z\" , \"rx\" , \"ry\" , \"rz\" ]) . issubset ( data . columns ): position = Position () for c in [ \"x\" , \"y\" , \"z\" ]: tmp = SpatialSeries ( name = c , data = data [ c ] . values , timestamps = data . index . values , unit = \"\" , reference_frame = \"\" , ) position . add_spatial_series ( tmp ) direction = CompassDirection () for c in [ \"rx\" , \"ry\" , \"rz\" ]: tmp = SpatialSeries ( name = c , data = data [ c ] . values , timestamps = data . index . values , unit = \"radian\" , reference_frame = \"\" , ) direction . add_spatial_series ( tmp ) nwbfile . add_acquisition ( position ) nwbfile . add_acquisition ( direction ) # Other types else : position = Position () for c in data . columns : tmp = SpatialSeries ( name = c , data = data [ c ] . values , timestamps = data . index . values , unit = \"\" , reference_frame = \"\" , ) position . add_spatial_series ( tmp ) nwbfile . add_acquisition ( position ) # Adding time support of position as TimeIntervals epochs = self . position . time_support . as_units ( \"s\" ) position_time_support = TimeIntervals ( name = \"position_time_support\" , description = \"The time support of the position i.e the real start and end of the tracking\" , ) for i in self . position . time_support . index : position_time_support . add_interval ( start_time = epochs . loc [ i , \"start\" ], stop_time = epochs . loc [ i , \"end\" ], tags = str ( i ), ) nwbfile . add_time_intervals ( position_time_support ) # Epochs for ep in self . epochs . keys (): epochs = self . epochs [ ep ] . as_units ( \"s\" ) for i in self . epochs [ ep ] . index : nwbfile . add_epoch ( start_time = epochs . loc [ i , \"start\" ], stop_time = epochs . loc [ i , \"end\" ], tags = [ ep ], # This is stupid nwb who tries to parse the string ) with NWBHDF5IO ( self . nwbfilepath , \"w\" ) as io : io . write ( nwbfile ) return def load_data ( self , path ): \"\"\" Load NWB data save with pynapple in the pynapplenwb folder Parameters ---------- path : str Path to the session folder \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () position = {} acq_keys = nwbfile . acquisition . keys () if \"CompassDirection\" in acq_keys : compass = nwbfile . acquisition [ \"CompassDirection\" ] for k in compass . spatial_series . keys (): position [ k ] = pd . Series ( index = compass . get_spatial_series ( k ) . timestamps [:], data = compass . get_spatial_series ( k ) . data [:], ) if \"Position\" in acq_keys : tracking = nwbfile . acquisition [ \"Position\" ] for k in tracking . spatial_series . keys (): position [ k ] = pd . Series ( index = tracking . get_spatial_series ( k ) . timestamps [:], data = tracking . get_spatial_series ( k ) . data [:], ) if len ( position ): position = pd . DataFrame . from_dict ( position ) # retrieveing time support position if in epochs if \"position_time_support\" in nwbfile . intervals . keys (): epochs = nwbfile . intervals [ \"position_time_support\" ] . to_dataframe () time_support = nap . IntervalSet ( start = epochs [ \"start_time\" ], end = epochs [ \"stop_time\" ], time_units = \"s\" ) self . position = nap . TsdFrame ( position , time_units = \"s\" , time_support = time_support ) if nwbfile . epochs is not None : epochs = nwbfile . epochs . to_dataframe () # NWB is dumb and cannot take a single string for labels epochs [ \"label\" ] = [ epochs . loc [ i , \"tags\" ][ 0 ] for i in epochs . index ] epochs = epochs . drop ( labels = \"tags\" , axis = 1 ) epochs = epochs . rename ( columns = { \"start_time\" : \"start\" , \"stop_time\" : \"end\" }) self . epochs = self . _make_epochs ( epochs ) self . time_support = self . _join_epochs ( epochs , \"s\" ) io . close () return def save_nwb_intervals ( self , iset , name , description = \"\" ): \"\"\" Add epochs to the NWB file (e.g. ripples epochs) See pynwb.epoch.TimeIntervals Parameters ---------- iset : IntervalSet The intervalSet to save name : str The name in the nwb file \"\"\" io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () epochs = iset . as_units ( \"s\" ) time_intervals = TimeIntervals ( name = name , description = description ) for i in epochs . index : time_intervals . add_interval ( start_time = epochs . loc [ i , \"start\" ], stop_time = epochs . loc [ i , \"end\" ], tags = str ( i ), ) nwbfile . add_time_intervals ( time_intervals ) io . write ( nwbfile ) io . close () return def save_nwb_timeseries ( self , tsd , name , description = \"\" ): \"\"\" Save timestamps in the NWB file (e.g. ripples time) with the time support. See pynwb.base.TimeSeries Parameters ---------- tsd : TsdFrame _ name : str _ description : str, optional _ \"\"\" io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () ts = TimeSeries ( name = name , unit = \"s\" , data = tsd . values , timestamps = tsd . as_units ( \"s\" ) . index . values , ) time_support = TimeIntervals ( name = name + \"_timesupport\" , description = \"The time support of the object\" ) epochs = tsd . time_support . as_units ( \"s\" ) for i in epochs . index : time_support . add_interval ( start_time = epochs . loc [ i , \"start\" ], stop_time = epochs . loc [ i , \"end\" ], tags = str ( i ), ) nwbfile . add_time_intervals ( time_support ) nwbfile . add_acquisition ( ts ) io . write ( nwbfile ) io . close () return def load_nwb_intervals ( self , name ): \"\"\" Load epochs from the NWB file (e.g. 'ripples') Parameters ---------- name : str The name in the nwb file \"\"\" io = NWBHDF5IO ( self . nwbfilepath , \"r\" ) nwbfile = io . read () if name in nwbfile . intervals . keys (): epochs = nwbfile . intervals [ name ] . to_dataframe () isets = nap . IntervalSet ( start = epochs [ \"start_time\" ], end = epochs [ \"stop_time\" ], time_units = \"s\" ) io . close () return isets else : io . close () return def load_nwb_timeseries ( self , name ): \"\"\" Load timestamps in the NWB file (e.g. ripples time) Parameters ---------- name : str _ Returns ------- Tsd _ \"\"\" io = NWBHDF5IO ( self . nwbfilepath , \"r\" ) nwbfile = io . read () ts = nwbfile . acquisition [ name ] time_support = self . load_nwb_intervals ( name + \"_timesupport\" ) tsd = nap . Tsd ( t = ts . timestamps [:], d = ts . data [:], time_units = \"s\" , time_support = time_support ) io . close () return tsd","title":"BaseLoader"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.load_default_csv","text":"Load tracking data. The default csv should have the time index in the first column in seconds. If no header is provided, the column names will be the column index. Parameters: Name Type Description Default csv_file str path to the csv file required Returns: Type Description pandas.DataFrame _ Source code in pynapple/io/loader.py def load_default_csv ( self , csv_file ): \"\"\" Load tracking data. The default csv should have the time index in the first column in seconds. If no header is provided, the column names will be the column index. Parameters ---------- csv_file : str path to the csv file Returns ------- pandas.DataFrame _ \"\"\" position = pd . read_csv ( csv_file , header = [ 0 ], index_col = 0 ) position = position [ ~ position . index . duplicated ( keep = \"first\" )] return position","title":"load_default_csv()"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.load_optitrack_csv","text":"Load tracking data exported with Optitrack. By default, the function reads rows 4 and 5 to build the column names. Parameters: Name Type Description Default csv_file str path to the csv file required Exceptions: Type Description RuntimeError If header names are unknown. Should be 'Position' and 'Rotation' Returns: Type Description pandas.DataFrame _ Source code in pynapple/io/loader.py def load_optitrack_csv ( self , csv_file ): \"\"\" Load tracking data exported with Optitrack. By default, the function reads rows 4 and 5 to build the column names. Parameters ---------- csv_file : str path to the csv file Raises ------ RuntimeError If header names are unknown. Should be 'Position' and 'Rotation' Returns ------- pandas.DataFrame _ \"\"\" position = pd . read_csv ( csv_file , header = [ 4 , 5 ], index_col = 1 ) if 1 in position . columns : position = position . drop ( labels = 1 , axis = 1 ) position = position [ ~ position . index . duplicated ( keep = \"first\" )] order = [] cols = [] for n in position . columns : if n [ 0 ] == \"Rotation\" : order . append ( \"r\" + n [ 1 ] . lower ()) cols . append ( n ) elif n [ 0 ] == \"Position\" : order . append ( n [ 1 ] . lower ()) cols . append ( n ) if len ( order ) == 0 : raise RuntimeError ( \"Unknow tracking format for csv file {} \" . format ( csv_file ) ) position = position [ cols ] position . columns = order return position","title":"load_optitrack_csv()"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.load_dlc_csv","text":"Load tracking data exported with DeepLabCut Parameters: Name Type Description Default csv_file str path to the csv file required Returns: Type Description pandas.DataFrame _ Source code in pynapple/io/loader.py def load_dlc_csv ( self , csv_file ): \"\"\" Load tracking data exported with DeepLabCut Parameters ---------- csv_file : str path to the csv file Returns ------- pandas.DataFrame _ \"\"\" position = pd . read_csv ( csv_file , header = [ 1 , 2 ], index_col = 0 ) position = position [ ~ position . index . duplicated ( keep = \"first\" )] position . columns = list ( map ( lambda x : \"_\" . join ( x ), position . columns . values )) return position","title":"load_dlc_csv()"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.load_ttl_pulse","text":"Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames. Parameters: Name Type Description Default ttl_file str File name required n_channels int The number of channels in the binary file. 1 channel int Which channel contains the TTL 0 bytes_size int Bytes size of the binary file. 2 fs float Sampling frequency of the binary file 20000.0 Returns: Type Description pd.Series A series containing the time index of the TTL. Source code in pynapple/io/loader.py def load_ttl_pulse ( self , ttl_file , tracking_frequency , n_channels = 1 , channel = 0 , bytes_size = 2 , fs = 20000.0 , threshold = 0.3 , ): \"\"\" Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames. Parameters ---------- ttl_file : str File name n_channels : int, optional The number of channels in the binary file. channel : int, optional Which channel contains the TTL bytes_size : int, optional Bytes size of the binary file. fs : float, optional Sampling frequency of the binary file Returns ------- pd.Series A series containing the time index of the TTL. \"\"\" f = open ( ttl_file , \"rb\" ) startoffile = f . seek ( 0 , 0 ) endoffile = f . seek ( 0 , 2 ) n_samples = int (( endoffile - startoffile ) / n_channels / bytes_size ) f . close () with open ( ttl_file , \"rb\" ) as f : data = np . fromfile ( f , np . uint16 ) . reshape (( n_samples , n_channels )) if n_channels == 1 : data = data . flatten () . astype ( np . int32 ) else : data = data [:, channel ] . flatten () . astype ( np . int32 ) data = data / data . max () peaks , _ = scipy . signal . find_peaks ( np . diff ( data ), height = threshold , distance = int ( fs / ( tracking_frequency * 2 )) ) timestep = np . arange ( 0 , len ( data )) / fs peaks += 1 ttl = pd . Series ( index = timestep [ peaks ], data = data [ peaks ]) return ttl","title":"load_ttl_pulse()"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.create_nwb_file","text":"Initialize the NWB file in the folder pynapplenwb within the data folder. Parameters: Name Type Description Default path str The path to save the data required Source code in pynapple/io/loader.py def create_nwb_file ( self , path ): \"\"\" Initialize the NWB file in the folder pynapplenwb within the data folder. Parameters ---------- path : str The path to save the data \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): os . makedirs ( self . nwb_path ) self . nwbfilepath = os . path . join ( self . nwb_path , self . session_information [ \"name\" ] + \".nwb\" ) self . subject_information [ \"date_of_birth\" ] = None nwbfile = NWBFile ( session_description = self . session_information [ \"description\" ], identifier = self . session_information [ \"name\" ], session_start_time = datetime . datetime . now ( datetime . timezone . utc ), experimenter = self . session_information [ \"experimenter\" ], lab = self . session_information [ \"lab\" ], institution = self . session_information [ \"institution\" ], subject = Subject ( ** self . subject_information ), ) # Tracking if self . position is not None : data = self . position . as_units ( \"s\" ) # specific to optitrack if set ([ \"x\" , \"y\" , \"z\" , \"rx\" , \"ry\" , \"rz\" ]) . issubset ( data . columns ): position = Position () for c in [ \"x\" , \"y\" , \"z\" ]: tmp = SpatialSeries ( name = c , data = data [ c ] . values , timestamps = data . index . values , unit = \"\" , reference_frame = \"\" , ) position . add_spatial_series ( tmp ) direction = CompassDirection () for c in [ \"rx\" , \"ry\" , \"rz\" ]: tmp = SpatialSeries ( name = c , data = data [ c ] . values , timestamps = data . index . values , unit = \"radian\" , reference_frame = \"\" , ) direction . add_spatial_series ( tmp ) nwbfile . add_acquisition ( position ) nwbfile . add_acquisition ( direction ) # Other types else : position = Position () for c in data . columns : tmp = SpatialSeries ( name = c , data = data [ c ] . values , timestamps = data . index . values , unit = \"\" , reference_frame = \"\" , ) position . add_spatial_series ( tmp ) nwbfile . add_acquisition ( position ) # Adding time support of position as TimeIntervals epochs = self . position . time_support . as_units ( \"s\" ) position_time_support = TimeIntervals ( name = \"position_time_support\" , description = \"The time support of the position i.e the real start and end of the tracking\" , ) for i in self . position . time_support . index : position_time_support . add_interval ( start_time = epochs . loc [ i , \"start\" ], stop_time = epochs . loc [ i , \"end\" ], tags = str ( i ), ) nwbfile . add_time_intervals ( position_time_support ) # Epochs for ep in self . epochs . keys (): epochs = self . epochs [ ep ] . as_units ( \"s\" ) for i in self . epochs [ ep ] . index : nwbfile . add_epoch ( start_time = epochs . loc [ i , \"start\" ], stop_time = epochs . loc [ i , \"end\" ], tags = [ ep ], # This is stupid nwb who tries to parse the string ) with NWBHDF5IO ( self . nwbfilepath , \"w\" ) as io : io . write ( nwbfile ) return","title":"create_nwb_file()"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.load_data","text":"Load NWB data save with pynapple in the pynapplenwb folder Parameters: Name Type Description Default path str Path to the session folder required Source code in pynapple/io/loader.py def load_data ( self , path ): \"\"\" Load NWB data save with pynapple in the pynapplenwb folder Parameters ---------- path : str Path to the session folder \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () position = {} acq_keys = nwbfile . acquisition . keys () if \"CompassDirection\" in acq_keys : compass = nwbfile . acquisition [ \"CompassDirection\" ] for k in compass . spatial_series . keys (): position [ k ] = pd . Series ( index = compass . get_spatial_series ( k ) . timestamps [:], data = compass . get_spatial_series ( k ) . data [:], ) if \"Position\" in acq_keys : tracking = nwbfile . acquisition [ \"Position\" ] for k in tracking . spatial_series . keys (): position [ k ] = pd . Series ( index = tracking . get_spatial_series ( k ) . timestamps [:], data = tracking . get_spatial_series ( k ) . data [:], ) if len ( position ): position = pd . DataFrame . from_dict ( position ) # retrieveing time support position if in epochs if \"position_time_support\" in nwbfile . intervals . keys (): epochs = nwbfile . intervals [ \"position_time_support\" ] . to_dataframe () time_support = nap . IntervalSet ( start = epochs [ \"start_time\" ], end = epochs [ \"stop_time\" ], time_units = \"s\" ) self . position = nap . TsdFrame ( position , time_units = \"s\" , time_support = time_support ) if nwbfile . epochs is not None : epochs = nwbfile . epochs . to_dataframe () # NWB is dumb and cannot take a single string for labels epochs [ \"label\" ] = [ epochs . loc [ i , \"tags\" ][ 0 ] for i in epochs . index ] epochs = epochs . drop ( labels = \"tags\" , axis = 1 ) epochs = epochs . rename ( columns = { \"start_time\" : \"start\" , \"stop_time\" : \"end\" }) self . epochs = self . _make_epochs ( epochs ) self . time_support = self . _join_epochs ( epochs , \"s\" ) io . close () return","title":"load_data()"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.save_nwb_intervals","text":"Add epochs to the NWB file (e.g. ripples epochs) See pynwb.epoch.TimeIntervals Parameters: Name Type Description Default iset IntervalSet The intervalSet to save required name str The name in the nwb file required Source code in pynapple/io/loader.py def save_nwb_intervals ( self , iset , name , description = \"\" ): \"\"\" Add epochs to the NWB file (e.g. ripples epochs) See pynwb.epoch.TimeIntervals Parameters ---------- iset : IntervalSet The intervalSet to save name : str The name in the nwb file \"\"\" io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () epochs = iset . as_units ( \"s\" ) time_intervals = TimeIntervals ( name = name , description = description ) for i in epochs . index : time_intervals . add_interval ( start_time = epochs . loc [ i , \"start\" ], stop_time = epochs . loc [ i , \"end\" ], tags = str ( i ), ) nwbfile . add_time_intervals ( time_intervals ) io . write ( nwbfile ) io . close () return","title":"save_nwb_intervals()"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.save_nwb_timeseries","text":"Save timestamps in the NWB file (e.g. ripples time) with the time support. See pynwb.base.TimeSeries Parameters: Name Type Description Default tsd TsdFrame _ required name str _ required description str _ '' Source code in pynapple/io/loader.py def save_nwb_timeseries ( self , tsd , name , description = \"\" ): \"\"\" Save timestamps in the NWB file (e.g. ripples time) with the time support. See pynwb.base.TimeSeries Parameters ---------- tsd : TsdFrame _ name : str _ description : str, optional _ \"\"\" io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () ts = TimeSeries ( name = name , unit = \"s\" , data = tsd . values , timestamps = tsd . as_units ( \"s\" ) . index . values , ) time_support = TimeIntervals ( name = name + \"_timesupport\" , description = \"The time support of the object\" ) epochs = tsd . time_support . as_units ( \"s\" ) for i in epochs . index : time_support . add_interval ( start_time = epochs . loc [ i , \"start\" ], stop_time = epochs . loc [ i , \"end\" ], tags = str ( i ), ) nwbfile . add_time_intervals ( time_support ) nwbfile . add_acquisition ( ts ) io . write ( nwbfile ) io . close () return","title":"save_nwb_timeseries()"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.load_nwb_intervals","text":"Load epochs from the NWB file (e.g. 'ripples') Parameters: Name Type Description Default name str The name in the nwb file required Source code in pynapple/io/loader.py def load_nwb_intervals ( self , name ): \"\"\" Load epochs from the NWB file (e.g. 'ripples') Parameters ---------- name : str The name in the nwb file \"\"\" io = NWBHDF5IO ( self . nwbfilepath , \"r\" ) nwbfile = io . read () if name in nwbfile . intervals . keys (): epochs = nwbfile . intervals [ name ] . to_dataframe () isets = nap . IntervalSet ( start = epochs [ \"start_time\" ], end = epochs [ \"stop_time\" ], time_units = \"s\" ) io . close () return isets else : io . close () return","title":"load_nwb_intervals()"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.load_nwb_timeseries","text":"Load timestamps in the NWB file (e.g. ripples time) Parameters: Name Type Description Default name str _ required Returns: Type Description Tsd _ Source code in pynapple/io/loader.py def load_nwb_timeseries ( self , name ): \"\"\" Load timestamps in the NWB file (e.g. ripples time) Parameters ---------- name : str _ Returns ------- Tsd _ \"\"\" io = NWBHDF5IO ( self . nwbfilepath , \"r\" ) nwbfile = io . read () ts = nwbfile . acquisition [ name ] time_support = self . load_nwb_intervals ( name + \"_timesupport\" ) tsd = nap . Tsd ( t = ts . timestamps [:], d = ts . data [:], time_units = \"s\" , time_support = time_support ) io . close () return tsd","title":"load_nwb_timeseries()"},{"location":"io/","text":"pynapple.io.misc Various io functions Functions load_session ( path = None , session_type = None ) General Loader for Neurosuite Phy Minian Inscopix-cnmfe Matlab-cnmfe Suite2p None for default session. Parameters: Name Type Description Default path str The path to load the data None session_type str Can be 'neurosuite', 'phy', 'minian', 'inscopix-cnmfe', 'cnmfe-matlab', 'suite2p' or None for default loader. None Returns: Type Description Session A class holding all the data from the session. Source code in pynapple/io/misc.py def load_session ( path = None , session_type = None ): \"\"\" General Loader for - Neurosuite\\n - Phy\\n - Minian\\n - Inscopix-cnmfe\\n - Matlab-cnmfe\\n - Suite2p - None for default session. Parameters ---------- path : str, optional The path to load the data session_type : str, optional Can be 'neurosuite', 'phy', 'minian', 'inscopix-cnmfe', 'cnmfe-matlab', 'suite2p' or None for default loader. Returns ------- Session A class holding all the data from the session. \"\"\" if path : if not os . path . isdir ( path ): raise RuntimeError ( \"Path {} is not found.\" . format ( path )) if isinstance ( session_type , str ): session_type = session_type . lower () if session_type == \"neurosuite\" : return NeuroSuite ( path ) elif session_type == \"phy\" : return Phy ( path ) elif session_type == \"inscopix-cnmfe\" : return InscopixCNMFE ( path ) elif session_type == \"minian\" : return Minian ( path ) elif session_type == \"cnmfe-matlab\" : return CNMF_E ( path ) elif session_type == \"suite2p\" : return Suite2P ( path ) else : return BaseLoader ( path ) load_eeg ( filepath , channel = None , n_channels = None , frequency = None , precision = 'int16' , bytes_size = 2 ) Standalone function to load eeg/lfp/dat file in binary format. Parameters: Name Type Description Default filepath str The path to the eeg file required channel int or list of int The channel(s) to load. If None return a memory map of the dat file to avoid memory error None n_channels int Number of channels None frequency float Sampling rate of the file None precision str The precision of the binary file 'int16' bytes_size int Bytes size of the binary file 2 Exceptions: Type Description RuntimeError If can't find the lfp/eeg/dat file Returns: Type Description Tsd or TsdFrame The lfp in a time series format Source code in pynapple/io/misc.py def load_eeg ( filepath , channel = None , n_channels = None , frequency = None , precision = \"int16\" , bytes_size = 2 , ): \"\"\" Standalone function to load eeg/lfp/dat file in binary format. Parameters ---------- filepath : str The path to the eeg file channel : int or list of int, optional The channel(s) to load. If None return a memory map of the dat file to avoid memory error n_channels : int, optional Number of channels frequency : float, optional Sampling rate of the file precision : str, optional The precision of the binary file bytes_size : int, optional Bytes size of the binary file Raises ------ RuntimeError If can't find the lfp/eeg/dat file Returns ------- Tsd or TsdFrame The lfp in a time series format Deleted Parameters ------------------ extension : str, optional The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match \"\"\" # Need to check if a xml file exists path = os . path . dirname ( filepath ) basename = os . path . basename ( filepath ) . split ( \".\" )[ 0 ] listdir = os . listdir ( path ) if frequency is None or n_channels is None : if basename + \".xml\" in listdir : xmlpath = os . path . join ( path , basename + \".xml\" ) xmldoc = minidom . parse ( xmlpath ) else : raise RuntimeError ( \"Can't find xml file; please specify sampling frequency or number of channels\" ) if frequency is None : if filepath . endswith ( \".dat\" ): fs_dat = int ( xmldoc . getElementsByTagName ( \"acquisitionSystem\" )[ 0 ] . getElementsByTagName ( \"samplingRate\" )[ 0 ] . firstChild . data ) frequency = fs_dat elif filepath . endswith (( \".lfp\" , \".eeg\" )): fs_eeg = int ( xmldoc . getElementsByTagName ( \"fieldPotentials\" )[ 0 ] . getElementsByTagName ( \"lfpSamplingRate\" )[ 0 ] . firstChild . data ) frequency = fs_eeg if n_channels is None : n_channels = int ( xmldoc . getElementsByTagName ( \"acquisitionSystem\" )[ 0 ] . getElementsByTagName ( \"nChannels\" )[ 0 ] . firstChild . data ) f = open ( filepath , \"rb\" ) startoffile = f . seek ( 0 , 0 ) endoffile = f . seek ( 0 , 2 ) bytes_size = 2 n_samples = int (( endoffile - startoffile ) / n_channels / bytes_size ) duration = n_samples / frequency f . close () fp = np . memmap ( filepath , np . int16 , \"r\" , shape = ( n_samples , n_channels )) timestep = np . arange ( 0 , n_samples ) / frequency time_support = nap . IntervalSet ( start = 0 , end = duration , time_units = \"s\" ) if channel is None : return fp elif type ( channel ) is int : return nap . Tsd ( t = timestep , d = fp [:, channel ], time_units = \"s\" , time_support = time_support ) elif type ( channel ) is list : return nap . TsdFrame ( t = timestep , d = fp [:, channel ], time_units = \"s\" , time_support = time_support , columns = channel , ) append_NWB_LFP ( path , lfp , channel = None ) Standalone function for adding lfp/eeg to already existing nwb files. Parameters: Name Type Description Default path str The path to the data. The function will looks for a nwb file in path or in path/pynapplenwb. required lfp Tsd or TsdFrame Description required channel None channel number in int ff lfp is a Tsd None Exceptions: Type Description RuntimeError If can't find the nwb file If no channel is specify when passing a Tsd Source code in pynapple/io/misc.py def append_NWB_LFP ( path , lfp , channel = None ): \"\"\"Standalone function for adding lfp/eeg to already existing nwb files. Parameters ---------- path : str The path to the data. The function will looks for a nwb file in path or in path/pynapplenwb. lfp : Tsd or TsdFrame Description channel : None, optional channel number in int ff lfp is a Tsd Raises ------ RuntimeError If can't find the nwb file \\n If no channel is specify when passing a Tsd \"\"\" new_path = os . path . join ( path , \"pynapplenwb\" ) nwb_path = \"\" if os . path . exists ( new_path ): nwbfilename = [ f for f in os . listdir ( new_path ) if f . endswith ( \".nwb\" )] if len ( nwbfilename ): nwb_path = os . path . join ( path , \"pynapplenwb\" , nwbfilename [ 0 ]) else : nwbfilename = [ f for f in os . listdir ( path ) if f . endswith ( \".nwb\" )] if len ( nwbfilename ): nwb_path = os . path . join ( path , \"pynapplenwb\" , nwbfilename [ 0 ]) if len ( nwb_path ) == 0 : raise RuntimeError ( \"Can't find nwb file in {} \" . format ( path )) if isinstance ( lfp , nap . TsdFrame ): channels = lfp . columns . values elif isinstance ( lfp , nap . Tsd ): if isinstance ( channel , int ): channels = [ channel ] else : raise RuntimeError ( \"Please specify which channel it is.\" ) io = NWBHDF5IO ( nwb_path , \"r+\" ) nwbfile = io . read () all_table_region = nwbfile . create_electrode_table_region ( region = channels , description = \"\" , name = \"electrodes\" ) lfp_electrical_series = ElectricalSeries ( name = \"ElectricalSeries\" , data = lfp . values , timestamps = lfp . index . values , electrodes = all_table_region , ) lfp = LFP ( electrical_series = lfp_electrical_series ) ecephys_module = nwbfile . create_processing_module ( name = \"ecephys\" , description = \"processed extracellular electrophysiology data\" ) ecephys_module . add ( lfp ) io . write ( nwbfile ) io . close () return","title":"Miscellaneous"},{"location":"io/#pynapple.io.misc","text":"Various io functions","title":"misc"},{"location":"io/#pynapple.io.misc-functions","text":"","title":"Functions"},{"location":"io/#pynapple.io.misc.load_session","text":"General Loader for Neurosuite Phy Minian Inscopix-cnmfe Matlab-cnmfe Suite2p None for default session. Parameters: Name Type Description Default path str The path to load the data None session_type str Can be 'neurosuite', 'phy', 'minian', 'inscopix-cnmfe', 'cnmfe-matlab', 'suite2p' or None for default loader. None Returns: Type Description Session A class holding all the data from the session. Source code in pynapple/io/misc.py def load_session ( path = None , session_type = None ): \"\"\" General Loader for - Neurosuite\\n - Phy\\n - Minian\\n - Inscopix-cnmfe\\n - Matlab-cnmfe\\n - Suite2p - None for default session. Parameters ---------- path : str, optional The path to load the data session_type : str, optional Can be 'neurosuite', 'phy', 'minian', 'inscopix-cnmfe', 'cnmfe-matlab', 'suite2p' or None for default loader. Returns ------- Session A class holding all the data from the session. \"\"\" if path : if not os . path . isdir ( path ): raise RuntimeError ( \"Path {} is not found.\" . format ( path )) if isinstance ( session_type , str ): session_type = session_type . lower () if session_type == \"neurosuite\" : return NeuroSuite ( path ) elif session_type == \"phy\" : return Phy ( path ) elif session_type == \"inscopix-cnmfe\" : return InscopixCNMFE ( path ) elif session_type == \"minian\" : return Minian ( path ) elif session_type == \"cnmfe-matlab\" : return CNMF_E ( path ) elif session_type == \"suite2p\" : return Suite2P ( path ) else : return BaseLoader ( path )","title":"load_session()"},{"location":"io/#pynapple.io.misc.load_eeg","text":"Standalone function to load eeg/lfp/dat file in binary format. Parameters: Name Type Description Default filepath str The path to the eeg file required channel int or list of int The channel(s) to load. If None return a memory map of the dat file to avoid memory error None n_channels int Number of channels None frequency float Sampling rate of the file None precision str The precision of the binary file 'int16' bytes_size int Bytes size of the binary file 2 Exceptions: Type Description RuntimeError If can't find the lfp/eeg/dat file Returns: Type Description Tsd or TsdFrame The lfp in a time series format Source code in pynapple/io/misc.py def load_eeg ( filepath , channel = None , n_channels = None , frequency = None , precision = \"int16\" , bytes_size = 2 , ): \"\"\" Standalone function to load eeg/lfp/dat file in binary format. Parameters ---------- filepath : str The path to the eeg file channel : int or list of int, optional The channel(s) to load. If None return a memory map of the dat file to avoid memory error n_channels : int, optional Number of channels frequency : float, optional Sampling rate of the file precision : str, optional The precision of the binary file bytes_size : int, optional Bytes size of the binary file Raises ------ RuntimeError If can't find the lfp/eeg/dat file Returns ------- Tsd or TsdFrame The lfp in a time series format Deleted Parameters ------------------ extension : str, optional The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match \"\"\" # Need to check if a xml file exists path = os . path . dirname ( filepath ) basename = os . path . basename ( filepath ) . split ( \".\" )[ 0 ] listdir = os . listdir ( path ) if frequency is None or n_channels is None : if basename + \".xml\" in listdir : xmlpath = os . path . join ( path , basename + \".xml\" ) xmldoc = minidom . parse ( xmlpath ) else : raise RuntimeError ( \"Can't find xml file; please specify sampling frequency or number of channels\" ) if frequency is None : if filepath . endswith ( \".dat\" ): fs_dat = int ( xmldoc . getElementsByTagName ( \"acquisitionSystem\" )[ 0 ] . getElementsByTagName ( \"samplingRate\" )[ 0 ] . firstChild . data ) frequency = fs_dat elif filepath . endswith (( \".lfp\" , \".eeg\" )): fs_eeg = int ( xmldoc . getElementsByTagName ( \"fieldPotentials\" )[ 0 ] . getElementsByTagName ( \"lfpSamplingRate\" )[ 0 ] . firstChild . data ) frequency = fs_eeg if n_channels is None : n_channels = int ( xmldoc . getElementsByTagName ( \"acquisitionSystem\" )[ 0 ] . getElementsByTagName ( \"nChannels\" )[ 0 ] . firstChild . data ) f = open ( filepath , \"rb\" ) startoffile = f . seek ( 0 , 0 ) endoffile = f . seek ( 0 , 2 ) bytes_size = 2 n_samples = int (( endoffile - startoffile ) / n_channels / bytes_size ) duration = n_samples / frequency f . close () fp = np . memmap ( filepath , np . int16 , \"r\" , shape = ( n_samples , n_channels )) timestep = np . arange ( 0 , n_samples ) / frequency time_support = nap . IntervalSet ( start = 0 , end = duration , time_units = \"s\" ) if channel is None : return fp elif type ( channel ) is int : return nap . Tsd ( t = timestep , d = fp [:, channel ], time_units = \"s\" , time_support = time_support ) elif type ( channel ) is list : return nap . TsdFrame ( t = timestep , d = fp [:, channel ], time_units = \"s\" , time_support = time_support , columns = channel , )","title":"load_eeg()"},{"location":"io/#pynapple.io.misc.append_NWB_LFP","text":"Standalone function for adding lfp/eeg to already existing nwb files. Parameters: Name Type Description Default path str The path to the data. The function will looks for a nwb file in path or in path/pynapplenwb. required lfp Tsd or TsdFrame Description required channel None channel number in int ff lfp is a Tsd None Exceptions: Type Description RuntimeError If can't find the nwb file If no channel is specify when passing a Tsd Source code in pynapple/io/misc.py def append_NWB_LFP ( path , lfp , channel = None ): \"\"\"Standalone function for adding lfp/eeg to already existing nwb files. Parameters ---------- path : str The path to the data. The function will looks for a nwb file in path or in path/pynapplenwb. lfp : Tsd or TsdFrame Description channel : None, optional channel number in int ff lfp is a Tsd Raises ------ RuntimeError If can't find the nwb file \\n If no channel is specify when passing a Tsd \"\"\" new_path = os . path . join ( path , \"pynapplenwb\" ) nwb_path = \"\" if os . path . exists ( new_path ): nwbfilename = [ f for f in os . listdir ( new_path ) if f . endswith ( \".nwb\" )] if len ( nwbfilename ): nwb_path = os . path . join ( path , \"pynapplenwb\" , nwbfilename [ 0 ]) else : nwbfilename = [ f for f in os . listdir ( path ) if f . endswith ( \".nwb\" )] if len ( nwbfilename ): nwb_path = os . path . join ( path , \"pynapplenwb\" , nwbfilename [ 0 ]) if len ( nwb_path ) == 0 : raise RuntimeError ( \"Can't find nwb file in {} \" . format ( path )) if isinstance ( lfp , nap . TsdFrame ): channels = lfp . columns . values elif isinstance ( lfp , nap . Tsd ): if isinstance ( channel , int ): channels = [ channel ] else : raise RuntimeError ( \"Please specify which channel it is.\" ) io = NWBHDF5IO ( nwb_path , \"r+\" ) nwbfile = io . read () all_table_region = nwbfile . create_electrode_table_region ( region = channels , description = \"\" , name = \"electrodes\" ) lfp_electrical_series = ElectricalSeries ( name = \"ElectricalSeries\" , data = lfp . values , timestamps = lfp . index . values , electrodes = all_table_region , ) lfp = LFP ( electrical_series = lfp_electrical_series ) ecephys_module = nwbfile . create_processing_module ( name = \"ecephys\" , description = \"processed extracellular electrophysiology data\" ) ecephys_module . add ( lfp ) io . write ( nwbfile ) io . close () return","title":"append_NWB_LFP()"},{"location":"io.neurosuite/","text":"Class and functions for loading data processed with the Neurosuite (Klusters, Neuroscope, NDmanager) NeuroSuite ( BaseLoader ) Loader for kluster data Source code in pynapple/io/neurosuite.py class NeuroSuite ( BaseLoader ): \"\"\" Loader for kluster data \"\"\" def __init__ ( self , path ): \"\"\" Instantiate the data class from a neurosuite folder. Parameters ---------- path : str The path to the data. \"\"\" self . basename = os . path . basename ( path ) self . time_support = None super () . __init__ ( path ) # Need to check if nwb file exists and if data are there loading_neurosuite = True if self . path is not None : nwb_path = os . path . join ( self . path , \"pynapplenwb\" ) if os . path . exists ( nwb_path ): files = os . listdir ( nwb_path ) if len ([ f for f in files if f . endswith ( \".nwb\" )]): success = self . load_nwb_spikes ( path ) if success : loading_neurosuite = False # Bypass if data have already been transfered to nwb if loading_neurosuite : self . load_neurosuite_xml ( path ) # print(\"XML loaded\") # To label the electrodes groups app = App () window = EphysGUI ( app , path = path , groups = self . group_to_channel ) app . mainloop () try : app . update () except Exception : pass # print(\"GUI DONE\") if window . status : self . ephys_information = window . ephys_information self . load_neurosuite_spikes ( path , self . basename , self . time_support ) self . save_data ( path ) def load_neurosuite_spikes ( self , path , basename , time_support = None , fs = 20000.0 ): \"\"\" Read the clus and res files and convert to NWB. Instantiate automatically a TsGroup object. Parameters ---------- path : str The path to the data basename : str Basename of the clu and res files. time_support : IntevalSet, optional The time support of the data fs : float, optional Sampling rate of the recording. Raises ------ RuntimeError If number of clu and res are not equal. \"\"\" files = os . listdir ( path ) clu_files = np . sort ([ f for f in files if \".clu.\" in f and f [ 0 ] != \".\" ]) res_files = np . sort ([ f for f in files if \".res.\" in f and f [ 0 ] != \".\" ]) clu1 = np . sort ([ int ( f . split ( \".\" )[ - 1 ]) for f in clu_files ]) clu2 = np . sort ([ int ( f . split ( \".\" )[ - 1 ]) for f in res_files ]) if len ( clu_files ) != len ( res_files ) or not ( clu1 == clu2 ) . any (): raise RuntimeError ( \"Not the same number of clu and res files in \" + path + \"; Exiting ...\" ) count = 0 spikes = {} group = pd . Series ( dtype = np . int32 ) for i , s in zip ( range ( len ( clu_files )), clu1 ): clu = np . genfromtxt ( os . path . join ( path , basename + \".clu.\" + str ( s )), dtype = np . int32 )[ 1 :] if np . max ( clu ) > 1 : # getting rid of mua and noise res = np . genfromtxt ( os . path . join ( path , basename + \".res.\" + str ( s ))) tmp = np . unique ( clu ) . astype ( int ) idx_clu = tmp [ tmp > 1 ] idx_out = np . arange ( count , count + len ( idx_clu )) for j , k in zip ( idx_clu , idx_out ): t = res [ clu == j ] / fs spikes [ k ] = nap . Ts ( t = t , time_units = \"s\" ) group . loc [ k ] = s count += len ( idx_clu ) group = group - 1 # better to start it a 0 self . spikes = nap . TsGroup ( spikes , time_support = time_support , time_units = \"s\" , group = group ) # adding some information to help parse the neurons names = pd . Series ( index = group . index , data = [ self . ephys_information [ group . loc [ i ]][ \"name\" ] for i in group . index ], ) if ~ np . all ( names . values == \"\" ): self . spikes . set_info ( name = names ) locations = pd . Series ( index = group . index , data = [ self . ephys_information [ group . loc [ i ]][ \"location\" ] for i in group . index ], ) if ~ np . all ( locations . values == \"\" ): self . spikes . set_info ( location = locations ) return def load_neurosuite_xml ( self , path ): \"\"\" path should be the folder session containing the XML file Function reads : 1. the number of channels 2. the sampling frequency of the dat file or the eeg file depending of what is present in the folder eeg file first if both are present or both are absent 3. the mappings shanks to channels as a dict Parameters ---------- path: str The path to the data Raises ------ RuntimeError If path does not contain the xml file. \"\"\" listdir = os . listdir ( path ) xmlfiles = [ f for f in listdir if f . endswith ( \".xml\" )] if not len ( xmlfiles ): raise RuntimeError ( \"Path {} contains no xml files;\" . format ( path )) sys . exit () new_path = os . path . join ( path , xmlfiles [ 0 ]) self . xmldoc = minidom . parse ( new_path ) self . nChannels = int ( self . xmldoc . getElementsByTagName ( \"acquisitionSystem\" )[ 0 ] . getElementsByTagName ( \"nChannels\" )[ 0 ] . firstChild . data ) self . fs_dat = int ( self . xmldoc . getElementsByTagName ( \"acquisitionSystem\" )[ 0 ] . getElementsByTagName ( \"samplingRate\" )[ 0 ] . firstChild . data ) self . fs_eeg = int ( self . xmldoc . getElementsByTagName ( \"fieldPotentials\" )[ 0 ] . getElementsByTagName ( \"lfpSamplingRate\" )[ 0 ] . firstChild . data ) self . group_to_channel = {} groups = ( self . xmldoc . getElementsByTagName ( \"anatomicalDescription\" )[ 0 ] . getElementsByTagName ( \"channelGroups\" )[ 0 ] . getElementsByTagName ( \"group\" ) ) for i in range ( len ( groups )): self . group_to_channel [ i ] = np . array ( [ int ( child . firstChild . data ) for child in groups [ i ] . getElementsByTagName ( \"channel\" ) ] ) return def save_data ( self , path ): \"\"\" Save the data to NWB format. Parameters ---------- path : str The path to save the data \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () electrode_groups = {} for g in self . group_to_channel : device = nwbfile . create_device ( name = self . ephys_information [ g ][ \"device\" ][ \"name\" ] + \"-\" + str ( g ), description = self . ephys_information [ g ][ \"device\" ][ \"description\" ], manufacturer = self . ephys_information [ g ][ \"device\" ][ \"manufacturer\" ], ) if ( len ( self . ephys_information [ g ][ \"position\" ]) and type ( self . ephys_information [ g ][ \"position\" ]) is str ): self . ephys_information [ g ][ \"position\" ] = re . split ( \";|,| \" , self . ephys_information [ g ][ \"position\" ] ) elif self . ephys_information [ g ][ \"position\" ] == \"\" : self . ephys_information [ g ][ \"position\" ] = None electrode_groups [ g ] = nwbfile . create_electrode_group ( name = \"group\" + str ( g ) + \"_\" + self . ephys_information [ g ][ \"name\" ], description = self . ephys_information [ g ][ \"description\" ], position = self . ephys_information [ g ][ \"position\" ], location = self . ephys_information [ g ][ \"location\" ], device = device , ) for idx in self . group_to_channel [ g ]: nwbfile . add_electrode ( id = idx , x = 0.0 , y = 0.0 , z = 0.0 , imp = 0.0 , location = self . ephys_information [ g ][ \"location\" ], filtering = \"none\" , group = electrode_groups [ g ], ) # Adding units nwbfile . add_unit_column ( \"location\" , \"the anatomical location of this unit\" ) nwbfile . add_unit_column ( \"group\" , \"the group of the unit\" ) for u in self . spikes . keys (): nwbfile . add_unit ( id = u , spike_times = self . spikes [ u ] . as_units ( \"s\" ) . index . values , electrode_group = electrode_groups [ self . spikes . get_info ( \"group\" ) . loc [ u ]], location = self . ephys_information [ self . spikes . get_info ( \"group\" ) . loc [ u ]][ \"location\" ], group = self . spikes . get_info ( \"group\" ) . loc [ u ], ) io . write ( nwbfile ) io . close () return def load_nwb_spikes ( self , path ): \"\"\" Read the NWB spikes to extract the spike times. Parameters ---------- path : str The path to the data Returns ------- TYPE Description \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r\" ) nwbfile = io . read () if nwbfile . units is None : io . close () return False else : units = nwbfile . units . to_dataframe () spikes = { n : nap . Ts ( t = units . loc [ n , \"spike_times\" ], time_units = \"s\" ) for n in units . index } self . spikes = nap . TsGroup ( spikes , time_support = self . time_support , time_units = \"s\" , group = units [ \"group\" ], ) if ~ np . all ( units [ \"location\" ] == \"\" ): self . spikes . set_info ( location = units [ \"location\" ]) io . close () return True def load_lfp ( self , filename = None , channel = None , extension = \".eeg\" , frequency = 1250.0 , precision = \"int16\" , bytes_size = 2 , ): \"\"\" Load the LFP. Parameters ---------- filename : str, optional The filename of the lfp file. It can be useful it multiple dat files are present in the data directory channel : int or list of int, optional The channel(s) to load. If None return a memory map of the dat file to avoid memory error extension : str, optional The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match frequency : float, optional Default 1250 Hz for the eeg file precision : str, optional The precision of the binary file bytes_size : int, optional Bytes size of the lfp file Raises ------ RuntimeError If can't find the lfp/eeg/dat file Returns ------- Tsd or TsdFrame The lfp in a time series format \"\"\" if filename is not None : filepath = os . path . join ( self . path , filename ) else : listdir = os . listdir ( self . path ) eegfile = [ f for f in listdir if f . endswith ( extension )] if not len ( eegfile ): raise RuntimeError ( \"Path {} contains no {} files;\" . format ( self . path , extension ) ) filepath = os . path . join ( self . path , eegfile [ 0 ]) self . load_neurosuite_xml ( self . path ) n_channels = int ( self . nChannels ) f = open ( filepath , \"rb\" ) startoffile = f . seek ( 0 , 0 ) endoffile = f . seek ( 0 , 2 ) bytes_size = 2 n_samples = int (( endoffile - startoffile ) / n_channels / bytes_size ) duration = n_samples / frequency f . close () fp = np . memmap ( filepath , np . int16 , \"r\" , shape = ( n_samples , n_channels )) timestep = np . arange ( 0 , n_samples ) / frequency time_support = nap . IntervalSet ( start = 0 , end = duration , time_units = \"s\" ) if channel is None : return nap . TsdFrame ( t = timestep , d = fp , time_units = \"s\" , time_support = time_support ) elif type ( channel ) is int : return nap . Tsd ( t = timestep , d = fp [:, channel ], time_units = \"s\" , time_support = time_support ) elif type ( channel ) is list : return nap . TsdFrame ( t = timestep , d = fp [:, channel ], time_units = \"s\" , time_support = time_support , columns = channel , ) def read_neuroscope_intervals ( self , name = None , path2file = None ): \"\"\" This function reads .evt files in which odd raws indicate the beginning of the time series and the even raws are the ends. If the file is present in the nwb, provide the just the name. If the file is not present in the nwb, it loads the events from the nwb directory. If just the path is provided but not the name, it takes the name from the file. Parameters ---------- name: str name of the epoch in the nwb file, e.g. \"rem\" or desired name save the data in the nwb. path2file: str Path of the file you want to load. Returns ------- IntervalSet Contains two columns corresponding to the start and end of the intervals. \"\"\" if name : isets = self . load_nwb_intervals ( name ) if isinstance ( isets , nap . IntervalSet ): return isets if name is not None and path2file is None : path2file = os . path . join ( self . path , self . basename + \".\" + name + \".evt\" ) if path2file is not None : try : # df = pd.read_csv(path2file, delimiter=' ', usecols = [0], header = None) tmp = np . genfromtxt ( path2file )[:, 0 ] df = tmp . reshape ( len ( tmp ) // 2 , 2 ) except ValueError : print ( \"specify a valid name\" ) isets = nap . IntervalSet ( df [:, 0 ], df [:, 1 ], time_units = \"ms\" ) if name is None : name = path2file . split ( \".\" )[ - 2 ] print ( \"*** saving file in the nwb as\" , name ) self . save_nwb_intervals ( isets , name ) else : raise ValueError ( \"specify a valid path\" ) return isets def write_neuroscope_intervals ( self , extension , isets , name ): \"\"\"Write events to load with neuroscope (e.g. ripples start and ends) Parameters ---------- extension : str The extension of the file (e.g. basename.evt.py.rip) isets : IntervalSet The IntervalSet to write name : str The name of the events (e.g. Ripples) \"\"\" start = isets . as_units ( \"ms\" )[ \"start\" ] . values ends = isets . as_units ( \"ms\" )[ \"end\" ] . values datatowrite = np . vstack (( start , ends )) . T . flatten () n = len ( isets ) texttowrite = np . vstack ( ( ( np . repeat ( np . array ([ name + \" start\" ]), n )), ( np . repeat ( np . array ([ name + \" end\" ]), n )), ) ) . T . flatten () evt_file = os . path . join ( self . path , self . basename + extension ) f = open ( evt_file , \"w\" ) for t , n in zip ( datatowrite , texttowrite ): f . writelines ( \" {:1.6f} \" . format ( t ) + \" \\t \" + n + \" \\n \" ) f . close () return def load_mean_waveforms ( self , epoch = None , waveform_window = None , spike_count = 1000 ): \"\"\" Load the mean waveforms from a dat file. Parameters ---------- epoch : IntervalSet default = None Restrict spikes to an epoch. waveform_window : IntervalSet default interval nap.IntervalSet(start = -0.0005, end = 0.001, time_units = 'ms') Limit waveform extraction before and after spike time spike_count : int default = 1000 Number of spikes used per neuron for the calculation of waveforms Returns ------- dictionary the waveforms for all neurons pandas.Series the channel with the maximum waveform for each neuron \"\"\" if not isinstance ( waveform_window , nap . IntervalSet ): waveform_window = nap . IntervalSet ( start =- 0.5 , end = 1 , time_units = \"ms\" ) spikes = self . spikes if not os . path . exists ( self . path ): # check if path exists print ( \"The path \" + self . path + \" doesn't exist; Exiting ...\" ) sys . exit () # Load XML INFO self . load_neurosuite_xml ( self . path ) n_channels = self . nChannels fs = self . fs_dat group_to_channel = self . group_to_channel group = spikes . get_info ( \"group\" ) # Check if there is an epoch, restrict spike times to epoch if epoch is not None : if type ( epoch ) is not nap . IntervalSet : print ( \"Epoch must be an IntervalSet\" ) sys . exit () else : print ( \"Restricting spikes to epoch\" ) spikes = spikes . restrict ( epoch ) epstart = int ( epoch . as_units ( \"s\" )[ \"start\" ] . values [ 0 ] * fs ) epend = int ( epoch . as_units ( \"s\" )[ \"end\" ] . values [ 0 ] * fs ) # Find dat file files = os . listdir ( self . path ) dat_files = np . sort ([ f for f in files if \"dat\" in f and f [ 0 ] != \".\" ]) # Need n_samples collected in the entire recording from dat file to load file = os . path . join ( self . path , dat_files [ 0 ]) f = open ( file , \"rb\" ) # open file to get number of samples collected in the entire recording startoffile = f . seek ( 0 , 0 ) endoffile = f . seek ( 0 , 2 ) bytes_size = 2 n_samples = int (( endoffile - startoffile ) / n_channels / bytes_size ) f . close () # map to memory all samples for all channels, channels are numbered according to neuroscope number fp = np . memmap ( file , np . int16 , \"r\" , shape = ( n_samples , n_channels )) # convert spike times to spikes in sample number sample_spikes = { neuron : ( spikes [ neuron ] . as_units ( \"s\" ) . index . values * fs ) . astype ( \"int\" ) for neuron in spikes } # prep for waveforms overlap = int ( waveform_window . tot_length ( time_units = \"s\" ) ) # one spike's worth of overlap between windows waveform_window = abs ( np . array ( waveform_window . as_units ( \"s\" ))[ 0 ] * fs ) . astype ( int ) # convert time to sample number neuron_waveforms = { n : np . zeros ([ np . sum ( waveform_window ), len ( group_to_channel [ group [ n ]])]) for n in sample_spikes } # divide dat file into batches that slightly overlap for faster loading batch_size = 3000000 windows = np . arange ( 0 , int ( endoffile / n_channels / bytes_size ), batch_size ) if epoch is not None : print ( \"Restricting dat file to epoch\" ) windows = windows [( windows >= epstart ) & ( windows <= epend )] batches = [] for ( i ) in windows : # make overlapping batches from the beginning to end of recording if i == windows [ - 1 ]: # the last batch cannot overlap with the next one batches . append ([ i , n_samples ]) else : batches . append ([ i , i + batch_size + overlap ]) batches = [ np . int32 ( batch ) for batch in batches ] sample_counted_spikes = {} for index , neuron in enumerate ( sample_spikes ): if len ( sample_spikes [ neuron ]) >= spike_count : sample_counted_spikes [ neuron ] = np . array ( np . random . choice ( list ( sample_spikes [ neuron ]), spike_count ) ) elif len ( sample_spikes [ neuron ]) < spike_count : print ( \"Not enough spikes in neuron \" + str ( index ) + \"... using all spikes\" ) sample_counted_spikes [ neuron ] = sample_spikes [ neuron ] # Make one array containing all selected spike times of all neurons - will be used to check for spikes before loading dat file spike_check = np . array ( [ int ( spikes_neuron ) for spikes_neuron in sample_counted_spikes [ neuron ] for neuron in sample_counted_spikes ] ) for index , timestep in enumerate ( batches ): print ( f \"Extracting waveforms from dat file: window { index + 1 } / { len ( windows ) } \" , end = \" \\r \" , ) if ( len ( spike_check [ ( timestep [ 0 ] < spike_check ) & ( timestep [ 1 ] > spike_check ) ] ) == 0 ): continue # if there are no spikes for any neurons in this batch, skip and go to the next one # Load dat file for timestep tmp = pd . DataFrame ( data = fp [ timestep [ 0 ] : timestep [ 1 ], :], columns = np . arange ( n_channels ), index = range ( timestep [ 0 ], timestep [ 1 ]), ) # load dat file # Check if any spikes are present for neuron in sample_counted_spikes : neurontmp = sample_counted_spikes [ neuron ] tmp2 = neurontmp [( timestep [ 0 ] < neurontmp ) & ( timestep [ 1 ] > neurontmp )] if len ( neurontmp ) == 0 : continue # skip neuron if it has no spikes in this batch tmpn = tmp [ group_to_channel [ group [ neuron ]] ] # restrict dat file to the channel group of the neuron for time in tmp2 : # add each spike waveform to neuron_waveform spikewindow = tmpn . loc [ time - waveform_window [ 0 ] : time + waveform_window [ 1 ] - 1 ] # waveform for this spike time try : neuron_waveforms [ neuron ] += spikewindow . values except ( Exception ): # ignore if full waveform is not present in this batch pass meanwf = { n : pd . DataFrame ( data = np . array ( neuron_waveforms [ n ]) / spike_count , columns = np . arange ( len ( group_to_channel [ group [ n ]])), index = np . array ( np . arange ( - waveform_window [ 0 ], waveform_window [ 1 ])) / fs , ) for n in sample_counted_spikes } # find the max channel for each neuron maxch = pd . Series ( data = [ meanwf [ n ][ meanwf [ n ] . loc [ 0 ] . idxmin ()] . name for n in meanwf ], index = spikes . keys (), ) return meanwf , maxch __init__ ( self , path ) special Instantiate the data class from a neurosuite folder. Parameters: Name Type Description Default path str The path to the data. required Source code in pynapple/io/neurosuite.py def __init__ ( self , path ): \"\"\" Instantiate the data class from a neurosuite folder. Parameters ---------- path : str The path to the data. \"\"\" self . basename = os . path . basename ( path ) self . time_support = None super () . __init__ ( path ) # Need to check if nwb file exists and if data are there loading_neurosuite = True if self . path is not None : nwb_path = os . path . join ( self . path , \"pynapplenwb\" ) if os . path . exists ( nwb_path ): files = os . listdir ( nwb_path ) if len ([ f for f in files if f . endswith ( \".nwb\" )]): success = self . load_nwb_spikes ( path ) if success : loading_neurosuite = False # Bypass if data have already been transfered to nwb if loading_neurosuite : self . load_neurosuite_xml ( path ) # print(\"XML loaded\") # To label the electrodes groups app = App () window = EphysGUI ( app , path = path , groups = self . group_to_channel ) app . mainloop () try : app . update () except Exception : pass # print(\"GUI DONE\") if window . status : self . ephys_information = window . ephys_information self . load_neurosuite_spikes ( path , self . basename , self . time_support ) self . save_data ( path ) load_neurosuite_spikes ( self , path , basename , time_support = None , fs = 20000.0 ) Read the clus and res files and convert to NWB. Instantiate automatically a TsGroup object. Parameters: Name Type Description Default path str The path to the data required basename str Basename of the clu and res files. required time_support IntevalSet The time support of the data None fs float Sampling rate of the recording. 20000.0 Exceptions: Type Description RuntimeError If number of clu and res are not equal. Source code in pynapple/io/neurosuite.py def load_neurosuite_spikes ( self , path , basename , time_support = None , fs = 20000.0 ): \"\"\" Read the clus and res files and convert to NWB. Instantiate automatically a TsGroup object. Parameters ---------- path : str The path to the data basename : str Basename of the clu and res files. time_support : IntevalSet, optional The time support of the data fs : float, optional Sampling rate of the recording. Raises ------ RuntimeError If number of clu and res are not equal. \"\"\" files = os . listdir ( path ) clu_files = np . sort ([ f for f in files if \".clu.\" in f and f [ 0 ] != \".\" ]) res_files = np . sort ([ f for f in files if \".res.\" in f and f [ 0 ] != \".\" ]) clu1 = np . sort ([ int ( f . split ( \".\" )[ - 1 ]) for f in clu_files ]) clu2 = np . sort ([ int ( f . split ( \".\" )[ - 1 ]) for f in res_files ]) if len ( clu_files ) != len ( res_files ) or not ( clu1 == clu2 ) . any (): raise RuntimeError ( \"Not the same number of clu and res files in \" + path + \"; Exiting ...\" ) count = 0 spikes = {} group = pd . Series ( dtype = np . int32 ) for i , s in zip ( range ( len ( clu_files )), clu1 ): clu = np . genfromtxt ( os . path . join ( path , basename + \".clu.\" + str ( s )), dtype = np . int32 )[ 1 :] if np . max ( clu ) > 1 : # getting rid of mua and noise res = np . genfromtxt ( os . path . join ( path , basename + \".res.\" + str ( s ))) tmp = np . unique ( clu ) . astype ( int ) idx_clu = tmp [ tmp > 1 ] idx_out = np . arange ( count , count + len ( idx_clu )) for j , k in zip ( idx_clu , idx_out ): t = res [ clu == j ] / fs spikes [ k ] = nap . Ts ( t = t , time_units = \"s\" ) group . loc [ k ] = s count += len ( idx_clu ) group = group - 1 # better to start it a 0 self . spikes = nap . TsGroup ( spikes , time_support = time_support , time_units = \"s\" , group = group ) # adding some information to help parse the neurons names = pd . Series ( index = group . index , data = [ self . ephys_information [ group . loc [ i ]][ \"name\" ] for i in group . index ], ) if ~ np . all ( names . values == \"\" ): self . spikes . set_info ( name = names ) locations = pd . Series ( index = group . index , data = [ self . ephys_information [ group . loc [ i ]][ \"location\" ] for i in group . index ], ) if ~ np . all ( locations . values == \"\" ): self . spikes . set_info ( location = locations ) return load_neurosuite_xml ( self , path ) path should be the folder session containing the XML file Function reads : 1. the number of channels 2. the sampling frequency of the dat file or the eeg file depending of what is present in the folder eeg file first if both are present or both are absent 3. the mappings shanks to channels as a dict Parameters: Name Type Description Default path str The path to the data required Exceptions: Type Description RuntimeError If path does not contain the xml file. Source code in pynapple/io/neurosuite.py def load_neurosuite_xml ( self , path ): \"\"\" path should be the folder session containing the XML file Function reads : 1. the number of channels 2. the sampling frequency of the dat file or the eeg file depending of what is present in the folder eeg file first if both are present or both are absent 3. the mappings shanks to channels as a dict Parameters ---------- path: str The path to the data Raises ------ RuntimeError If path does not contain the xml file. \"\"\" listdir = os . listdir ( path ) xmlfiles = [ f for f in listdir if f . endswith ( \".xml\" )] if not len ( xmlfiles ): raise RuntimeError ( \"Path {} contains no xml files;\" . format ( path )) sys . exit () new_path = os . path . join ( path , xmlfiles [ 0 ]) self . xmldoc = minidom . parse ( new_path ) self . nChannels = int ( self . xmldoc . getElementsByTagName ( \"acquisitionSystem\" )[ 0 ] . getElementsByTagName ( \"nChannels\" )[ 0 ] . firstChild . data ) self . fs_dat = int ( self . xmldoc . getElementsByTagName ( \"acquisitionSystem\" )[ 0 ] . getElementsByTagName ( \"samplingRate\" )[ 0 ] . firstChild . data ) self . fs_eeg = int ( self . xmldoc . getElementsByTagName ( \"fieldPotentials\" )[ 0 ] . getElementsByTagName ( \"lfpSamplingRate\" )[ 0 ] . firstChild . data ) self . group_to_channel = {} groups = ( self . xmldoc . getElementsByTagName ( \"anatomicalDescription\" )[ 0 ] . getElementsByTagName ( \"channelGroups\" )[ 0 ] . getElementsByTagName ( \"group\" ) ) for i in range ( len ( groups )): self . group_to_channel [ i ] = np . array ( [ int ( child . firstChild . data ) for child in groups [ i ] . getElementsByTagName ( \"channel\" ) ] ) return save_data ( self , path ) Save the data to NWB format. Parameters: Name Type Description Default path str The path to save the data required Source code in pynapple/io/neurosuite.py def save_data ( self , path ): \"\"\" Save the data to NWB format. Parameters ---------- path : str The path to save the data \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () electrode_groups = {} for g in self . group_to_channel : device = nwbfile . create_device ( name = self . ephys_information [ g ][ \"device\" ][ \"name\" ] + \"-\" + str ( g ), description = self . ephys_information [ g ][ \"device\" ][ \"description\" ], manufacturer = self . ephys_information [ g ][ \"device\" ][ \"manufacturer\" ], ) if ( len ( self . ephys_information [ g ][ \"position\" ]) and type ( self . ephys_information [ g ][ \"position\" ]) is str ): self . ephys_information [ g ][ \"position\" ] = re . split ( \";|,| \" , self . ephys_information [ g ][ \"position\" ] ) elif self . ephys_information [ g ][ \"position\" ] == \"\" : self . ephys_information [ g ][ \"position\" ] = None electrode_groups [ g ] = nwbfile . create_electrode_group ( name = \"group\" + str ( g ) + \"_\" + self . ephys_information [ g ][ \"name\" ], description = self . ephys_information [ g ][ \"description\" ], position = self . ephys_information [ g ][ \"position\" ], location = self . ephys_information [ g ][ \"location\" ], device = device , ) for idx in self . group_to_channel [ g ]: nwbfile . add_electrode ( id = idx , x = 0.0 , y = 0.0 , z = 0.0 , imp = 0.0 , location = self . ephys_information [ g ][ \"location\" ], filtering = \"none\" , group = electrode_groups [ g ], ) # Adding units nwbfile . add_unit_column ( \"location\" , \"the anatomical location of this unit\" ) nwbfile . add_unit_column ( \"group\" , \"the group of the unit\" ) for u in self . spikes . keys (): nwbfile . add_unit ( id = u , spike_times = self . spikes [ u ] . as_units ( \"s\" ) . index . values , electrode_group = electrode_groups [ self . spikes . get_info ( \"group\" ) . loc [ u ]], location = self . ephys_information [ self . spikes . get_info ( \"group\" ) . loc [ u ]][ \"location\" ], group = self . spikes . get_info ( \"group\" ) . loc [ u ], ) io . write ( nwbfile ) io . close () return load_nwb_spikes ( self , path ) Read the NWB spikes to extract the spike times. Parameters: Name Type Description Default path str The path to the data required Returns: Type Description TYPE Description Source code in pynapple/io/neurosuite.py def load_nwb_spikes ( self , path ): \"\"\" Read the NWB spikes to extract the spike times. Parameters ---------- path : str The path to the data Returns ------- TYPE Description \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r\" ) nwbfile = io . read () if nwbfile . units is None : io . close () return False else : units = nwbfile . units . to_dataframe () spikes = { n : nap . Ts ( t = units . loc [ n , \"spike_times\" ], time_units = \"s\" ) for n in units . index } self . spikes = nap . TsGroup ( spikes , time_support = self . time_support , time_units = \"s\" , group = units [ \"group\" ], ) if ~ np . all ( units [ \"location\" ] == \"\" ): self . spikes . set_info ( location = units [ \"location\" ]) io . close () return True load_lfp ( self , filename = None , channel = None , extension = '.eeg' , frequency = 1250.0 , precision = 'int16' , bytes_size = 2 ) Load the LFP. Parameters: Name Type Description Default filename str The filename of the lfp file. It can be useful it multiple dat files are present in the data directory None channel int or list of int The channel(s) to load. If None return a memory map of the dat file to avoid memory error None extension str The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match '.eeg' frequency float Default 1250 Hz for the eeg file 1250.0 precision str The precision of the binary file 'int16' bytes_size int Bytes size of the lfp file 2 Exceptions: Type Description RuntimeError If can't find the lfp/eeg/dat file Returns: Type Description Tsd or TsdFrame The lfp in a time series format Source code in pynapple/io/neurosuite.py def load_lfp ( self , filename = None , channel = None , extension = \".eeg\" , frequency = 1250.0 , precision = \"int16\" , bytes_size = 2 , ): \"\"\" Load the LFP. Parameters ---------- filename : str, optional The filename of the lfp file. It can be useful it multiple dat files are present in the data directory channel : int or list of int, optional The channel(s) to load. If None return a memory map of the dat file to avoid memory error extension : str, optional The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match frequency : float, optional Default 1250 Hz for the eeg file precision : str, optional The precision of the binary file bytes_size : int, optional Bytes size of the lfp file Raises ------ RuntimeError If can't find the lfp/eeg/dat file Returns ------- Tsd or TsdFrame The lfp in a time series format \"\"\" if filename is not None : filepath = os . path . join ( self . path , filename ) else : listdir = os . listdir ( self . path ) eegfile = [ f for f in listdir if f . endswith ( extension )] if not len ( eegfile ): raise RuntimeError ( \"Path {} contains no {} files;\" . format ( self . path , extension ) ) filepath = os . path . join ( self . path , eegfile [ 0 ]) self . load_neurosuite_xml ( self . path ) n_channels = int ( self . nChannels ) f = open ( filepath , \"rb\" ) startoffile = f . seek ( 0 , 0 ) endoffile = f . seek ( 0 , 2 ) bytes_size = 2 n_samples = int (( endoffile - startoffile ) / n_channels / bytes_size ) duration = n_samples / frequency f . close () fp = np . memmap ( filepath , np . int16 , \"r\" , shape = ( n_samples , n_channels )) timestep = np . arange ( 0 , n_samples ) / frequency time_support = nap . IntervalSet ( start = 0 , end = duration , time_units = \"s\" ) if channel is None : return nap . TsdFrame ( t = timestep , d = fp , time_units = \"s\" , time_support = time_support ) elif type ( channel ) is int : return nap . Tsd ( t = timestep , d = fp [:, channel ], time_units = \"s\" , time_support = time_support ) elif type ( channel ) is list : return nap . TsdFrame ( t = timestep , d = fp [:, channel ], time_units = \"s\" , time_support = time_support , columns = channel , ) read_neuroscope_intervals ( self , name = None , path2file = None ) This function reads .evt files in which odd raws indicate the beginning of the time series and the even raws are the ends. If the file is present in the nwb, provide the just the name. If the file is not present in the nwb, it loads the events from the nwb directory. If just the path is provided but not the name, it takes the name from the file. Parameters: Name Type Description Default name str name of the epoch in the nwb file, e.g. \"rem\" or desired name save the data in the nwb. None path2file str Path of the file you want to load. None Returns: Type Description IntervalSet Contains two columns corresponding to the start and end of the intervals. Source code in pynapple/io/neurosuite.py def read_neuroscope_intervals ( self , name = None , path2file = None ): \"\"\" This function reads .evt files in which odd raws indicate the beginning of the time series and the even raws are the ends. If the file is present in the nwb, provide the just the name. If the file is not present in the nwb, it loads the events from the nwb directory. If just the path is provided but not the name, it takes the name from the file. Parameters ---------- name: str name of the epoch in the nwb file, e.g. \"rem\" or desired name save the data in the nwb. path2file: str Path of the file you want to load. Returns ------- IntervalSet Contains two columns corresponding to the start and end of the intervals. \"\"\" if name : isets = self . load_nwb_intervals ( name ) if isinstance ( isets , nap . IntervalSet ): return isets if name is not None and path2file is None : path2file = os . path . join ( self . path , self . basename + \".\" + name + \".evt\" ) if path2file is not None : try : # df = pd.read_csv(path2file, delimiter=' ', usecols = [0], header = None) tmp = np . genfromtxt ( path2file )[:, 0 ] df = tmp . reshape ( len ( tmp ) // 2 , 2 ) except ValueError : print ( \"specify a valid name\" ) isets = nap . IntervalSet ( df [:, 0 ], df [:, 1 ], time_units = \"ms\" ) if name is None : name = path2file . split ( \".\" )[ - 2 ] print ( \"*** saving file in the nwb as\" , name ) self . save_nwb_intervals ( isets , name ) else : raise ValueError ( \"specify a valid path\" ) return isets write_neuroscope_intervals ( self , extension , isets , name ) Write events to load with neuroscope (e.g. ripples start and ends) Parameters: Name Type Description Default extension str The extension of the file (e.g. basename.evt.py.rip) required isets IntervalSet The IntervalSet to write required name str The name of the events (e.g. Ripples) required Source code in pynapple/io/neurosuite.py def write_neuroscope_intervals ( self , extension , isets , name ): \"\"\"Write events to load with neuroscope (e.g. ripples start and ends) Parameters ---------- extension : str The extension of the file (e.g. basename.evt.py.rip) isets : IntervalSet The IntervalSet to write name : str The name of the events (e.g. Ripples) \"\"\" start = isets . as_units ( \"ms\" )[ \"start\" ] . values ends = isets . as_units ( \"ms\" )[ \"end\" ] . values datatowrite = np . vstack (( start , ends )) . T . flatten () n = len ( isets ) texttowrite = np . vstack ( ( ( np . repeat ( np . array ([ name + \" start\" ]), n )), ( np . repeat ( np . array ([ name + \" end\" ]), n )), ) ) . T . flatten () evt_file = os . path . join ( self . path , self . basename + extension ) f = open ( evt_file , \"w\" ) for t , n in zip ( datatowrite , texttowrite ): f . writelines ( \" {:1.6f} \" . format ( t ) + \" \\t \" + n + \" \\n \" ) f . close () return load_mean_waveforms ( self , epoch = None , waveform_window = None , spike_count = 1000 ) Load the mean waveforms from a dat file. Parameters: Name Type Description Default epoch IntervalSet default = None Restrict spikes to an epoch. None waveform_window IntervalSet default interval nap.IntervalSet(start = -0.0005, end = 0.001, time_units = 'ms') Limit waveform extraction before and after spike time None spike_count int default = 1000 Number of spikes used per neuron for the calculation of waveforms 1000 Returns: Type Description dictionary the waveforms for all neurons Source code in pynapple/io/neurosuite.py def load_mean_waveforms ( self , epoch = None , waveform_window = None , spike_count = 1000 ): \"\"\" Load the mean waveforms from a dat file. Parameters ---------- epoch : IntervalSet default = None Restrict spikes to an epoch. waveform_window : IntervalSet default interval nap.IntervalSet(start = -0.0005, end = 0.001, time_units = 'ms') Limit waveform extraction before and after spike time spike_count : int default = 1000 Number of spikes used per neuron for the calculation of waveforms Returns ------- dictionary the waveforms for all neurons pandas.Series the channel with the maximum waveform for each neuron \"\"\" if not isinstance ( waveform_window , nap . IntervalSet ): waveform_window = nap . IntervalSet ( start =- 0.5 , end = 1 , time_units = \"ms\" ) spikes = self . spikes if not os . path . exists ( self . path ): # check if path exists print ( \"The path \" + self . path + \" doesn't exist; Exiting ...\" ) sys . exit () # Load XML INFO self . load_neurosuite_xml ( self . path ) n_channels = self . nChannels fs = self . fs_dat group_to_channel = self . group_to_channel group = spikes . get_info ( \"group\" ) # Check if there is an epoch, restrict spike times to epoch if epoch is not None : if type ( epoch ) is not nap . IntervalSet : print ( \"Epoch must be an IntervalSet\" ) sys . exit () else : print ( \"Restricting spikes to epoch\" ) spikes = spikes . restrict ( epoch ) epstart = int ( epoch . as_units ( \"s\" )[ \"start\" ] . values [ 0 ] * fs ) epend = int ( epoch . as_units ( \"s\" )[ \"end\" ] . values [ 0 ] * fs ) # Find dat file files = os . listdir ( self . path ) dat_files = np . sort ([ f for f in files if \"dat\" in f and f [ 0 ] != \".\" ]) # Need n_samples collected in the entire recording from dat file to load file = os . path . join ( self . path , dat_files [ 0 ]) f = open ( file , \"rb\" ) # open file to get number of samples collected in the entire recording startoffile = f . seek ( 0 , 0 ) endoffile = f . seek ( 0 , 2 ) bytes_size = 2 n_samples = int (( endoffile - startoffile ) / n_channels / bytes_size ) f . close () # map to memory all samples for all channels, channels are numbered according to neuroscope number fp = np . memmap ( file , np . int16 , \"r\" , shape = ( n_samples , n_channels )) # convert spike times to spikes in sample number sample_spikes = { neuron : ( spikes [ neuron ] . as_units ( \"s\" ) . index . values * fs ) . astype ( \"int\" ) for neuron in spikes } # prep for waveforms overlap = int ( waveform_window . tot_length ( time_units = \"s\" ) ) # one spike's worth of overlap between windows waveform_window = abs ( np . array ( waveform_window . as_units ( \"s\" ))[ 0 ] * fs ) . astype ( int ) # convert time to sample number neuron_waveforms = { n : np . zeros ([ np . sum ( waveform_window ), len ( group_to_channel [ group [ n ]])]) for n in sample_spikes } # divide dat file into batches that slightly overlap for faster loading batch_size = 3000000 windows = np . arange ( 0 , int ( endoffile / n_channels / bytes_size ), batch_size ) if epoch is not None : print ( \"Restricting dat file to epoch\" ) windows = windows [( windows >= epstart ) & ( windows <= epend )] batches = [] for ( i ) in windows : # make overlapping batches from the beginning to end of recording if i == windows [ - 1 ]: # the last batch cannot overlap with the next one batches . append ([ i , n_samples ]) else : batches . append ([ i , i + batch_size + overlap ]) batches = [ np . int32 ( batch ) for batch in batches ] sample_counted_spikes = {} for index , neuron in enumerate ( sample_spikes ): if len ( sample_spikes [ neuron ]) >= spike_count : sample_counted_spikes [ neuron ] = np . array ( np . random . choice ( list ( sample_spikes [ neuron ]), spike_count ) ) elif len ( sample_spikes [ neuron ]) < spike_count : print ( \"Not enough spikes in neuron \" + str ( index ) + \"... using all spikes\" ) sample_counted_spikes [ neuron ] = sample_spikes [ neuron ] # Make one array containing all selected spike times of all neurons - will be used to check for spikes before loading dat file spike_check = np . array ( [ int ( spikes_neuron ) for spikes_neuron in sample_counted_spikes [ neuron ] for neuron in sample_counted_spikes ] ) for index , timestep in enumerate ( batches ): print ( f \"Extracting waveforms from dat file: window { index + 1 } / { len ( windows ) } \" , end = \" \\r \" , ) if ( len ( spike_check [ ( timestep [ 0 ] < spike_check ) & ( timestep [ 1 ] > spike_check ) ] ) == 0 ): continue # if there are no spikes for any neurons in this batch, skip and go to the next one # Load dat file for timestep tmp = pd . DataFrame ( data = fp [ timestep [ 0 ] : timestep [ 1 ], :], columns = np . arange ( n_channels ), index = range ( timestep [ 0 ], timestep [ 1 ]), ) # load dat file # Check if any spikes are present for neuron in sample_counted_spikes : neurontmp = sample_counted_spikes [ neuron ] tmp2 = neurontmp [( timestep [ 0 ] < neurontmp ) & ( timestep [ 1 ] > neurontmp )] if len ( neurontmp ) == 0 : continue # skip neuron if it has no spikes in this batch tmpn = tmp [ group_to_channel [ group [ neuron ]] ] # restrict dat file to the channel group of the neuron for time in tmp2 : # add each spike waveform to neuron_waveform spikewindow = tmpn . loc [ time - waveform_window [ 0 ] : time + waveform_window [ 1 ] - 1 ] # waveform for this spike time try : neuron_waveforms [ neuron ] += spikewindow . values except ( Exception ): # ignore if full waveform is not present in this batch pass meanwf = { n : pd . DataFrame ( data = np . array ( neuron_waveforms [ n ]) / spike_count , columns = np . arange ( len ( group_to_channel [ group [ n ]])), index = np . array ( np . arange ( - waveform_window [ 0 ], waveform_window [ 1 ])) / fs , ) for n in sample_counted_spikes } # find the max channel for each neuron maxch = pd . Series ( data = [ meanwf [ n ][ meanwf [ n ] . loc [ 0 ] . idxmin ()] . name for n in meanwf ], index = spikes . keys (), ) return meanwf , maxch","title":"Neurosuite"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite","text":"Loader for kluster data Source code in pynapple/io/neurosuite.py class NeuroSuite ( BaseLoader ): \"\"\" Loader for kluster data \"\"\" def __init__ ( self , path ): \"\"\" Instantiate the data class from a neurosuite folder. Parameters ---------- path : str The path to the data. \"\"\" self . basename = os . path . basename ( path ) self . time_support = None super () . __init__ ( path ) # Need to check if nwb file exists and if data are there loading_neurosuite = True if self . path is not None : nwb_path = os . path . join ( self . path , \"pynapplenwb\" ) if os . path . exists ( nwb_path ): files = os . listdir ( nwb_path ) if len ([ f for f in files if f . endswith ( \".nwb\" )]): success = self . load_nwb_spikes ( path ) if success : loading_neurosuite = False # Bypass if data have already been transfered to nwb if loading_neurosuite : self . load_neurosuite_xml ( path ) # print(\"XML loaded\") # To label the electrodes groups app = App () window = EphysGUI ( app , path = path , groups = self . group_to_channel ) app . mainloop () try : app . update () except Exception : pass # print(\"GUI DONE\") if window . status : self . ephys_information = window . ephys_information self . load_neurosuite_spikes ( path , self . basename , self . time_support ) self . save_data ( path ) def load_neurosuite_spikes ( self , path , basename , time_support = None , fs = 20000.0 ): \"\"\" Read the clus and res files and convert to NWB. Instantiate automatically a TsGroup object. Parameters ---------- path : str The path to the data basename : str Basename of the clu and res files. time_support : IntevalSet, optional The time support of the data fs : float, optional Sampling rate of the recording. Raises ------ RuntimeError If number of clu and res are not equal. \"\"\" files = os . listdir ( path ) clu_files = np . sort ([ f for f in files if \".clu.\" in f and f [ 0 ] != \".\" ]) res_files = np . sort ([ f for f in files if \".res.\" in f and f [ 0 ] != \".\" ]) clu1 = np . sort ([ int ( f . split ( \".\" )[ - 1 ]) for f in clu_files ]) clu2 = np . sort ([ int ( f . split ( \".\" )[ - 1 ]) for f in res_files ]) if len ( clu_files ) != len ( res_files ) or not ( clu1 == clu2 ) . any (): raise RuntimeError ( \"Not the same number of clu and res files in \" + path + \"; Exiting ...\" ) count = 0 spikes = {} group = pd . Series ( dtype = np . int32 ) for i , s in zip ( range ( len ( clu_files )), clu1 ): clu = np . genfromtxt ( os . path . join ( path , basename + \".clu.\" + str ( s )), dtype = np . int32 )[ 1 :] if np . max ( clu ) > 1 : # getting rid of mua and noise res = np . genfromtxt ( os . path . join ( path , basename + \".res.\" + str ( s ))) tmp = np . unique ( clu ) . astype ( int ) idx_clu = tmp [ tmp > 1 ] idx_out = np . arange ( count , count + len ( idx_clu )) for j , k in zip ( idx_clu , idx_out ): t = res [ clu == j ] / fs spikes [ k ] = nap . Ts ( t = t , time_units = \"s\" ) group . loc [ k ] = s count += len ( idx_clu ) group = group - 1 # better to start it a 0 self . spikes = nap . TsGroup ( spikes , time_support = time_support , time_units = \"s\" , group = group ) # adding some information to help parse the neurons names = pd . Series ( index = group . index , data = [ self . ephys_information [ group . loc [ i ]][ \"name\" ] for i in group . index ], ) if ~ np . all ( names . values == \"\" ): self . spikes . set_info ( name = names ) locations = pd . Series ( index = group . index , data = [ self . ephys_information [ group . loc [ i ]][ \"location\" ] for i in group . index ], ) if ~ np . all ( locations . values == \"\" ): self . spikes . set_info ( location = locations ) return def load_neurosuite_xml ( self , path ): \"\"\" path should be the folder session containing the XML file Function reads : 1. the number of channels 2. the sampling frequency of the dat file or the eeg file depending of what is present in the folder eeg file first if both are present or both are absent 3. the mappings shanks to channels as a dict Parameters ---------- path: str The path to the data Raises ------ RuntimeError If path does not contain the xml file. \"\"\" listdir = os . listdir ( path ) xmlfiles = [ f for f in listdir if f . endswith ( \".xml\" )] if not len ( xmlfiles ): raise RuntimeError ( \"Path {} contains no xml files;\" . format ( path )) sys . exit () new_path = os . path . join ( path , xmlfiles [ 0 ]) self . xmldoc = minidom . parse ( new_path ) self . nChannels = int ( self . xmldoc . getElementsByTagName ( \"acquisitionSystem\" )[ 0 ] . getElementsByTagName ( \"nChannels\" )[ 0 ] . firstChild . data ) self . fs_dat = int ( self . xmldoc . getElementsByTagName ( \"acquisitionSystem\" )[ 0 ] . getElementsByTagName ( \"samplingRate\" )[ 0 ] . firstChild . data ) self . fs_eeg = int ( self . xmldoc . getElementsByTagName ( \"fieldPotentials\" )[ 0 ] . getElementsByTagName ( \"lfpSamplingRate\" )[ 0 ] . firstChild . data ) self . group_to_channel = {} groups = ( self . xmldoc . getElementsByTagName ( \"anatomicalDescription\" )[ 0 ] . getElementsByTagName ( \"channelGroups\" )[ 0 ] . getElementsByTagName ( \"group\" ) ) for i in range ( len ( groups )): self . group_to_channel [ i ] = np . array ( [ int ( child . firstChild . data ) for child in groups [ i ] . getElementsByTagName ( \"channel\" ) ] ) return def save_data ( self , path ): \"\"\" Save the data to NWB format. Parameters ---------- path : str The path to save the data \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () electrode_groups = {} for g in self . group_to_channel : device = nwbfile . create_device ( name = self . ephys_information [ g ][ \"device\" ][ \"name\" ] + \"-\" + str ( g ), description = self . ephys_information [ g ][ \"device\" ][ \"description\" ], manufacturer = self . ephys_information [ g ][ \"device\" ][ \"manufacturer\" ], ) if ( len ( self . ephys_information [ g ][ \"position\" ]) and type ( self . ephys_information [ g ][ \"position\" ]) is str ): self . ephys_information [ g ][ \"position\" ] = re . split ( \";|,| \" , self . ephys_information [ g ][ \"position\" ] ) elif self . ephys_information [ g ][ \"position\" ] == \"\" : self . ephys_information [ g ][ \"position\" ] = None electrode_groups [ g ] = nwbfile . create_electrode_group ( name = \"group\" + str ( g ) + \"_\" + self . ephys_information [ g ][ \"name\" ], description = self . ephys_information [ g ][ \"description\" ], position = self . ephys_information [ g ][ \"position\" ], location = self . ephys_information [ g ][ \"location\" ], device = device , ) for idx in self . group_to_channel [ g ]: nwbfile . add_electrode ( id = idx , x = 0.0 , y = 0.0 , z = 0.0 , imp = 0.0 , location = self . ephys_information [ g ][ \"location\" ], filtering = \"none\" , group = electrode_groups [ g ], ) # Adding units nwbfile . add_unit_column ( \"location\" , \"the anatomical location of this unit\" ) nwbfile . add_unit_column ( \"group\" , \"the group of the unit\" ) for u in self . spikes . keys (): nwbfile . add_unit ( id = u , spike_times = self . spikes [ u ] . as_units ( \"s\" ) . index . values , electrode_group = electrode_groups [ self . spikes . get_info ( \"group\" ) . loc [ u ]], location = self . ephys_information [ self . spikes . get_info ( \"group\" ) . loc [ u ]][ \"location\" ], group = self . spikes . get_info ( \"group\" ) . loc [ u ], ) io . write ( nwbfile ) io . close () return def load_nwb_spikes ( self , path ): \"\"\" Read the NWB spikes to extract the spike times. Parameters ---------- path : str The path to the data Returns ------- TYPE Description \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r\" ) nwbfile = io . read () if nwbfile . units is None : io . close () return False else : units = nwbfile . units . to_dataframe () spikes = { n : nap . Ts ( t = units . loc [ n , \"spike_times\" ], time_units = \"s\" ) for n in units . index } self . spikes = nap . TsGroup ( spikes , time_support = self . time_support , time_units = \"s\" , group = units [ \"group\" ], ) if ~ np . all ( units [ \"location\" ] == \"\" ): self . spikes . set_info ( location = units [ \"location\" ]) io . close () return True def load_lfp ( self , filename = None , channel = None , extension = \".eeg\" , frequency = 1250.0 , precision = \"int16\" , bytes_size = 2 , ): \"\"\" Load the LFP. Parameters ---------- filename : str, optional The filename of the lfp file. It can be useful it multiple dat files are present in the data directory channel : int or list of int, optional The channel(s) to load. If None return a memory map of the dat file to avoid memory error extension : str, optional The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match frequency : float, optional Default 1250 Hz for the eeg file precision : str, optional The precision of the binary file bytes_size : int, optional Bytes size of the lfp file Raises ------ RuntimeError If can't find the lfp/eeg/dat file Returns ------- Tsd or TsdFrame The lfp in a time series format \"\"\" if filename is not None : filepath = os . path . join ( self . path , filename ) else : listdir = os . listdir ( self . path ) eegfile = [ f for f in listdir if f . endswith ( extension )] if not len ( eegfile ): raise RuntimeError ( \"Path {} contains no {} files;\" . format ( self . path , extension ) ) filepath = os . path . join ( self . path , eegfile [ 0 ]) self . load_neurosuite_xml ( self . path ) n_channels = int ( self . nChannels ) f = open ( filepath , \"rb\" ) startoffile = f . seek ( 0 , 0 ) endoffile = f . seek ( 0 , 2 ) bytes_size = 2 n_samples = int (( endoffile - startoffile ) / n_channels / bytes_size ) duration = n_samples / frequency f . close () fp = np . memmap ( filepath , np . int16 , \"r\" , shape = ( n_samples , n_channels )) timestep = np . arange ( 0 , n_samples ) / frequency time_support = nap . IntervalSet ( start = 0 , end = duration , time_units = \"s\" ) if channel is None : return nap . TsdFrame ( t = timestep , d = fp , time_units = \"s\" , time_support = time_support ) elif type ( channel ) is int : return nap . Tsd ( t = timestep , d = fp [:, channel ], time_units = \"s\" , time_support = time_support ) elif type ( channel ) is list : return nap . TsdFrame ( t = timestep , d = fp [:, channel ], time_units = \"s\" , time_support = time_support , columns = channel , ) def read_neuroscope_intervals ( self , name = None , path2file = None ): \"\"\" This function reads .evt files in which odd raws indicate the beginning of the time series and the even raws are the ends. If the file is present in the nwb, provide the just the name. If the file is not present in the nwb, it loads the events from the nwb directory. If just the path is provided but not the name, it takes the name from the file. Parameters ---------- name: str name of the epoch in the nwb file, e.g. \"rem\" or desired name save the data in the nwb. path2file: str Path of the file you want to load. Returns ------- IntervalSet Contains two columns corresponding to the start and end of the intervals. \"\"\" if name : isets = self . load_nwb_intervals ( name ) if isinstance ( isets , nap . IntervalSet ): return isets if name is not None and path2file is None : path2file = os . path . join ( self . path , self . basename + \".\" + name + \".evt\" ) if path2file is not None : try : # df = pd.read_csv(path2file, delimiter=' ', usecols = [0], header = None) tmp = np . genfromtxt ( path2file )[:, 0 ] df = tmp . reshape ( len ( tmp ) // 2 , 2 ) except ValueError : print ( \"specify a valid name\" ) isets = nap . IntervalSet ( df [:, 0 ], df [:, 1 ], time_units = \"ms\" ) if name is None : name = path2file . split ( \".\" )[ - 2 ] print ( \"*** saving file in the nwb as\" , name ) self . save_nwb_intervals ( isets , name ) else : raise ValueError ( \"specify a valid path\" ) return isets def write_neuroscope_intervals ( self , extension , isets , name ): \"\"\"Write events to load with neuroscope (e.g. ripples start and ends) Parameters ---------- extension : str The extension of the file (e.g. basename.evt.py.rip) isets : IntervalSet The IntervalSet to write name : str The name of the events (e.g. Ripples) \"\"\" start = isets . as_units ( \"ms\" )[ \"start\" ] . values ends = isets . as_units ( \"ms\" )[ \"end\" ] . values datatowrite = np . vstack (( start , ends )) . T . flatten () n = len ( isets ) texttowrite = np . vstack ( ( ( np . repeat ( np . array ([ name + \" start\" ]), n )), ( np . repeat ( np . array ([ name + \" end\" ]), n )), ) ) . T . flatten () evt_file = os . path . join ( self . path , self . basename + extension ) f = open ( evt_file , \"w\" ) for t , n in zip ( datatowrite , texttowrite ): f . writelines ( \" {:1.6f} \" . format ( t ) + \" \\t \" + n + \" \\n \" ) f . close () return def load_mean_waveforms ( self , epoch = None , waveform_window = None , spike_count = 1000 ): \"\"\" Load the mean waveforms from a dat file. Parameters ---------- epoch : IntervalSet default = None Restrict spikes to an epoch. waveform_window : IntervalSet default interval nap.IntervalSet(start = -0.0005, end = 0.001, time_units = 'ms') Limit waveform extraction before and after spike time spike_count : int default = 1000 Number of spikes used per neuron for the calculation of waveforms Returns ------- dictionary the waveforms for all neurons pandas.Series the channel with the maximum waveform for each neuron \"\"\" if not isinstance ( waveform_window , nap . IntervalSet ): waveform_window = nap . IntervalSet ( start =- 0.5 , end = 1 , time_units = \"ms\" ) spikes = self . spikes if not os . path . exists ( self . path ): # check if path exists print ( \"The path \" + self . path + \" doesn't exist; Exiting ...\" ) sys . exit () # Load XML INFO self . load_neurosuite_xml ( self . path ) n_channels = self . nChannels fs = self . fs_dat group_to_channel = self . group_to_channel group = spikes . get_info ( \"group\" ) # Check if there is an epoch, restrict spike times to epoch if epoch is not None : if type ( epoch ) is not nap . IntervalSet : print ( \"Epoch must be an IntervalSet\" ) sys . exit () else : print ( \"Restricting spikes to epoch\" ) spikes = spikes . restrict ( epoch ) epstart = int ( epoch . as_units ( \"s\" )[ \"start\" ] . values [ 0 ] * fs ) epend = int ( epoch . as_units ( \"s\" )[ \"end\" ] . values [ 0 ] * fs ) # Find dat file files = os . listdir ( self . path ) dat_files = np . sort ([ f for f in files if \"dat\" in f and f [ 0 ] != \".\" ]) # Need n_samples collected in the entire recording from dat file to load file = os . path . join ( self . path , dat_files [ 0 ]) f = open ( file , \"rb\" ) # open file to get number of samples collected in the entire recording startoffile = f . seek ( 0 , 0 ) endoffile = f . seek ( 0 , 2 ) bytes_size = 2 n_samples = int (( endoffile - startoffile ) / n_channels / bytes_size ) f . close () # map to memory all samples for all channels, channels are numbered according to neuroscope number fp = np . memmap ( file , np . int16 , \"r\" , shape = ( n_samples , n_channels )) # convert spike times to spikes in sample number sample_spikes = { neuron : ( spikes [ neuron ] . as_units ( \"s\" ) . index . values * fs ) . astype ( \"int\" ) for neuron in spikes } # prep for waveforms overlap = int ( waveform_window . tot_length ( time_units = \"s\" ) ) # one spike's worth of overlap between windows waveform_window = abs ( np . array ( waveform_window . as_units ( \"s\" ))[ 0 ] * fs ) . astype ( int ) # convert time to sample number neuron_waveforms = { n : np . zeros ([ np . sum ( waveform_window ), len ( group_to_channel [ group [ n ]])]) for n in sample_spikes } # divide dat file into batches that slightly overlap for faster loading batch_size = 3000000 windows = np . arange ( 0 , int ( endoffile / n_channels / bytes_size ), batch_size ) if epoch is not None : print ( \"Restricting dat file to epoch\" ) windows = windows [( windows >= epstart ) & ( windows <= epend )] batches = [] for ( i ) in windows : # make overlapping batches from the beginning to end of recording if i == windows [ - 1 ]: # the last batch cannot overlap with the next one batches . append ([ i , n_samples ]) else : batches . append ([ i , i + batch_size + overlap ]) batches = [ np . int32 ( batch ) for batch in batches ] sample_counted_spikes = {} for index , neuron in enumerate ( sample_spikes ): if len ( sample_spikes [ neuron ]) >= spike_count : sample_counted_spikes [ neuron ] = np . array ( np . random . choice ( list ( sample_spikes [ neuron ]), spike_count ) ) elif len ( sample_spikes [ neuron ]) < spike_count : print ( \"Not enough spikes in neuron \" + str ( index ) + \"... using all spikes\" ) sample_counted_spikes [ neuron ] = sample_spikes [ neuron ] # Make one array containing all selected spike times of all neurons - will be used to check for spikes before loading dat file spike_check = np . array ( [ int ( spikes_neuron ) for spikes_neuron in sample_counted_spikes [ neuron ] for neuron in sample_counted_spikes ] ) for index , timestep in enumerate ( batches ): print ( f \"Extracting waveforms from dat file: window { index + 1 } / { len ( windows ) } \" , end = \" \\r \" , ) if ( len ( spike_check [ ( timestep [ 0 ] < spike_check ) & ( timestep [ 1 ] > spike_check ) ] ) == 0 ): continue # if there are no spikes for any neurons in this batch, skip and go to the next one # Load dat file for timestep tmp = pd . DataFrame ( data = fp [ timestep [ 0 ] : timestep [ 1 ], :], columns = np . arange ( n_channels ), index = range ( timestep [ 0 ], timestep [ 1 ]), ) # load dat file # Check if any spikes are present for neuron in sample_counted_spikes : neurontmp = sample_counted_spikes [ neuron ] tmp2 = neurontmp [( timestep [ 0 ] < neurontmp ) & ( timestep [ 1 ] > neurontmp )] if len ( neurontmp ) == 0 : continue # skip neuron if it has no spikes in this batch tmpn = tmp [ group_to_channel [ group [ neuron ]] ] # restrict dat file to the channel group of the neuron for time in tmp2 : # add each spike waveform to neuron_waveform spikewindow = tmpn . loc [ time - waveform_window [ 0 ] : time + waveform_window [ 1 ] - 1 ] # waveform for this spike time try : neuron_waveforms [ neuron ] += spikewindow . values except ( Exception ): # ignore if full waveform is not present in this batch pass meanwf = { n : pd . DataFrame ( data = np . array ( neuron_waveforms [ n ]) / spike_count , columns = np . arange ( len ( group_to_channel [ group [ n ]])), index = np . array ( np . arange ( - waveform_window [ 0 ], waveform_window [ 1 ])) / fs , ) for n in sample_counted_spikes } # find the max channel for each neuron maxch = pd . Series ( data = [ meanwf [ n ][ meanwf [ n ] . loc [ 0 ] . idxmin ()] . name for n in meanwf ], index = spikes . keys (), ) return meanwf , maxch","title":"NeuroSuite"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.__init__","text":"Instantiate the data class from a neurosuite folder. Parameters: Name Type Description Default path str The path to the data. required Source code in pynapple/io/neurosuite.py def __init__ ( self , path ): \"\"\" Instantiate the data class from a neurosuite folder. Parameters ---------- path : str The path to the data. \"\"\" self . basename = os . path . basename ( path ) self . time_support = None super () . __init__ ( path ) # Need to check if nwb file exists and if data are there loading_neurosuite = True if self . path is not None : nwb_path = os . path . join ( self . path , \"pynapplenwb\" ) if os . path . exists ( nwb_path ): files = os . listdir ( nwb_path ) if len ([ f for f in files if f . endswith ( \".nwb\" )]): success = self . load_nwb_spikes ( path ) if success : loading_neurosuite = False # Bypass if data have already been transfered to nwb if loading_neurosuite : self . load_neurosuite_xml ( path ) # print(\"XML loaded\") # To label the electrodes groups app = App () window = EphysGUI ( app , path = path , groups = self . group_to_channel ) app . mainloop () try : app . update () except Exception : pass # print(\"GUI DONE\") if window . status : self . ephys_information = window . ephys_information self . load_neurosuite_spikes ( path , self . basename , self . time_support ) self . save_data ( path )","title":"__init__()"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_neurosuite_spikes","text":"Read the clus and res files and convert to NWB. Instantiate automatically a TsGroup object. Parameters: Name Type Description Default path str The path to the data required basename str Basename of the clu and res files. required time_support IntevalSet The time support of the data None fs float Sampling rate of the recording. 20000.0 Exceptions: Type Description RuntimeError If number of clu and res are not equal. Source code in pynapple/io/neurosuite.py def load_neurosuite_spikes ( self , path , basename , time_support = None , fs = 20000.0 ): \"\"\" Read the clus and res files and convert to NWB. Instantiate automatically a TsGroup object. Parameters ---------- path : str The path to the data basename : str Basename of the clu and res files. time_support : IntevalSet, optional The time support of the data fs : float, optional Sampling rate of the recording. Raises ------ RuntimeError If number of clu and res are not equal. \"\"\" files = os . listdir ( path ) clu_files = np . sort ([ f for f in files if \".clu.\" in f and f [ 0 ] != \".\" ]) res_files = np . sort ([ f for f in files if \".res.\" in f and f [ 0 ] != \".\" ]) clu1 = np . sort ([ int ( f . split ( \".\" )[ - 1 ]) for f in clu_files ]) clu2 = np . sort ([ int ( f . split ( \".\" )[ - 1 ]) for f in res_files ]) if len ( clu_files ) != len ( res_files ) or not ( clu1 == clu2 ) . any (): raise RuntimeError ( \"Not the same number of clu and res files in \" + path + \"; Exiting ...\" ) count = 0 spikes = {} group = pd . Series ( dtype = np . int32 ) for i , s in zip ( range ( len ( clu_files )), clu1 ): clu = np . genfromtxt ( os . path . join ( path , basename + \".clu.\" + str ( s )), dtype = np . int32 )[ 1 :] if np . max ( clu ) > 1 : # getting rid of mua and noise res = np . genfromtxt ( os . path . join ( path , basename + \".res.\" + str ( s ))) tmp = np . unique ( clu ) . astype ( int ) idx_clu = tmp [ tmp > 1 ] idx_out = np . arange ( count , count + len ( idx_clu )) for j , k in zip ( idx_clu , idx_out ): t = res [ clu == j ] / fs spikes [ k ] = nap . Ts ( t = t , time_units = \"s\" ) group . loc [ k ] = s count += len ( idx_clu ) group = group - 1 # better to start it a 0 self . spikes = nap . TsGroup ( spikes , time_support = time_support , time_units = \"s\" , group = group ) # adding some information to help parse the neurons names = pd . Series ( index = group . index , data = [ self . ephys_information [ group . loc [ i ]][ \"name\" ] for i in group . index ], ) if ~ np . all ( names . values == \"\" ): self . spikes . set_info ( name = names ) locations = pd . Series ( index = group . index , data = [ self . ephys_information [ group . loc [ i ]][ \"location\" ] for i in group . index ], ) if ~ np . all ( locations . values == \"\" ): self . spikes . set_info ( location = locations ) return","title":"load_neurosuite_spikes()"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_neurosuite_xml","text":"path should be the folder session containing the XML file Function reads : 1. the number of channels 2. the sampling frequency of the dat file or the eeg file depending of what is present in the folder eeg file first if both are present or both are absent 3. the mappings shanks to channels as a dict Parameters: Name Type Description Default path str The path to the data required Exceptions: Type Description RuntimeError If path does not contain the xml file. Source code in pynapple/io/neurosuite.py def load_neurosuite_xml ( self , path ): \"\"\" path should be the folder session containing the XML file Function reads : 1. the number of channels 2. the sampling frequency of the dat file or the eeg file depending of what is present in the folder eeg file first if both are present or both are absent 3. the mappings shanks to channels as a dict Parameters ---------- path: str The path to the data Raises ------ RuntimeError If path does not contain the xml file. \"\"\" listdir = os . listdir ( path ) xmlfiles = [ f for f in listdir if f . endswith ( \".xml\" )] if not len ( xmlfiles ): raise RuntimeError ( \"Path {} contains no xml files;\" . format ( path )) sys . exit () new_path = os . path . join ( path , xmlfiles [ 0 ]) self . xmldoc = minidom . parse ( new_path ) self . nChannels = int ( self . xmldoc . getElementsByTagName ( \"acquisitionSystem\" )[ 0 ] . getElementsByTagName ( \"nChannels\" )[ 0 ] . firstChild . data ) self . fs_dat = int ( self . xmldoc . getElementsByTagName ( \"acquisitionSystem\" )[ 0 ] . getElementsByTagName ( \"samplingRate\" )[ 0 ] . firstChild . data ) self . fs_eeg = int ( self . xmldoc . getElementsByTagName ( \"fieldPotentials\" )[ 0 ] . getElementsByTagName ( \"lfpSamplingRate\" )[ 0 ] . firstChild . data ) self . group_to_channel = {} groups = ( self . xmldoc . getElementsByTagName ( \"anatomicalDescription\" )[ 0 ] . getElementsByTagName ( \"channelGroups\" )[ 0 ] . getElementsByTagName ( \"group\" ) ) for i in range ( len ( groups )): self . group_to_channel [ i ] = np . array ( [ int ( child . firstChild . data ) for child in groups [ i ] . getElementsByTagName ( \"channel\" ) ] ) return","title":"load_neurosuite_xml()"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.save_data","text":"Save the data to NWB format. Parameters: Name Type Description Default path str The path to save the data required Source code in pynapple/io/neurosuite.py def save_data ( self , path ): \"\"\" Save the data to NWB format. Parameters ---------- path : str The path to save the data \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () electrode_groups = {} for g in self . group_to_channel : device = nwbfile . create_device ( name = self . ephys_information [ g ][ \"device\" ][ \"name\" ] + \"-\" + str ( g ), description = self . ephys_information [ g ][ \"device\" ][ \"description\" ], manufacturer = self . ephys_information [ g ][ \"device\" ][ \"manufacturer\" ], ) if ( len ( self . ephys_information [ g ][ \"position\" ]) and type ( self . ephys_information [ g ][ \"position\" ]) is str ): self . ephys_information [ g ][ \"position\" ] = re . split ( \";|,| \" , self . ephys_information [ g ][ \"position\" ] ) elif self . ephys_information [ g ][ \"position\" ] == \"\" : self . ephys_information [ g ][ \"position\" ] = None electrode_groups [ g ] = nwbfile . create_electrode_group ( name = \"group\" + str ( g ) + \"_\" + self . ephys_information [ g ][ \"name\" ], description = self . ephys_information [ g ][ \"description\" ], position = self . ephys_information [ g ][ \"position\" ], location = self . ephys_information [ g ][ \"location\" ], device = device , ) for idx in self . group_to_channel [ g ]: nwbfile . add_electrode ( id = idx , x = 0.0 , y = 0.0 , z = 0.0 , imp = 0.0 , location = self . ephys_information [ g ][ \"location\" ], filtering = \"none\" , group = electrode_groups [ g ], ) # Adding units nwbfile . add_unit_column ( \"location\" , \"the anatomical location of this unit\" ) nwbfile . add_unit_column ( \"group\" , \"the group of the unit\" ) for u in self . spikes . keys (): nwbfile . add_unit ( id = u , spike_times = self . spikes [ u ] . as_units ( \"s\" ) . index . values , electrode_group = electrode_groups [ self . spikes . get_info ( \"group\" ) . loc [ u ]], location = self . ephys_information [ self . spikes . get_info ( \"group\" ) . loc [ u ]][ \"location\" ], group = self . spikes . get_info ( \"group\" ) . loc [ u ], ) io . write ( nwbfile ) io . close () return","title":"save_data()"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_nwb_spikes","text":"Read the NWB spikes to extract the spike times. Parameters: Name Type Description Default path str The path to the data required Returns: Type Description TYPE Description Source code in pynapple/io/neurosuite.py def load_nwb_spikes ( self , path ): \"\"\" Read the NWB spikes to extract the spike times. Parameters ---------- path : str The path to the data Returns ------- TYPE Description \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r\" ) nwbfile = io . read () if nwbfile . units is None : io . close () return False else : units = nwbfile . units . to_dataframe () spikes = { n : nap . Ts ( t = units . loc [ n , \"spike_times\" ], time_units = \"s\" ) for n in units . index } self . spikes = nap . TsGroup ( spikes , time_support = self . time_support , time_units = \"s\" , group = units [ \"group\" ], ) if ~ np . all ( units [ \"location\" ] == \"\" ): self . spikes . set_info ( location = units [ \"location\" ]) io . close () return True","title":"load_nwb_spikes()"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_lfp","text":"Load the LFP. Parameters: Name Type Description Default filename str The filename of the lfp file. It can be useful it multiple dat files are present in the data directory None channel int or list of int The channel(s) to load. If None return a memory map of the dat file to avoid memory error None extension str The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match '.eeg' frequency float Default 1250 Hz for the eeg file 1250.0 precision str The precision of the binary file 'int16' bytes_size int Bytes size of the lfp file 2 Exceptions: Type Description RuntimeError If can't find the lfp/eeg/dat file Returns: Type Description Tsd or TsdFrame The lfp in a time series format Source code in pynapple/io/neurosuite.py def load_lfp ( self , filename = None , channel = None , extension = \".eeg\" , frequency = 1250.0 , precision = \"int16\" , bytes_size = 2 , ): \"\"\" Load the LFP. Parameters ---------- filename : str, optional The filename of the lfp file. It can be useful it multiple dat files are present in the data directory channel : int or list of int, optional The channel(s) to load. If None return a memory map of the dat file to avoid memory error extension : str, optional The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match frequency : float, optional Default 1250 Hz for the eeg file precision : str, optional The precision of the binary file bytes_size : int, optional Bytes size of the lfp file Raises ------ RuntimeError If can't find the lfp/eeg/dat file Returns ------- Tsd or TsdFrame The lfp in a time series format \"\"\" if filename is not None : filepath = os . path . join ( self . path , filename ) else : listdir = os . listdir ( self . path ) eegfile = [ f for f in listdir if f . endswith ( extension )] if not len ( eegfile ): raise RuntimeError ( \"Path {} contains no {} files;\" . format ( self . path , extension ) ) filepath = os . path . join ( self . path , eegfile [ 0 ]) self . load_neurosuite_xml ( self . path ) n_channels = int ( self . nChannels ) f = open ( filepath , \"rb\" ) startoffile = f . seek ( 0 , 0 ) endoffile = f . seek ( 0 , 2 ) bytes_size = 2 n_samples = int (( endoffile - startoffile ) / n_channels / bytes_size ) duration = n_samples / frequency f . close () fp = np . memmap ( filepath , np . int16 , \"r\" , shape = ( n_samples , n_channels )) timestep = np . arange ( 0 , n_samples ) / frequency time_support = nap . IntervalSet ( start = 0 , end = duration , time_units = \"s\" ) if channel is None : return nap . TsdFrame ( t = timestep , d = fp , time_units = \"s\" , time_support = time_support ) elif type ( channel ) is int : return nap . Tsd ( t = timestep , d = fp [:, channel ], time_units = \"s\" , time_support = time_support ) elif type ( channel ) is list : return nap . TsdFrame ( t = timestep , d = fp [:, channel ], time_units = \"s\" , time_support = time_support , columns = channel , )","title":"load_lfp()"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.read_neuroscope_intervals","text":"This function reads .evt files in which odd raws indicate the beginning of the time series and the even raws are the ends. If the file is present in the nwb, provide the just the name. If the file is not present in the nwb, it loads the events from the nwb directory. If just the path is provided but not the name, it takes the name from the file. Parameters: Name Type Description Default name str name of the epoch in the nwb file, e.g. \"rem\" or desired name save the data in the nwb. None path2file str Path of the file you want to load. None Returns: Type Description IntervalSet Contains two columns corresponding to the start and end of the intervals. Source code in pynapple/io/neurosuite.py def read_neuroscope_intervals ( self , name = None , path2file = None ): \"\"\" This function reads .evt files in which odd raws indicate the beginning of the time series and the even raws are the ends. If the file is present in the nwb, provide the just the name. If the file is not present in the nwb, it loads the events from the nwb directory. If just the path is provided but not the name, it takes the name from the file. Parameters ---------- name: str name of the epoch in the nwb file, e.g. \"rem\" or desired name save the data in the nwb. path2file: str Path of the file you want to load. Returns ------- IntervalSet Contains two columns corresponding to the start and end of the intervals. \"\"\" if name : isets = self . load_nwb_intervals ( name ) if isinstance ( isets , nap . IntervalSet ): return isets if name is not None and path2file is None : path2file = os . path . join ( self . path , self . basename + \".\" + name + \".evt\" ) if path2file is not None : try : # df = pd.read_csv(path2file, delimiter=' ', usecols = [0], header = None) tmp = np . genfromtxt ( path2file )[:, 0 ] df = tmp . reshape ( len ( tmp ) // 2 , 2 ) except ValueError : print ( \"specify a valid name\" ) isets = nap . IntervalSet ( df [:, 0 ], df [:, 1 ], time_units = \"ms\" ) if name is None : name = path2file . split ( \".\" )[ - 2 ] print ( \"*** saving file in the nwb as\" , name ) self . save_nwb_intervals ( isets , name ) else : raise ValueError ( \"specify a valid path\" ) return isets","title":"read_neuroscope_intervals()"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.write_neuroscope_intervals","text":"Write events to load with neuroscope (e.g. ripples start and ends) Parameters: Name Type Description Default extension str The extension of the file (e.g. basename.evt.py.rip) required isets IntervalSet The IntervalSet to write required name str The name of the events (e.g. Ripples) required Source code in pynapple/io/neurosuite.py def write_neuroscope_intervals ( self , extension , isets , name ): \"\"\"Write events to load with neuroscope (e.g. ripples start and ends) Parameters ---------- extension : str The extension of the file (e.g. basename.evt.py.rip) isets : IntervalSet The IntervalSet to write name : str The name of the events (e.g. Ripples) \"\"\" start = isets . as_units ( \"ms\" )[ \"start\" ] . values ends = isets . as_units ( \"ms\" )[ \"end\" ] . values datatowrite = np . vstack (( start , ends )) . T . flatten () n = len ( isets ) texttowrite = np . vstack ( ( ( np . repeat ( np . array ([ name + \" start\" ]), n )), ( np . repeat ( np . array ([ name + \" end\" ]), n )), ) ) . T . flatten () evt_file = os . path . join ( self . path , self . basename + extension ) f = open ( evt_file , \"w\" ) for t , n in zip ( datatowrite , texttowrite ): f . writelines ( \" {:1.6f} \" . format ( t ) + \" \\t \" + n + \" \\n \" ) f . close () return","title":"write_neuroscope_intervals()"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_mean_waveforms","text":"Load the mean waveforms from a dat file. Parameters: Name Type Description Default epoch IntervalSet default = None Restrict spikes to an epoch. None waveform_window IntervalSet default interval nap.IntervalSet(start = -0.0005, end = 0.001, time_units = 'ms') Limit waveform extraction before and after spike time None spike_count int default = 1000 Number of spikes used per neuron for the calculation of waveforms 1000 Returns: Type Description dictionary the waveforms for all neurons Source code in pynapple/io/neurosuite.py def load_mean_waveforms ( self , epoch = None , waveform_window = None , spike_count = 1000 ): \"\"\" Load the mean waveforms from a dat file. Parameters ---------- epoch : IntervalSet default = None Restrict spikes to an epoch. waveform_window : IntervalSet default interval nap.IntervalSet(start = -0.0005, end = 0.001, time_units = 'ms') Limit waveform extraction before and after spike time spike_count : int default = 1000 Number of spikes used per neuron for the calculation of waveforms Returns ------- dictionary the waveforms for all neurons pandas.Series the channel with the maximum waveform for each neuron \"\"\" if not isinstance ( waveform_window , nap . IntervalSet ): waveform_window = nap . IntervalSet ( start =- 0.5 , end = 1 , time_units = \"ms\" ) spikes = self . spikes if not os . path . exists ( self . path ): # check if path exists print ( \"The path \" + self . path + \" doesn't exist; Exiting ...\" ) sys . exit () # Load XML INFO self . load_neurosuite_xml ( self . path ) n_channels = self . nChannels fs = self . fs_dat group_to_channel = self . group_to_channel group = spikes . get_info ( \"group\" ) # Check if there is an epoch, restrict spike times to epoch if epoch is not None : if type ( epoch ) is not nap . IntervalSet : print ( \"Epoch must be an IntervalSet\" ) sys . exit () else : print ( \"Restricting spikes to epoch\" ) spikes = spikes . restrict ( epoch ) epstart = int ( epoch . as_units ( \"s\" )[ \"start\" ] . values [ 0 ] * fs ) epend = int ( epoch . as_units ( \"s\" )[ \"end\" ] . values [ 0 ] * fs ) # Find dat file files = os . listdir ( self . path ) dat_files = np . sort ([ f for f in files if \"dat\" in f and f [ 0 ] != \".\" ]) # Need n_samples collected in the entire recording from dat file to load file = os . path . join ( self . path , dat_files [ 0 ]) f = open ( file , \"rb\" ) # open file to get number of samples collected in the entire recording startoffile = f . seek ( 0 , 0 ) endoffile = f . seek ( 0 , 2 ) bytes_size = 2 n_samples = int (( endoffile - startoffile ) / n_channels / bytes_size ) f . close () # map to memory all samples for all channels, channels are numbered according to neuroscope number fp = np . memmap ( file , np . int16 , \"r\" , shape = ( n_samples , n_channels )) # convert spike times to spikes in sample number sample_spikes = { neuron : ( spikes [ neuron ] . as_units ( \"s\" ) . index . values * fs ) . astype ( \"int\" ) for neuron in spikes } # prep for waveforms overlap = int ( waveform_window . tot_length ( time_units = \"s\" ) ) # one spike's worth of overlap between windows waveform_window = abs ( np . array ( waveform_window . as_units ( \"s\" ))[ 0 ] * fs ) . astype ( int ) # convert time to sample number neuron_waveforms = { n : np . zeros ([ np . sum ( waveform_window ), len ( group_to_channel [ group [ n ]])]) for n in sample_spikes } # divide dat file into batches that slightly overlap for faster loading batch_size = 3000000 windows = np . arange ( 0 , int ( endoffile / n_channels / bytes_size ), batch_size ) if epoch is not None : print ( \"Restricting dat file to epoch\" ) windows = windows [( windows >= epstart ) & ( windows <= epend )] batches = [] for ( i ) in windows : # make overlapping batches from the beginning to end of recording if i == windows [ - 1 ]: # the last batch cannot overlap with the next one batches . append ([ i , n_samples ]) else : batches . append ([ i , i + batch_size + overlap ]) batches = [ np . int32 ( batch ) for batch in batches ] sample_counted_spikes = {} for index , neuron in enumerate ( sample_spikes ): if len ( sample_spikes [ neuron ]) >= spike_count : sample_counted_spikes [ neuron ] = np . array ( np . random . choice ( list ( sample_spikes [ neuron ]), spike_count ) ) elif len ( sample_spikes [ neuron ]) < spike_count : print ( \"Not enough spikes in neuron \" + str ( index ) + \"... using all spikes\" ) sample_counted_spikes [ neuron ] = sample_spikes [ neuron ] # Make one array containing all selected spike times of all neurons - will be used to check for spikes before loading dat file spike_check = np . array ( [ int ( spikes_neuron ) for spikes_neuron in sample_counted_spikes [ neuron ] for neuron in sample_counted_spikes ] ) for index , timestep in enumerate ( batches ): print ( f \"Extracting waveforms from dat file: window { index + 1 } / { len ( windows ) } \" , end = \" \\r \" , ) if ( len ( spike_check [ ( timestep [ 0 ] < spike_check ) & ( timestep [ 1 ] > spike_check ) ] ) == 0 ): continue # if there are no spikes for any neurons in this batch, skip and go to the next one # Load dat file for timestep tmp = pd . DataFrame ( data = fp [ timestep [ 0 ] : timestep [ 1 ], :], columns = np . arange ( n_channels ), index = range ( timestep [ 0 ], timestep [ 1 ]), ) # load dat file # Check if any spikes are present for neuron in sample_counted_spikes : neurontmp = sample_counted_spikes [ neuron ] tmp2 = neurontmp [( timestep [ 0 ] < neurontmp ) & ( timestep [ 1 ] > neurontmp )] if len ( neurontmp ) == 0 : continue # skip neuron if it has no spikes in this batch tmpn = tmp [ group_to_channel [ group [ neuron ]] ] # restrict dat file to the channel group of the neuron for time in tmp2 : # add each spike waveform to neuron_waveform spikewindow = tmpn . loc [ time - waveform_window [ 0 ] : time + waveform_window [ 1 ] - 1 ] # waveform for this spike time try : neuron_waveforms [ neuron ] += spikewindow . values except ( Exception ): # ignore if full waveform is not present in this batch pass meanwf = { n : pd . DataFrame ( data = np . array ( neuron_waveforms [ n ]) / spike_count , columns = np . arange ( len ( group_to_channel [ group [ n ]])), index = np . array ( np . arange ( - waveform_window [ 0 ], waveform_window [ 1 ])) / fs , ) for n in sample_counted_spikes } # find the max channel for each neuron maxch = pd . Series ( data = [ meanwf [ n ][ meanwf [ n ] . loc [ 0 ] . idxmin ()] . name for n in meanwf ], index = spikes . keys (), ) return meanwf , maxch","title":"load_mean_waveforms()"},{"location":"io.phy/","text":"Class and functions for loading data processed with Phy2 Phy ( BaseLoader ) Loader for Phy data Source code in pynapple/io/phy.py class Phy ( BaseLoader ): \"\"\" Loader for Phy data \"\"\" def __init__ ( self , path ): \"\"\" Instantiate the data class from a Phy folder. Parameters ---------- path : str or Path object The path to the data. \"\"\" self . time_support = None self . sample_rate = None self . n_channels_dat = None self . channel_map = None self . ch_to_sh = None self . spikes = None self . channel_positions = None super () . __init__ ( path ) # This path stuff should happen only once in the parent class self . path = Path ( path ) self . basename = self . path . name self . nwb_path = self . path / \"pynapplenwb\" # from what I can see in the loading function, only one nwb file per folder: try : self . nwb_file = list ( self . nwb_path . glob ( \"*.nwb\" ))[ 0 ] except IndexError : self . nwb_file = None # Need to check if nwb file exists and if data are there # if self.path is not None: -> are there any cases where this is None? if self . nwb_file is not None : loaded_spikes = self . load_nwb_spikes () if loaded_spikes is not None : return # Bypass if data have already been transferred to nwb self . load_phy_params () app = App () window = EphysGUI ( app , path = path , groups = self . channel_map ) app . mainloop () try : app . update () except Exception : pass if window . status : self . ephys_information = window . ephys_information self . load_phy_spikes ( self . time_support ) self . save_data () app . quit () def load_phy_params ( self ): \"\"\" path should be the folder session containing the params.py file Function reads : 1. the number of channels 2. the sampling frequency of the dat file Raises ------ AssertionError If path does not contain the params file or channel_map.npy \"\"\" assert ( self . path / \"params.py\" ) . exists (), f \"Can't find params.py in { self . path } \" # It is strongly recommended not to conflate parameters and code! Also, there's a library called params. # I would recommend putting in the folder a file called params.json, or .txt, or .yml, but not .py! # In this way we just read the file, and we don't have to add to sys to import... # TODO maybe remove this sys . path . append ( str ( self . path )) import params as params self . sample_rate = params . sample_rate self . n_channels_dat = params . n_channels_dat assert ( self . path / \"channel_map.npy\" ) . exists (), f \"Can't find channel_map.npy in { self . path } \" channel_map = np . load ( self . path / \"channel_map.npy\" ) if ( self . path / \"channel_shanks.npy\" ) . exists (): channel_shank = np . load ( self . path / \"channel_shanks.npy\" ) n_shanks = len ( np . unique ( channel_shank )) self . channel_map = { i : channel_map [ channel_shank == i ] for i in range ( n_shanks ) } self . ch_to_sh = pd . Series ( index = channel_map . flatten (), data = channel_shank . flatten (), ) else : self . channel_map = { i : channel_map [ i ] for i in range ( len ( channel_map ))} self . ch_to_sh = pd . Series ( index = channel_map . flatten (), data = np . hstack ( [ np . ones ( len ( channel_map [ i ]), dtype = int ) * i for i in range ( len ( channel_map )) ] ), ) return def load_phy_spikes ( self , time_support = None ): \"\"\" Load Phy spike times and convert to NWB. Instantiate automatically a TsGroup object. The cluster group is taken first from cluster_info.tsv and second from cluster_group.tsv Parameters ---------- path : Path object The path to the data time_support : IntevalSet, optional The time support of the data Raises ------ RuntimeError If files are missing. The function needs : - cluster_info.tsv or cluster_group.tsv - spike_times.npy - spike_clusters.npy - channel_positions.npy - templates.npy \"\"\" # Check if cluster_info.tsv or cluster_group.tsv exists. If both exist, cluster_info.tsv is used: has_cluster_info = False if ( self . path / \"cluster_info.tsv\" ) . exists (): cluster_info_file = self . path / \"cluster_info.tsv\" has_cluster_info = True elif ( self . path / \"cluster_group.tsv\" ) . exists (): cluster_info_file = self . path / \"cluster_group.tsv\" else : raise RuntimeError ( \"Can't find cluster_info.tsv or cluster_group.tsv in {} ;\" . format ( self . path ) ) cluster_info = pd . read_csv ( cluster_info_file , sep = \" \\t \" , index_col = \"cluster_id\" ) # In my processed data with KiloSort 3.0, the column is named KSLabel if \"group\" in cluster_info . columns : cluster_id_good = cluster_info [ cluster_info . group == \"good\" ] . index . values elif \"KSLabel\" in cluster_info . columns : cluster_id_good = cluster_info [ cluster_info . KSLabel == \"good\" ] . index . values else : raise RuntimeError ( \"Can't find column group or KSLabel in {} ;\" . format ( cluster_info_file ) ) spike_times = np . load ( self . path / \"spike_times.npy\" ) spike_clusters = np . load ( self . path / \"spike_clusters.npy\" ) spikes = {} for n in cluster_id_good : spikes [ n ] = nap . Ts ( t = spike_times [ spike_clusters == n ] / self . sample_rate , time_support = time_support , ) self . spikes = nap . TsGroup ( spikes , time_support = time_support ) # Adding the position of the electrodes in case self . channel_positions = np . load ( self . path / \"channel_positions.npy\" ) # Adding shank group info from cluster_info if present if has_cluster_info : group = cluster_info . loc [ cluster_id_good , \"sh\" ] self . spikes . set_info ( group = group ) else : template = np . load ( self . path / \"templates.npy\" ) template = template [ cluster_id_good ] ch = np . power ( template , 2 ) . max ( 1 ) . argmax ( 1 ) group = pd . Series ( index = cluster_id_good , data = self . ch_to_sh [ ch ] . values ) self . spikes . set_info ( group = group ) names = pd . Series ( index = group . index , data = [ self . ephys_information [ group . loc [ i ]][ \"name\" ] for i in group . index ], ) if ~ np . all ( names . values == \"\" ): self . spikes . set_info ( name = names ) locations = pd . Series ( index = group . index , data = [ self . ephys_information [ group . loc [ i ]][ \"location\" ] for i in group . index ], ) if ~ np . all ( locations . values == \"\" ): self . spikes . set_info ( location = locations ) return def save_data ( self ): \"\"\"Save the data to NWB format.\"\"\" io = NWBHDF5IO ( self . nwb_file , \"r+\" ) nwbfile = io . read () electrode_groups = {} for g in self . channel_map : device = nwbfile . create_device ( name = self . ephys_information [ g ][ \"device\" ][ \"name\" ] + \"-\" + str ( g ), description = self . ephys_information [ g ][ \"device\" ][ \"description\" ], manufacturer = self . ephys_information [ g ][ \"device\" ][ \"manufacturer\" ], ) if ( len ( self . ephys_information [ g ][ \"position\" ]) and type ( self . ephys_information [ g ][ \"position\" ]) is str ): self . ephys_information [ g ][ \"position\" ] = re . split ( \";|,| \" , self . ephys_information [ g ][ \"position\" ] ) elif self . ephys_information [ g ][ \"position\" ] == \"\" : self . ephys_information [ g ][ \"position\" ] = None electrode_groups [ g ] = nwbfile . create_electrode_group ( name = \"group\" + str ( g ) + \"_\" + self . ephys_information [ g ][ \"name\" ], description = self . ephys_information [ g ][ \"description\" ], position = self . ephys_information [ g ][ \"position\" ], location = self . ephys_information [ g ][ \"location\" ], device = device , ) for idx in self . channel_map [ g ]: nwbfile . add_electrode ( id = idx , x = 0.0 , y = 0.0 , z = 0.0 , imp = 0.0 , location = self . ephys_information [ g ][ \"location\" ], filtering = \"none\" , group = electrode_groups [ g ], ) # Adding units nwbfile . add_unit_column ( \"location\" , \"the anatomical location of this unit\" ) nwbfile . add_unit_column ( \"group\" , \"the group of the unit\" ) for u in self . spikes . keys (): nwbfile . add_unit ( id = u , spike_times = self . spikes [ u ] . as_units ( \"s\" ) . index . values , electrode_group = electrode_groups [ self . spikes . get_info ( \"group\" ) . loc [ u ]], location = self . ephys_information [ self . spikes . get_info ( \"group\" ) . loc [ u ]][ \"location\" ], group = self . spikes . get_info ( \"group\" ) . loc [ u ], ) io . write ( nwbfile ) io . close () return def load_nwb_spikes ( self ): \"\"\"Read the NWB spikes to extract the spike times. Returns ------- TYPE Description \"\"\" io = NWBHDF5IO ( self . nwb_file , \"r\" ) nwbfile = io . read () if nwbfile . units is None : io . close () return None else : units = nwbfile . units . to_dataframe () spikes = { n : nap . Ts ( t = units . loc [ n , \"spike_times\" ], time_units = \"s\" ) for n in units . index } self . spikes = nap . TsGroup ( spikes , time_support = self . time_support , time_units = \"s\" , group = units [ \"group\" ], ) if ~ np . all ( units [ \"location\" ] == \"\" ): self . spikes . set_info ( location = units [ \"location\" ]) io . close () return True def load_lfp ( self , filename = None , channel = None , extension = \".eeg\" , frequency = 1250.0 , precision = \"int16\" , bytes_size = 2 , ): \"\"\" Load the LFP. Parameters ---------- filename : str, optional The filename of the lfp file. It can be useful it multiple dat files are present in the data directory channel : int or list of int, optional The channel(s) to load. If None return a memory map of the dat file to avoid memory error extension : str, optional The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match frequency : float, optional Default 1250 Hz for the eeg file precision : str, optional The precision of the binary file bytes_size : int, optional Bytes size of the lfp file Raises ------ RuntimeError If can't find the lfp/eeg/dat file Returns ------- Tsd or TsdFrame The lfp in a time series format \"\"\" if filename is not None : filepath = self . path / filename else : try : filepath = list ( self . path . glob ( f \"* { extension } \" ))[ 0 ] except IndexError : raise RuntimeError ( f \"Path { self . path } contains no { extension } files;\" ) # is it possible that this is a leftover from neurosuite data? # This is not implemented for this class. self . load_neurosuite_xml ( self . path ) n_channels = int ( self . nChannels ) f = open ( filepath , \"rb\" ) startoffile = f . seek ( 0 , 0 ) endoffile = f . seek ( 0 , 2 ) bytes_size = 2 n_samples = int (( endoffile - startoffile ) / n_channels / bytes_size ) duration = n_samples / frequency f . close () fp = np . memmap ( filepath , np . int16 , \"r\" , shape = ( n_samples , n_channels )) timestep = np . arange ( 0 , n_samples ) / frequency time_support = nap . IntervalSet ( start = 0 , end = duration , time_units = \"s\" ) if channel is None : return nap . TsdFrame ( t = timestep , d = fp , time_units = \"s\" , time_support = time_support ) elif type ( channel ) is int : return nap . Tsd ( t = timestep , d = fp [:, channel ], time_units = \"s\" , time_support = time_support ) elif type ( channel ) is list : return nap . TsdFrame ( t = timestep , d = fp [:, channel ], time_units = \"s\" , time_support = time_support , columns = channel , ) __init__ ( self , path ) special Instantiate the data class from a Phy folder. Parameters: Name Type Description Default path str or Path object The path to the data. required Source code in pynapple/io/phy.py def __init__ ( self , path ): \"\"\" Instantiate the data class from a Phy folder. Parameters ---------- path : str or Path object The path to the data. \"\"\" self . time_support = None self . sample_rate = None self . n_channels_dat = None self . channel_map = None self . ch_to_sh = None self . spikes = None self . channel_positions = None super () . __init__ ( path ) # This path stuff should happen only once in the parent class self . path = Path ( path ) self . basename = self . path . name self . nwb_path = self . path / \"pynapplenwb\" # from what I can see in the loading function, only one nwb file per folder: try : self . nwb_file = list ( self . nwb_path . glob ( \"*.nwb\" ))[ 0 ] except IndexError : self . nwb_file = None # Need to check if nwb file exists and if data are there # if self.path is not None: -> are there any cases where this is None? if self . nwb_file is not None : loaded_spikes = self . load_nwb_spikes () if loaded_spikes is not None : return # Bypass if data have already been transferred to nwb self . load_phy_params () app = App () window = EphysGUI ( app , path = path , groups = self . channel_map ) app . mainloop () try : app . update () except Exception : pass if window . status : self . ephys_information = window . ephys_information self . load_phy_spikes ( self . time_support ) self . save_data () app . quit () load_phy_params ( self ) path should be the folder session containing the params.py file Function reads : 1. the number of channels 2. the sampling frequency of the dat file Exceptions: Type Description AssertionError If path does not contain the params file or channel_map.npy Source code in pynapple/io/phy.py def load_phy_params ( self ): \"\"\" path should be the folder session containing the params.py file Function reads : 1. the number of channels 2. the sampling frequency of the dat file Raises ------ AssertionError If path does not contain the params file or channel_map.npy \"\"\" assert ( self . path / \"params.py\" ) . exists (), f \"Can't find params.py in { self . path } \" # It is strongly recommended not to conflate parameters and code! Also, there's a library called params. # I would recommend putting in the folder a file called params.json, or .txt, or .yml, but not .py! # In this way we just read the file, and we don't have to add to sys to import... # TODO maybe remove this sys . path . append ( str ( self . path )) import params as params self . sample_rate = params . sample_rate self . n_channels_dat = params . n_channels_dat assert ( self . path / \"channel_map.npy\" ) . exists (), f \"Can't find channel_map.npy in { self . path } \" channel_map = np . load ( self . path / \"channel_map.npy\" ) if ( self . path / \"channel_shanks.npy\" ) . exists (): channel_shank = np . load ( self . path / \"channel_shanks.npy\" ) n_shanks = len ( np . unique ( channel_shank )) self . channel_map = { i : channel_map [ channel_shank == i ] for i in range ( n_shanks ) } self . ch_to_sh = pd . Series ( index = channel_map . flatten (), data = channel_shank . flatten (), ) else : self . channel_map = { i : channel_map [ i ] for i in range ( len ( channel_map ))} self . ch_to_sh = pd . Series ( index = channel_map . flatten (), data = np . hstack ( [ np . ones ( len ( channel_map [ i ]), dtype = int ) * i for i in range ( len ( channel_map )) ] ), ) return load_phy_spikes ( self , time_support = None ) Load Phy spike times and convert to NWB. Instantiate automatically a TsGroup object. The cluster group is taken first from cluster_info.tsv and second from cluster_group.tsv Parameters: Name Type Description Default path Path object The path to the data required time_support IntevalSet The time support of the data None Exceptions: Type Description RuntimeError If files are missing. The function needs : - cluster_info.tsv or cluster_group.tsv - spike_times.npy - spike_clusters.npy - channel_positions.npy - templates.npy Source code in pynapple/io/phy.py def load_phy_spikes ( self , time_support = None ): \"\"\" Load Phy spike times and convert to NWB. Instantiate automatically a TsGroup object. The cluster group is taken first from cluster_info.tsv and second from cluster_group.tsv Parameters ---------- path : Path object The path to the data time_support : IntevalSet, optional The time support of the data Raises ------ RuntimeError If files are missing. The function needs : - cluster_info.tsv or cluster_group.tsv - spike_times.npy - spike_clusters.npy - channel_positions.npy - templates.npy \"\"\" # Check if cluster_info.tsv or cluster_group.tsv exists. If both exist, cluster_info.tsv is used: has_cluster_info = False if ( self . path / \"cluster_info.tsv\" ) . exists (): cluster_info_file = self . path / \"cluster_info.tsv\" has_cluster_info = True elif ( self . path / \"cluster_group.tsv\" ) . exists (): cluster_info_file = self . path / \"cluster_group.tsv\" else : raise RuntimeError ( \"Can't find cluster_info.tsv or cluster_group.tsv in {} ;\" . format ( self . path ) ) cluster_info = pd . read_csv ( cluster_info_file , sep = \" \\t \" , index_col = \"cluster_id\" ) # In my processed data with KiloSort 3.0, the column is named KSLabel if \"group\" in cluster_info . columns : cluster_id_good = cluster_info [ cluster_info . group == \"good\" ] . index . values elif \"KSLabel\" in cluster_info . columns : cluster_id_good = cluster_info [ cluster_info . KSLabel == \"good\" ] . index . values else : raise RuntimeError ( \"Can't find column group or KSLabel in {} ;\" . format ( cluster_info_file ) ) spike_times = np . load ( self . path / \"spike_times.npy\" ) spike_clusters = np . load ( self . path / \"spike_clusters.npy\" ) spikes = {} for n in cluster_id_good : spikes [ n ] = nap . Ts ( t = spike_times [ spike_clusters == n ] / self . sample_rate , time_support = time_support , ) self . spikes = nap . TsGroup ( spikes , time_support = time_support ) # Adding the position of the electrodes in case self . channel_positions = np . load ( self . path / \"channel_positions.npy\" ) # Adding shank group info from cluster_info if present if has_cluster_info : group = cluster_info . loc [ cluster_id_good , \"sh\" ] self . spikes . set_info ( group = group ) else : template = np . load ( self . path / \"templates.npy\" ) template = template [ cluster_id_good ] ch = np . power ( template , 2 ) . max ( 1 ) . argmax ( 1 ) group = pd . Series ( index = cluster_id_good , data = self . ch_to_sh [ ch ] . values ) self . spikes . set_info ( group = group ) names = pd . Series ( index = group . index , data = [ self . ephys_information [ group . loc [ i ]][ \"name\" ] for i in group . index ], ) if ~ np . all ( names . values == \"\" ): self . spikes . set_info ( name = names ) locations = pd . Series ( index = group . index , data = [ self . ephys_information [ group . loc [ i ]][ \"location\" ] for i in group . index ], ) if ~ np . all ( locations . values == \"\" ): self . spikes . set_info ( location = locations ) return save_data ( self ) Save the data to NWB format. Source code in pynapple/io/phy.py def save_data ( self ): \"\"\"Save the data to NWB format.\"\"\" io = NWBHDF5IO ( self . nwb_file , \"r+\" ) nwbfile = io . read () electrode_groups = {} for g in self . channel_map : device = nwbfile . create_device ( name = self . ephys_information [ g ][ \"device\" ][ \"name\" ] + \"-\" + str ( g ), description = self . ephys_information [ g ][ \"device\" ][ \"description\" ], manufacturer = self . ephys_information [ g ][ \"device\" ][ \"manufacturer\" ], ) if ( len ( self . ephys_information [ g ][ \"position\" ]) and type ( self . ephys_information [ g ][ \"position\" ]) is str ): self . ephys_information [ g ][ \"position\" ] = re . split ( \";|,| \" , self . ephys_information [ g ][ \"position\" ] ) elif self . ephys_information [ g ][ \"position\" ] == \"\" : self . ephys_information [ g ][ \"position\" ] = None electrode_groups [ g ] = nwbfile . create_electrode_group ( name = \"group\" + str ( g ) + \"_\" + self . ephys_information [ g ][ \"name\" ], description = self . ephys_information [ g ][ \"description\" ], position = self . ephys_information [ g ][ \"position\" ], location = self . ephys_information [ g ][ \"location\" ], device = device , ) for idx in self . channel_map [ g ]: nwbfile . add_electrode ( id = idx , x = 0.0 , y = 0.0 , z = 0.0 , imp = 0.0 , location = self . ephys_information [ g ][ \"location\" ], filtering = \"none\" , group = electrode_groups [ g ], ) # Adding units nwbfile . add_unit_column ( \"location\" , \"the anatomical location of this unit\" ) nwbfile . add_unit_column ( \"group\" , \"the group of the unit\" ) for u in self . spikes . keys (): nwbfile . add_unit ( id = u , spike_times = self . spikes [ u ] . as_units ( \"s\" ) . index . values , electrode_group = electrode_groups [ self . spikes . get_info ( \"group\" ) . loc [ u ]], location = self . ephys_information [ self . spikes . get_info ( \"group\" ) . loc [ u ]][ \"location\" ], group = self . spikes . get_info ( \"group\" ) . loc [ u ], ) io . write ( nwbfile ) io . close () return load_nwb_spikes ( self ) Read the NWB spikes to extract the spike times. Returns: Type Description TYPE Description Source code in pynapple/io/phy.py def load_nwb_spikes ( self ): \"\"\"Read the NWB spikes to extract the spike times. Returns ------- TYPE Description \"\"\" io = NWBHDF5IO ( self . nwb_file , \"r\" ) nwbfile = io . read () if nwbfile . units is None : io . close () return None else : units = nwbfile . units . to_dataframe () spikes = { n : nap . Ts ( t = units . loc [ n , \"spike_times\" ], time_units = \"s\" ) for n in units . index } self . spikes = nap . TsGroup ( spikes , time_support = self . time_support , time_units = \"s\" , group = units [ \"group\" ], ) if ~ np . all ( units [ \"location\" ] == \"\" ): self . spikes . set_info ( location = units [ \"location\" ]) io . close () return True load_lfp ( self , filename = None , channel = None , extension = '.eeg' , frequency = 1250.0 , precision = 'int16' , bytes_size = 2 ) Load the LFP. Parameters: Name Type Description Default filename str The filename of the lfp file. It can be useful it multiple dat files are present in the data directory None channel int or list of int The channel(s) to load. If None return a memory map of the dat file to avoid memory error None extension str The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match '.eeg' frequency float Default 1250 Hz for the eeg file 1250.0 precision str The precision of the binary file 'int16' bytes_size int Bytes size of the lfp file 2 Exceptions: Type Description RuntimeError If can't find the lfp/eeg/dat file Returns: Type Description Tsd or TsdFrame The lfp in a time series format Source code in pynapple/io/phy.py def load_lfp ( self , filename = None , channel = None , extension = \".eeg\" , frequency = 1250.0 , precision = \"int16\" , bytes_size = 2 , ): \"\"\" Load the LFP. Parameters ---------- filename : str, optional The filename of the lfp file. It can be useful it multiple dat files are present in the data directory channel : int or list of int, optional The channel(s) to load. If None return a memory map of the dat file to avoid memory error extension : str, optional The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match frequency : float, optional Default 1250 Hz for the eeg file precision : str, optional The precision of the binary file bytes_size : int, optional Bytes size of the lfp file Raises ------ RuntimeError If can't find the lfp/eeg/dat file Returns ------- Tsd or TsdFrame The lfp in a time series format \"\"\" if filename is not None : filepath = self . path / filename else : try : filepath = list ( self . path . glob ( f \"* { extension } \" ))[ 0 ] except IndexError : raise RuntimeError ( f \"Path { self . path } contains no { extension } files;\" ) # is it possible that this is a leftover from neurosuite data? # This is not implemented for this class. self . load_neurosuite_xml ( self . path ) n_channels = int ( self . nChannels ) f = open ( filepath , \"rb\" ) startoffile = f . seek ( 0 , 0 ) endoffile = f . seek ( 0 , 2 ) bytes_size = 2 n_samples = int (( endoffile - startoffile ) / n_channels / bytes_size ) duration = n_samples / frequency f . close () fp = np . memmap ( filepath , np . int16 , \"r\" , shape = ( n_samples , n_channels )) timestep = np . arange ( 0 , n_samples ) / frequency time_support = nap . IntervalSet ( start = 0 , end = duration , time_units = \"s\" ) if channel is None : return nap . TsdFrame ( t = timestep , d = fp , time_units = \"s\" , time_support = time_support ) elif type ( channel ) is int : return nap . Tsd ( t = timestep , d = fp [:, channel ], time_units = \"s\" , time_support = time_support ) elif type ( channel ) is list : return nap . TsdFrame ( t = timestep , d = fp [:, channel ], time_units = \"s\" , time_support = time_support , columns = channel , )","title":"Phy"},{"location":"io.phy/#pynapple.io.phy.Phy","text":"Loader for Phy data Source code in pynapple/io/phy.py class Phy ( BaseLoader ): \"\"\" Loader for Phy data \"\"\" def __init__ ( self , path ): \"\"\" Instantiate the data class from a Phy folder. Parameters ---------- path : str or Path object The path to the data. \"\"\" self . time_support = None self . sample_rate = None self . n_channels_dat = None self . channel_map = None self . ch_to_sh = None self . spikes = None self . channel_positions = None super () . __init__ ( path ) # This path stuff should happen only once in the parent class self . path = Path ( path ) self . basename = self . path . name self . nwb_path = self . path / \"pynapplenwb\" # from what I can see in the loading function, only one nwb file per folder: try : self . nwb_file = list ( self . nwb_path . glob ( \"*.nwb\" ))[ 0 ] except IndexError : self . nwb_file = None # Need to check if nwb file exists and if data are there # if self.path is not None: -> are there any cases where this is None? if self . nwb_file is not None : loaded_spikes = self . load_nwb_spikes () if loaded_spikes is not None : return # Bypass if data have already been transferred to nwb self . load_phy_params () app = App () window = EphysGUI ( app , path = path , groups = self . channel_map ) app . mainloop () try : app . update () except Exception : pass if window . status : self . ephys_information = window . ephys_information self . load_phy_spikes ( self . time_support ) self . save_data () app . quit () def load_phy_params ( self ): \"\"\" path should be the folder session containing the params.py file Function reads : 1. the number of channels 2. the sampling frequency of the dat file Raises ------ AssertionError If path does not contain the params file or channel_map.npy \"\"\" assert ( self . path / \"params.py\" ) . exists (), f \"Can't find params.py in { self . path } \" # It is strongly recommended not to conflate parameters and code! Also, there's a library called params. # I would recommend putting in the folder a file called params.json, or .txt, or .yml, but not .py! # In this way we just read the file, and we don't have to add to sys to import... # TODO maybe remove this sys . path . append ( str ( self . path )) import params as params self . sample_rate = params . sample_rate self . n_channels_dat = params . n_channels_dat assert ( self . path / \"channel_map.npy\" ) . exists (), f \"Can't find channel_map.npy in { self . path } \" channel_map = np . load ( self . path / \"channel_map.npy\" ) if ( self . path / \"channel_shanks.npy\" ) . exists (): channel_shank = np . load ( self . path / \"channel_shanks.npy\" ) n_shanks = len ( np . unique ( channel_shank )) self . channel_map = { i : channel_map [ channel_shank == i ] for i in range ( n_shanks ) } self . ch_to_sh = pd . Series ( index = channel_map . flatten (), data = channel_shank . flatten (), ) else : self . channel_map = { i : channel_map [ i ] for i in range ( len ( channel_map ))} self . ch_to_sh = pd . Series ( index = channel_map . flatten (), data = np . hstack ( [ np . ones ( len ( channel_map [ i ]), dtype = int ) * i for i in range ( len ( channel_map )) ] ), ) return def load_phy_spikes ( self , time_support = None ): \"\"\" Load Phy spike times and convert to NWB. Instantiate automatically a TsGroup object. The cluster group is taken first from cluster_info.tsv and second from cluster_group.tsv Parameters ---------- path : Path object The path to the data time_support : IntevalSet, optional The time support of the data Raises ------ RuntimeError If files are missing. The function needs : - cluster_info.tsv or cluster_group.tsv - spike_times.npy - spike_clusters.npy - channel_positions.npy - templates.npy \"\"\" # Check if cluster_info.tsv or cluster_group.tsv exists. If both exist, cluster_info.tsv is used: has_cluster_info = False if ( self . path / \"cluster_info.tsv\" ) . exists (): cluster_info_file = self . path / \"cluster_info.tsv\" has_cluster_info = True elif ( self . path / \"cluster_group.tsv\" ) . exists (): cluster_info_file = self . path / \"cluster_group.tsv\" else : raise RuntimeError ( \"Can't find cluster_info.tsv or cluster_group.tsv in {} ;\" . format ( self . path ) ) cluster_info = pd . read_csv ( cluster_info_file , sep = \" \\t \" , index_col = \"cluster_id\" ) # In my processed data with KiloSort 3.0, the column is named KSLabel if \"group\" in cluster_info . columns : cluster_id_good = cluster_info [ cluster_info . group == \"good\" ] . index . values elif \"KSLabel\" in cluster_info . columns : cluster_id_good = cluster_info [ cluster_info . KSLabel == \"good\" ] . index . values else : raise RuntimeError ( \"Can't find column group or KSLabel in {} ;\" . format ( cluster_info_file ) ) spike_times = np . load ( self . path / \"spike_times.npy\" ) spike_clusters = np . load ( self . path / \"spike_clusters.npy\" ) spikes = {} for n in cluster_id_good : spikes [ n ] = nap . Ts ( t = spike_times [ spike_clusters == n ] / self . sample_rate , time_support = time_support , ) self . spikes = nap . TsGroup ( spikes , time_support = time_support ) # Adding the position of the electrodes in case self . channel_positions = np . load ( self . path / \"channel_positions.npy\" ) # Adding shank group info from cluster_info if present if has_cluster_info : group = cluster_info . loc [ cluster_id_good , \"sh\" ] self . spikes . set_info ( group = group ) else : template = np . load ( self . path / \"templates.npy\" ) template = template [ cluster_id_good ] ch = np . power ( template , 2 ) . max ( 1 ) . argmax ( 1 ) group = pd . Series ( index = cluster_id_good , data = self . ch_to_sh [ ch ] . values ) self . spikes . set_info ( group = group ) names = pd . Series ( index = group . index , data = [ self . ephys_information [ group . loc [ i ]][ \"name\" ] for i in group . index ], ) if ~ np . all ( names . values == \"\" ): self . spikes . set_info ( name = names ) locations = pd . Series ( index = group . index , data = [ self . ephys_information [ group . loc [ i ]][ \"location\" ] for i in group . index ], ) if ~ np . all ( locations . values == \"\" ): self . spikes . set_info ( location = locations ) return def save_data ( self ): \"\"\"Save the data to NWB format.\"\"\" io = NWBHDF5IO ( self . nwb_file , \"r+\" ) nwbfile = io . read () electrode_groups = {} for g in self . channel_map : device = nwbfile . create_device ( name = self . ephys_information [ g ][ \"device\" ][ \"name\" ] + \"-\" + str ( g ), description = self . ephys_information [ g ][ \"device\" ][ \"description\" ], manufacturer = self . ephys_information [ g ][ \"device\" ][ \"manufacturer\" ], ) if ( len ( self . ephys_information [ g ][ \"position\" ]) and type ( self . ephys_information [ g ][ \"position\" ]) is str ): self . ephys_information [ g ][ \"position\" ] = re . split ( \";|,| \" , self . ephys_information [ g ][ \"position\" ] ) elif self . ephys_information [ g ][ \"position\" ] == \"\" : self . ephys_information [ g ][ \"position\" ] = None electrode_groups [ g ] = nwbfile . create_electrode_group ( name = \"group\" + str ( g ) + \"_\" + self . ephys_information [ g ][ \"name\" ], description = self . ephys_information [ g ][ \"description\" ], position = self . ephys_information [ g ][ \"position\" ], location = self . ephys_information [ g ][ \"location\" ], device = device , ) for idx in self . channel_map [ g ]: nwbfile . add_electrode ( id = idx , x = 0.0 , y = 0.0 , z = 0.0 , imp = 0.0 , location = self . ephys_information [ g ][ \"location\" ], filtering = \"none\" , group = electrode_groups [ g ], ) # Adding units nwbfile . add_unit_column ( \"location\" , \"the anatomical location of this unit\" ) nwbfile . add_unit_column ( \"group\" , \"the group of the unit\" ) for u in self . spikes . keys (): nwbfile . add_unit ( id = u , spike_times = self . spikes [ u ] . as_units ( \"s\" ) . index . values , electrode_group = electrode_groups [ self . spikes . get_info ( \"group\" ) . loc [ u ]], location = self . ephys_information [ self . spikes . get_info ( \"group\" ) . loc [ u ]][ \"location\" ], group = self . spikes . get_info ( \"group\" ) . loc [ u ], ) io . write ( nwbfile ) io . close () return def load_nwb_spikes ( self ): \"\"\"Read the NWB spikes to extract the spike times. Returns ------- TYPE Description \"\"\" io = NWBHDF5IO ( self . nwb_file , \"r\" ) nwbfile = io . read () if nwbfile . units is None : io . close () return None else : units = nwbfile . units . to_dataframe () spikes = { n : nap . Ts ( t = units . loc [ n , \"spike_times\" ], time_units = \"s\" ) for n in units . index } self . spikes = nap . TsGroup ( spikes , time_support = self . time_support , time_units = \"s\" , group = units [ \"group\" ], ) if ~ np . all ( units [ \"location\" ] == \"\" ): self . spikes . set_info ( location = units [ \"location\" ]) io . close () return True def load_lfp ( self , filename = None , channel = None , extension = \".eeg\" , frequency = 1250.0 , precision = \"int16\" , bytes_size = 2 , ): \"\"\" Load the LFP. Parameters ---------- filename : str, optional The filename of the lfp file. It can be useful it multiple dat files are present in the data directory channel : int or list of int, optional The channel(s) to load. If None return a memory map of the dat file to avoid memory error extension : str, optional The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match frequency : float, optional Default 1250 Hz for the eeg file precision : str, optional The precision of the binary file bytes_size : int, optional Bytes size of the lfp file Raises ------ RuntimeError If can't find the lfp/eeg/dat file Returns ------- Tsd or TsdFrame The lfp in a time series format \"\"\" if filename is not None : filepath = self . path / filename else : try : filepath = list ( self . path . glob ( f \"* { extension } \" ))[ 0 ] except IndexError : raise RuntimeError ( f \"Path { self . path } contains no { extension } files;\" ) # is it possible that this is a leftover from neurosuite data? # This is not implemented for this class. self . load_neurosuite_xml ( self . path ) n_channels = int ( self . nChannels ) f = open ( filepath , \"rb\" ) startoffile = f . seek ( 0 , 0 ) endoffile = f . seek ( 0 , 2 ) bytes_size = 2 n_samples = int (( endoffile - startoffile ) / n_channels / bytes_size ) duration = n_samples / frequency f . close () fp = np . memmap ( filepath , np . int16 , \"r\" , shape = ( n_samples , n_channels )) timestep = np . arange ( 0 , n_samples ) / frequency time_support = nap . IntervalSet ( start = 0 , end = duration , time_units = \"s\" ) if channel is None : return nap . TsdFrame ( t = timestep , d = fp , time_units = \"s\" , time_support = time_support ) elif type ( channel ) is int : return nap . Tsd ( t = timestep , d = fp [:, channel ], time_units = \"s\" , time_support = time_support ) elif type ( channel ) is list : return nap . TsdFrame ( t = timestep , d = fp [:, channel ], time_units = \"s\" , time_support = time_support , columns = channel , )","title":"Phy"},{"location":"io.phy/#pynapple.io.phy.Phy.__init__","text":"Instantiate the data class from a Phy folder. Parameters: Name Type Description Default path str or Path object The path to the data. required Source code in pynapple/io/phy.py def __init__ ( self , path ): \"\"\" Instantiate the data class from a Phy folder. Parameters ---------- path : str or Path object The path to the data. \"\"\" self . time_support = None self . sample_rate = None self . n_channels_dat = None self . channel_map = None self . ch_to_sh = None self . spikes = None self . channel_positions = None super () . __init__ ( path ) # This path stuff should happen only once in the parent class self . path = Path ( path ) self . basename = self . path . name self . nwb_path = self . path / \"pynapplenwb\" # from what I can see in the loading function, only one nwb file per folder: try : self . nwb_file = list ( self . nwb_path . glob ( \"*.nwb\" ))[ 0 ] except IndexError : self . nwb_file = None # Need to check if nwb file exists and if data are there # if self.path is not None: -> are there any cases where this is None? if self . nwb_file is not None : loaded_spikes = self . load_nwb_spikes () if loaded_spikes is not None : return # Bypass if data have already been transferred to nwb self . load_phy_params () app = App () window = EphysGUI ( app , path = path , groups = self . channel_map ) app . mainloop () try : app . update () except Exception : pass if window . status : self . ephys_information = window . ephys_information self . load_phy_spikes ( self . time_support ) self . save_data () app . quit ()","title":"__init__()"},{"location":"io.phy/#pynapple.io.phy.Phy.load_phy_params","text":"path should be the folder session containing the params.py file Function reads : 1. the number of channels 2. the sampling frequency of the dat file Exceptions: Type Description AssertionError If path does not contain the params file or channel_map.npy Source code in pynapple/io/phy.py def load_phy_params ( self ): \"\"\" path should be the folder session containing the params.py file Function reads : 1. the number of channels 2. the sampling frequency of the dat file Raises ------ AssertionError If path does not contain the params file or channel_map.npy \"\"\" assert ( self . path / \"params.py\" ) . exists (), f \"Can't find params.py in { self . path } \" # It is strongly recommended not to conflate parameters and code! Also, there's a library called params. # I would recommend putting in the folder a file called params.json, or .txt, or .yml, but not .py! # In this way we just read the file, and we don't have to add to sys to import... # TODO maybe remove this sys . path . append ( str ( self . path )) import params as params self . sample_rate = params . sample_rate self . n_channels_dat = params . n_channels_dat assert ( self . path / \"channel_map.npy\" ) . exists (), f \"Can't find channel_map.npy in { self . path } \" channel_map = np . load ( self . path / \"channel_map.npy\" ) if ( self . path / \"channel_shanks.npy\" ) . exists (): channel_shank = np . load ( self . path / \"channel_shanks.npy\" ) n_shanks = len ( np . unique ( channel_shank )) self . channel_map = { i : channel_map [ channel_shank == i ] for i in range ( n_shanks ) } self . ch_to_sh = pd . Series ( index = channel_map . flatten (), data = channel_shank . flatten (), ) else : self . channel_map = { i : channel_map [ i ] for i in range ( len ( channel_map ))} self . ch_to_sh = pd . Series ( index = channel_map . flatten (), data = np . hstack ( [ np . ones ( len ( channel_map [ i ]), dtype = int ) * i for i in range ( len ( channel_map )) ] ), ) return","title":"load_phy_params()"},{"location":"io.phy/#pynapple.io.phy.Phy.load_phy_spikes","text":"Load Phy spike times and convert to NWB. Instantiate automatically a TsGroup object. The cluster group is taken first from cluster_info.tsv and second from cluster_group.tsv Parameters: Name Type Description Default path Path object The path to the data required time_support IntevalSet The time support of the data None Exceptions: Type Description RuntimeError If files are missing. The function needs : - cluster_info.tsv or cluster_group.tsv - spike_times.npy - spike_clusters.npy - channel_positions.npy - templates.npy Source code in pynapple/io/phy.py def load_phy_spikes ( self , time_support = None ): \"\"\" Load Phy spike times and convert to NWB. Instantiate automatically a TsGroup object. The cluster group is taken first from cluster_info.tsv and second from cluster_group.tsv Parameters ---------- path : Path object The path to the data time_support : IntevalSet, optional The time support of the data Raises ------ RuntimeError If files are missing. The function needs : - cluster_info.tsv or cluster_group.tsv - spike_times.npy - spike_clusters.npy - channel_positions.npy - templates.npy \"\"\" # Check if cluster_info.tsv or cluster_group.tsv exists. If both exist, cluster_info.tsv is used: has_cluster_info = False if ( self . path / \"cluster_info.tsv\" ) . exists (): cluster_info_file = self . path / \"cluster_info.tsv\" has_cluster_info = True elif ( self . path / \"cluster_group.tsv\" ) . exists (): cluster_info_file = self . path / \"cluster_group.tsv\" else : raise RuntimeError ( \"Can't find cluster_info.tsv or cluster_group.tsv in {} ;\" . format ( self . path ) ) cluster_info = pd . read_csv ( cluster_info_file , sep = \" \\t \" , index_col = \"cluster_id\" ) # In my processed data with KiloSort 3.0, the column is named KSLabel if \"group\" in cluster_info . columns : cluster_id_good = cluster_info [ cluster_info . group == \"good\" ] . index . values elif \"KSLabel\" in cluster_info . columns : cluster_id_good = cluster_info [ cluster_info . KSLabel == \"good\" ] . index . values else : raise RuntimeError ( \"Can't find column group or KSLabel in {} ;\" . format ( cluster_info_file ) ) spike_times = np . load ( self . path / \"spike_times.npy\" ) spike_clusters = np . load ( self . path / \"spike_clusters.npy\" ) spikes = {} for n in cluster_id_good : spikes [ n ] = nap . Ts ( t = spike_times [ spike_clusters == n ] / self . sample_rate , time_support = time_support , ) self . spikes = nap . TsGroup ( spikes , time_support = time_support ) # Adding the position of the electrodes in case self . channel_positions = np . load ( self . path / \"channel_positions.npy\" ) # Adding shank group info from cluster_info if present if has_cluster_info : group = cluster_info . loc [ cluster_id_good , \"sh\" ] self . spikes . set_info ( group = group ) else : template = np . load ( self . path / \"templates.npy\" ) template = template [ cluster_id_good ] ch = np . power ( template , 2 ) . max ( 1 ) . argmax ( 1 ) group = pd . Series ( index = cluster_id_good , data = self . ch_to_sh [ ch ] . values ) self . spikes . set_info ( group = group ) names = pd . Series ( index = group . index , data = [ self . ephys_information [ group . loc [ i ]][ \"name\" ] for i in group . index ], ) if ~ np . all ( names . values == \"\" ): self . spikes . set_info ( name = names ) locations = pd . Series ( index = group . index , data = [ self . ephys_information [ group . loc [ i ]][ \"location\" ] for i in group . index ], ) if ~ np . all ( locations . values == \"\" ): self . spikes . set_info ( location = locations ) return","title":"load_phy_spikes()"},{"location":"io.phy/#pynapple.io.phy.Phy.save_data","text":"Save the data to NWB format. Source code in pynapple/io/phy.py def save_data ( self ): \"\"\"Save the data to NWB format.\"\"\" io = NWBHDF5IO ( self . nwb_file , \"r+\" ) nwbfile = io . read () electrode_groups = {} for g in self . channel_map : device = nwbfile . create_device ( name = self . ephys_information [ g ][ \"device\" ][ \"name\" ] + \"-\" + str ( g ), description = self . ephys_information [ g ][ \"device\" ][ \"description\" ], manufacturer = self . ephys_information [ g ][ \"device\" ][ \"manufacturer\" ], ) if ( len ( self . ephys_information [ g ][ \"position\" ]) and type ( self . ephys_information [ g ][ \"position\" ]) is str ): self . ephys_information [ g ][ \"position\" ] = re . split ( \";|,| \" , self . ephys_information [ g ][ \"position\" ] ) elif self . ephys_information [ g ][ \"position\" ] == \"\" : self . ephys_information [ g ][ \"position\" ] = None electrode_groups [ g ] = nwbfile . create_electrode_group ( name = \"group\" + str ( g ) + \"_\" + self . ephys_information [ g ][ \"name\" ], description = self . ephys_information [ g ][ \"description\" ], position = self . ephys_information [ g ][ \"position\" ], location = self . ephys_information [ g ][ \"location\" ], device = device , ) for idx in self . channel_map [ g ]: nwbfile . add_electrode ( id = idx , x = 0.0 , y = 0.0 , z = 0.0 , imp = 0.0 , location = self . ephys_information [ g ][ \"location\" ], filtering = \"none\" , group = electrode_groups [ g ], ) # Adding units nwbfile . add_unit_column ( \"location\" , \"the anatomical location of this unit\" ) nwbfile . add_unit_column ( \"group\" , \"the group of the unit\" ) for u in self . spikes . keys (): nwbfile . add_unit ( id = u , spike_times = self . spikes [ u ] . as_units ( \"s\" ) . index . values , electrode_group = electrode_groups [ self . spikes . get_info ( \"group\" ) . loc [ u ]], location = self . ephys_information [ self . spikes . get_info ( \"group\" ) . loc [ u ]][ \"location\" ], group = self . spikes . get_info ( \"group\" ) . loc [ u ], ) io . write ( nwbfile ) io . close () return","title":"save_data()"},{"location":"io.phy/#pynapple.io.phy.Phy.load_nwb_spikes","text":"Read the NWB spikes to extract the spike times. Returns: Type Description TYPE Description Source code in pynapple/io/phy.py def load_nwb_spikes ( self ): \"\"\"Read the NWB spikes to extract the spike times. Returns ------- TYPE Description \"\"\" io = NWBHDF5IO ( self . nwb_file , \"r\" ) nwbfile = io . read () if nwbfile . units is None : io . close () return None else : units = nwbfile . units . to_dataframe () spikes = { n : nap . Ts ( t = units . loc [ n , \"spike_times\" ], time_units = \"s\" ) for n in units . index } self . spikes = nap . TsGroup ( spikes , time_support = self . time_support , time_units = \"s\" , group = units [ \"group\" ], ) if ~ np . all ( units [ \"location\" ] == \"\" ): self . spikes . set_info ( location = units [ \"location\" ]) io . close () return True","title":"load_nwb_spikes()"},{"location":"io.phy/#pynapple.io.phy.Phy.load_lfp","text":"Load the LFP. Parameters: Name Type Description Default filename str The filename of the lfp file. It can be useful it multiple dat files are present in the data directory None channel int or list of int The channel(s) to load. If None return a memory map of the dat file to avoid memory error None extension str The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match '.eeg' frequency float Default 1250 Hz for the eeg file 1250.0 precision str The precision of the binary file 'int16' bytes_size int Bytes size of the lfp file 2 Exceptions: Type Description RuntimeError If can't find the lfp/eeg/dat file Returns: Type Description Tsd or TsdFrame The lfp in a time series format Source code in pynapple/io/phy.py def load_lfp ( self , filename = None , channel = None , extension = \".eeg\" , frequency = 1250.0 , precision = \"int16\" , bytes_size = 2 , ): \"\"\" Load the LFP. Parameters ---------- filename : str, optional The filename of the lfp file. It can be useful it multiple dat files are present in the data directory channel : int or list of int, optional The channel(s) to load. If None return a memory map of the dat file to avoid memory error extension : str, optional The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match frequency : float, optional Default 1250 Hz for the eeg file precision : str, optional The precision of the binary file bytes_size : int, optional Bytes size of the lfp file Raises ------ RuntimeError If can't find the lfp/eeg/dat file Returns ------- Tsd or TsdFrame The lfp in a time series format \"\"\" if filename is not None : filepath = self . path / filename else : try : filepath = list ( self . path . glob ( f \"* { extension } \" ))[ 0 ] except IndexError : raise RuntimeError ( f \"Path { self . path } contains no { extension } files;\" ) # is it possible that this is a leftover from neurosuite data? # This is not implemented for this class. self . load_neurosuite_xml ( self . path ) n_channels = int ( self . nChannels ) f = open ( filepath , \"rb\" ) startoffile = f . seek ( 0 , 0 ) endoffile = f . seek ( 0 , 2 ) bytes_size = 2 n_samples = int (( endoffile - startoffile ) / n_channels / bytes_size ) duration = n_samples / frequency f . close () fp = np . memmap ( filepath , np . int16 , \"r\" , shape = ( n_samples , n_channels )) timestep = np . arange ( 0 , n_samples ) / frequency time_support = nap . IntervalSet ( start = 0 , end = duration , time_units = \"s\" ) if channel is None : return nap . TsdFrame ( t = timestep , d = fp , time_units = \"s\" , time_support = time_support ) elif type ( channel ) is int : return nap . Tsd ( t = timestep , d = fp [:, channel ], time_units = \"s\" , time_support = time_support ) elif type ( channel ) is list : return nap . TsdFrame ( t = timestep , d = fp [:, channel ], time_units = \"s\" , time_support = time_support , columns = channel , )","title":"load_lfp()"},{"location":"io.suite2p/","text":"Loader for Suite2P https://github.com/MouseLand/suite2p Suite2P ( BaseLoader ) Loader for data processed with Suite2P. Pynapple will try to look for data in this order : pynapplenwb/session_name.nwb suite2p/plane / .npy Attributes: Name Type Description F TsdFrame Fluorescence traces (timepoints x ROIs) for all planes Fneu TsdFrame Neuropil fluorescence traces (timepoints x ROIs) for all planes spks TsdFrame Deconvolved traces (timepoints x ROIS) for all planes plane_info pandas.DataFrame Contains plane identity of each cell stats dict dictionnay of statistics from stat.npy for each planes only for the neurons that were classified as cells (Can be smaller when loading from the NWB file) ops dict Parameters from Suite2p. (Can be smaller when loading from the NWB file) iscell numpy.ndarray Cell classification Source code in pynapple/io/suite2p.py class Suite2P ( BaseLoader ): \"\"\"Loader for data processed with Suite2P. Pynapple will try to look for data in this order : 1. pynapplenwb/session_name.nwb 2. suite2p/plane*/*.npy Attributes ---------- F : TsdFrame Fluorescence traces (timepoints x ROIs) for all planes Fneu : TsdFrame Neuropil fluorescence traces (timepoints x ROIs) for all planes spks : TsdFrame Deconvolved traces (timepoints x ROIS) for all planes plane_info : pandas.DataFrame Contains plane identity of each cell stats : dict dictionnay of statistics from stat.npy for each planes only for the neurons that were classified as cells (Can be smaller when loading from the NWB file) ops : dict Parameters from Suite2p. (Can be smaller when loading from the NWB file) iscell : numpy.ndarray Cell classification \"\"\" def __init__ ( self , path ): \"\"\" Parameters ---------- path : str The path of the session \"\"\" self . basename = os . path . basename ( path ) super () . __init__ ( path ) # Need to check if nwb file exists and if data are there loading_my_data = True if self . path is not None : nwb_path = os . path . join ( self . path , \"pynapplenwb\" ) if os . path . exists ( nwb_path ): files = os . listdir ( nwb_path ) if len ([ f for f in files if f . endswith ( \".nwb\" )]): success = self . load_suite2p_nwb ( path ) if success : loading_my_data = False # Bypass if data have already been transfered to nwb if loading_my_data : app = App () window = OphysGUI ( app , path = path ) app . mainloop () try : app . update () except Exception : pass if window . status : self . ophys_information = window . ophys_information self . load_suite2p ( path ) self . save_suite2p_nwb ( path ) def load_suite2p ( self , path ): \"\"\" Looking for suite2/plane* Parameters ---------- path : str The path of the session \"\"\" self . path_suite2p = os . path . join ( path , \"suite2p\" ) self . sampling_rate = float ( self . ophys_information [ \"ImagingPlane\" ][ \"imaging_rate\" ] ) data = { \"F\" : [], \"Fneu\" : [], \"spks\" : [], } plane_info = [] self . stats = {} self . pops = {} self . iscells = {} self . planes = [] if os . path . exists ( self . path_suite2p ): planes = glob . glob ( os . path . join ( self . path_suite2p , \"plane*\" )) if len ( planes ): # count = 0 for plane_dir in planes : n = int ( os . path . basename ( plane_dir )[ - 1 ]) self . planes . append ( n ) # Loading iscell.npy try : iscell = np . load ( os . path . join ( plane_dir , \"iscell.npy\" ), allow_pickle = True ) idx = np . where ( iscell . astype ( \"int\" )[:, 0 ])[ 0 ] plane_info . append ( np . ones ( len ( idx ), dtype = \"int\" ) * n ) except OSError as e : print ( e ) sys . exit () # Loading F.npy, Fneu.py and spks.npy for obj in [ \"F.npy\" , \"Fneu.npy\" , \"spks.npy\" ]: try : name = obj . split ( \".\" )[ 0 ] tmp = np . load ( os . path . join ( plane_dir , obj ), allow_pickle = True ) data [ name ] . append ( tmp [ idx ]) except OSError as e : print ( e ) sys . exit () # Loading stat.npy and ops.npy try : stat = np . load ( os . path . join ( plane_dir , \"stat.npy\" ), allow_pickle = True ) ops = np . load ( os . path . join ( plane_dir , \"ops.npy\" ), allow_pickle = True ) . item () except OSError as e : print ( e ) sys . exit () # Saving stat, ops and iscell self . stats [ n ] = stat self . pops [ n ] = ops self . iscells [ n ] = iscell # count += len(idx) else : warnings . warn ( \"Couldn't find planes in %s \" % self . path_suite2p , stacklevel = 2 ) sys . exit () else : warnings . warn ( \"No suite2p folder in %s \" % path , stacklevel = 2 ) sys . exit () # Calcium transients data [ \"F\" ] = np . transpose ( np . vstack ( data [ \"F\" ])) data [ \"Fneu\" ] = np . transpose ( np . vstack ( data [ \"Fneu\" ])) data [ \"spks\" ] = np . transpose ( np . vstack ( data [ \"spks\" ])) time_index = np . arange ( 0 , len ( data [ \"F\" ])) / self . sampling_rate self . F = nap . TsdFrame ( t = time_index , d = data [ \"F\" ]) self . Fneu = nap . TsdFrame ( t = time_index , d = data [ \"Fneu\" ]) self . spks = nap . TsdFrame ( t = time_index , d = data [ \"spks\" ]) self . ops = self . pops [ 0 ] self . iscell = np . vstack ([ self . iscells [ k ] for k in self . iscells . keys ()]) # Metadata self . plane_info = pd . DataFrame . from_dict ({ \"plane\" : np . hstack ( plane_info )}) return def save_suite2p_nwb ( self , path ): \"\"\" Save the data to NWB. To ensure continuity, this function is based on : https://github.com/MouseLand/suite2p/blob/main/suite2p/io/nwb.py. Parameters ---------- path : str The path of the session \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) multiplane = True if len ( self . planes ) > 1 else False ops = self . pops [ list ( self . pops . keys ())[ 0 ]] io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () device = nwbfile . create_device ( name = self . ophys_information [ \"device\" ][ \"name\" ], description = self . ophys_information [ \"device\" ][ \"description\" ], manufacturer = self . ophys_information [ \"device\" ][ \"manufacturer\" ], ) imaging_plane = nwbfile . create_imaging_plane ( name = self . ophys_information [ \"ImagingPlane\" ][ \"name\" ], optical_channel = OpticalChannel ( name = self . ophys_information [ \"OpticalChannel\" ][ \"name\" ], description = self . ophys_information [ \"OpticalChannel\" ][ \"description\" ], emission_lambda = float ( self . ophys_information [ \"OpticalChannel\" ][ \"emission_lambda\" ] ), ), imaging_rate = self . sampling_rate , description = self . ophys_information [ \"ImagingPlane\" ][ \"description\" ], device = device , excitation_lambda = float ( self . ophys_information [ \"ImagingPlane\" ][ \"excitation_lambda\" ] ), indicator = self . ophys_information [ \"ImagingPlane\" ][ \"indicator\" ], location = self . ophys_information [ \"ImagingPlane\" ][ \"location\" ], grid_spacing = ([ 2.0 , 2.0 , 30.0 ] if multiplane else [ 2.0 , 2.0 ]), grid_spacing_unit = \"microns\" , ) # link to external data image_series = TwoPhotonSeries ( name = \"TwoPhotonSeries\" , dimension = [ ops [ \"Ly\" ], ops [ \"Lx\" ]], external_file = ( ops [ \"filelist\" ] if \"filelist\" in ops else [ \"\" ]), imaging_plane = imaging_plane , starting_frame = [ 0 ], format = \"external\" , starting_time = 0.0 , rate = ops [ \"fs\" ] * ops [ \"nplanes\" ], ) nwbfile . add_acquisition ( image_series ) # processing img_seg = ImageSegmentation () ps = img_seg . create_plane_segmentation ( name = self . ophys_information [ \"PlaneSegmentation\" ][ \"name\" ], description = self . ophys_information [ \"PlaneSegmentation\" ][ \"description\" ], imaging_plane = imaging_plane , # reference_images=image_series, ) ophys_module = nwbfile . create_processing_module ( name = \"ophys\" , description = \"optical physiology processed data\" ) ophys_module . add ( img_seg ) file_strs = [ \"F.npy\" , \"Fneu.npy\" , \"spks.npy\" ] traces = [] ncells = np . zeros ( len ( self . pops ), dtype = np . int_ ) Nfr = np . array ([ self . pops [ k ][ \"nframes\" ] for k in self . pops . keys ()]) . max () for iplane , ops in self . pops . items (): if iplane == 0 : iscell = self . iscells [ iplane ] for fstr in file_strs : traces . append ( np . load ( os . path . join ( ops [ \"save_path\" ], fstr ))) PlaneCellsIdx = iplane * np . ones ( len ( iscell )) else : iscell = np . append ( iscell , self . iscells [ iplane ], axis = 0 , ) for i , fstr in enumerate ( file_strs ): trace = np . load ( os . path . join ( ops [ \"save_path\" ], fstr )) if trace . shape [ 1 ] < Nfr : fcat = np . zeros ( ( trace . shape [ 0 ], Nfr - trace . shape [ 1 ]), \"float32\" ) trace = np . concatenate (( trace , fcat ), axis = 1 ) traces [ i ] = np . append ( traces [ i ], trace , axis = 0 ) PlaneCellsIdx = np . append ( PlaneCellsIdx , iplane * np . ones ( len ( iscell ) - len ( PlaneCellsIdx )) ) stat = self . stats [ iplane ] ncells [ iplane ] = len ( stat ) for n in range ( ncells [ iplane ]): if multiplane : pixel_mask = np . array ( [ stat [ n ][ \"ypix\" ], stat [ n ][ \"xpix\" ], iplane * np . ones ( stat [ n ][ \"npix\" ]), stat [ n ][ \"lam\" ], ] ) ps . add_roi ( voxel_mask = pixel_mask . T ) else : pixel_mask = np . array ( [ stat [ n ][ \"ypix\" ], stat [ n ][ \"xpix\" ], stat [ n ][ \"lam\" ]] ) ps . add_roi ( pixel_mask = pixel_mask . T ) ps . add_column ( \"iscell\" , \"two columns - iscell & probcell\" , iscell ) rt_region = [] for iplane , ops in self . pops . items (): if iplane == 0 : rt_region . append ( ps . create_roi_table_region ( region = list ( np . arange ( 0 , ncells [ iplane ]), ), description = f \"ROIs for plane { int ( iplane ) } \" , ) ) else : rt_region . append ( ps . create_roi_table_region ( region = list ( np . arange ( np . sum ( ncells [: iplane ]), ncells [ iplane ] + np . sum ( ncells [: iplane ]), ) ), description = f \"ROIs for plane { int ( iplane ) } \" , ) ) # FLUORESCENCE (all are required) name_strs = [ \"Fluorescence\" , \"Neuropil\" , \"Deconvolved\" ] for i , ( fstr , nstr ) in enumerate ( zip ( file_strs , name_strs )): for iplane , ops in self . pops . items (): roi_resp_series = RoiResponseSeries ( name = f \"plane { int ( iplane ) } \" , data = traces [ i ][ PlaneCellsIdx == iplane ], rois = rt_region [ iplane ], unit = \"lumens\" , rate = ops [ \"fs\" ], ) if iplane == 0 : fl = Fluorescence ( roi_response_series = roi_resp_series , name = nstr ) else : fl . add_roi_response_series ( roi_response_series = roi_resp_series ) ophys_module . add ( fl ) io . write ( nwbfile ) io . close () return def load_suite2p_nwb ( self , path ): \"\"\" Load suite2p data from NWB Parameters ---------- path : str Path to the session \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r\" ) nwbfile = io . read () if \"ophys\" in nwbfile . processing . keys (): ophys = nwbfile . processing [ \"ophys\" ] ################################################################# # STATS, OPS and ISCELL ################################################################# dims = nwbfile . acquisition [ \"TwoPhotonSeries\" ] . dimension [:] self . ops = { \"Ly\" : dims [ 0 ], \"Lx\" : dims [ 1 ]} self . rate = nwbfile . acquisition [ \"TwoPhotonSeries\" ] . imaging_plane . imaging_rate self . stats = { 0 : {}} self . iscell = ophys [ \"ImageSegmentation\" ][ \"PlaneSegmentation\" ][ \"iscell\" ] . data [:] info = pd . DataFrame ( data = self . iscell [:, 0 ] . astype ( \"int\" ), columns = [ \"iscell\" ] ) ################################################################# # ROIS ################################################################# try : rois = nwbfile . processing [ \"ophys\" ][ \"ImageSegmentation\" ][ \"PlaneSegmentation\" ][ \"pixel_mask\" ] multiplane = False except Exception : rois = nwbfile . processing [ \"ophys\" ][ \"ImageSegmentation\" ][ \"PlaneSegmentation\" ][ \"voxel_mask\" ] multiplane = True idx = np . where ( self . iscell [:, 0 ])[ 0 ] info [ \"plane\" ] = 0 for n in range ( len ( rois )): roi = pd . DataFrame ( rois [ n ]) if \"z\" in roi . columns : pl = roi [ \"z\" ][ 0 ] else : pl = 0 info . loc [ n , \"plane\" ] = pl if pl not in self . stats . keys (): self . stats [ pl ] = {} if n in idx : self . stats [ pl ][ n ] = { \"xpix\" : roi [ \"y\" ] . values , \"ypix\" : roi [ \"x\" ] . values , \"lam\" : roi [ \"weight\" ] . values , } ################################################################# # Time Series ################################################################# fields = np . intersect1d ( [ \"Fluorescence\" , \"Neuropil\" , \"Deconvolved\" ], list ( ophys . fields [ \"data_interfaces\" ] . keys ()), ) if len ( fields ) == 0 : print ( \"No \" + \" or \" . join ([ \"Fluorescence\" , \"Neuropil\" , \"Deconvolved\" ]), \"found in nwb {} \" . format ( self . nwbfilepath ), ) return False keys = ophys [ fields [ 0 ]] . roi_response_series . keys () planes = [ int ( k [ - 1 ]) for k in keys if \"plane\" in k ] data = {} if multiplane : keys = ophys [ fields [ 0 ]] . roi_response_series . keys () planes = [ int ( k [ - 1 ]) for k in keys if \"plane\" in k ] else : planes = [ 0 ] for k , name in zip ( [ \"F\" , \"Fneu\" , \"spks\" ], [ \"Fluorescence\" , \"Neuropil\" , \"Deconvolved\" ] ): tmp = [] timestamps = [] for i , n in enumerate ( planes ): if multiplane : pl = \"plane {} \" . format ( n ) else : pl = name # This doesn't make sense tokeep = info [ \"iscell\" ][ info [ \"plane\" ] == n ] . values == 1 d = np . transpose ( ophys [ name ][ pl ] . data [:][ tokeep ]) if ophys [ name ][ pl ] . timestamps is not None : t = ophys [ name ][ pl ] . timestamps [:] else : t = ( np . arange ( 0 , len ( d )) / self . rate ) + ophys [ name ][ pl ] . starting_time tmp . append ( d ) timestamps . append ( t ) data [ k ] = nap . TsdFrame ( t = timestamps [ 0 ], d = np . hstack ( tmp )) if \"F\" in data . keys (): self . F = data [ \"F\" ] if \"Fneu\" in data . keys (): self . Fneu = data [ \"Fneu\" ] if \"spks\" in data . keys (): self . spks = data [ \"spks\" ] self . plane_info = pd . DataFrame ( data = info [ \"plane\" ][ info [ \"iscell\" ] == 1 ] . values , columns = [ \"plane\" ] ) io . close () return True else : io . close () return False __init__ ( self , path ) special Parameters: Name Type Description Default path str The path of the session required Source code in pynapple/io/suite2p.py def __init__ ( self , path ): \"\"\" Parameters ---------- path : str The path of the session \"\"\" self . basename = os . path . basename ( path ) super () . __init__ ( path ) # Need to check if nwb file exists and if data are there loading_my_data = True if self . path is not None : nwb_path = os . path . join ( self . path , \"pynapplenwb\" ) if os . path . exists ( nwb_path ): files = os . listdir ( nwb_path ) if len ([ f for f in files if f . endswith ( \".nwb\" )]): success = self . load_suite2p_nwb ( path ) if success : loading_my_data = False # Bypass if data have already been transfered to nwb if loading_my_data : app = App () window = OphysGUI ( app , path = path ) app . mainloop () try : app . update () except Exception : pass if window . status : self . ophys_information = window . ophys_information self . load_suite2p ( path ) self . save_suite2p_nwb ( path ) load_suite2p ( self , path ) Looking for suite2/plane* Parameters: Name Type Description Default path str The path of the session required Source code in pynapple/io/suite2p.py def load_suite2p ( self , path ): \"\"\" Looking for suite2/plane* Parameters ---------- path : str The path of the session \"\"\" self . path_suite2p = os . path . join ( path , \"suite2p\" ) self . sampling_rate = float ( self . ophys_information [ \"ImagingPlane\" ][ \"imaging_rate\" ] ) data = { \"F\" : [], \"Fneu\" : [], \"spks\" : [], } plane_info = [] self . stats = {} self . pops = {} self . iscells = {} self . planes = [] if os . path . exists ( self . path_suite2p ): planes = glob . glob ( os . path . join ( self . path_suite2p , \"plane*\" )) if len ( planes ): # count = 0 for plane_dir in planes : n = int ( os . path . basename ( plane_dir )[ - 1 ]) self . planes . append ( n ) # Loading iscell.npy try : iscell = np . load ( os . path . join ( plane_dir , \"iscell.npy\" ), allow_pickle = True ) idx = np . where ( iscell . astype ( \"int\" )[:, 0 ])[ 0 ] plane_info . append ( np . ones ( len ( idx ), dtype = \"int\" ) * n ) except OSError as e : print ( e ) sys . exit () # Loading F.npy, Fneu.py and spks.npy for obj in [ \"F.npy\" , \"Fneu.npy\" , \"spks.npy\" ]: try : name = obj . split ( \".\" )[ 0 ] tmp = np . load ( os . path . join ( plane_dir , obj ), allow_pickle = True ) data [ name ] . append ( tmp [ idx ]) except OSError as e : print ( e ) sys . exit () # Loading stat.npy and ops.npy try : stat = np . load ( os . path . join ( plane_dir , \"stat.npy\" ), allow_pickle = True ) ops = np . load ( os . path . join ( plane_dir , \"ops.npy\" ), allow_pickle = True ) . item () except OSError as e : print ( e ) sys . exit () # Saving stat, ops and iscell self . stats [ n ] = stat self . pops [ n ] = ops self . iscells [ n ] = iscell # count += len(idx) else : warnings . warn ( \"Couldn't find planes in %s \" % self . path_suite2p , stacklevel = 2 ) sys . exit () else : warnings . warn ( \"No suite2p folder in %s \" % path , stacklevel = 2 ) sys . exit () # Calcium transients data [ \"F\" ] = np . transpose ( np . vstack ( data [ \"F\" ])) data [ \"Fneu\" ] = np . transpose ( np . vstack ( data [ \"Fneu\" ])) data [ \"spks\" ] = np . transpose ( np . vstack ( data [ \"spks\" ])) time_index = np . arange ( 0 , len ( data [ \"F\" ])) / self . sampling_rate self . F = nap . TsdFrame ( t = time_index , d = data [ \"F\" ]) self . Fneu = nap . TsdFrame ( t = time_index , d = data [ \"Fneu\" ]) self . spks = nap . TsdFrame ( t = time_index , d = data [ \"spks\" ]) self . ops = self . pops [ 0 ] self . iscell = np . vstack ([ self . iscells [ k ] for k in self . iscells . keys ()]) # Metadata self . plane_info = pd . DataFrame . from_dict ({ \"plane\" : np . hstack ( plane_info )}) return save_suite2p_nwb ( self , path ) Save the data to NWB. To ensure continuity, this function is based on : https://github.com/MouseLand/suite2p/blob/main/suite2p/io/nwb.py. Parameters: Name Type Description Default path str The path of the session required Source code in pynapple/io/suite2p.py def save_suite2p_nwb ( self , path ): \"\"\" Save the data to NWB. To ensure continuity, this function is based on : https://github.com/MouseLand/suite2p/blob/main/suite2p/io/nwb.py. Parameters ---------- path : str The path of the session \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) multiplane = True if len ( self . planes ) > 1 else False ops = self . pops [ list ( self . pops . keys ())[ 0 ]] io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () device = nwbfile . create_device ( name = self . ophys_information [ \"device\" ][ \"name\" ], description = self . ophys_information [ \"device\" ][ \"description\" ], manufacturer = self . ophys_information [ \"device\" ][ \"manufacturer\" ], ) imaging_plane = nwbfile . create_imaging_plane ( name = self . ophys_information [ \"ImagingPlane\" ][ \"name\" ], optical_channel = OpticalChannel ( name = self . ophys_information [ \"OpticalChannel\" ][ \"name\" ], description = self . ophys_information [ \"OpticalChannel\" ][ \"description\" ], emission_lambda = float ( self . ophys_information [ \"OpticalChannel\" ][ \"emission_lambda\" ] ), ), imaging_rate = self . sampling_rate , description = self . ophys_information [ \"ImagingPlane\" ][ \"description\" ], device = device , excitation_lambda = float ( self . ophys_information [ \"ImagingPlane\" ][ \"excitation_lambda\" ] ), indicator = self . ophys_information [ \"ImagingPlane\" ][ \"indicator\" ], location = self . ophys_information [ \"ImagingPlane\" ][ \"location\" ], grid_spacing = ([ 2.0 , 2.0 , 30.0 ] if multiplane else [ 2.0 , 2.0 ]), grid_spacing_unit = \"microns\" , ) # link to external data image_series = TwoPhotonSeries ( name = \"TwoPhotonSeries\" , dimension = [ ops [ \"Ly\" ], ops [ \"Lx\" ]], external_file = ( ops [ \"filelist\" ] if \"filelist\" in ops else [ \"\" ]), imaging_plane = imaging_plane , starting_frame = [ 0 ], format = \"external\" , starting_time = 0.0 , rate = ops [ \"fs\" ] * ops [ \"nplanes\" ], ) nwbfile . add_acquisition ( image_series ) # processing img_seg = ImageSegmentation () ps = img_seg . create_plane_segmentation ( name = self . ophys_information [ \"PlaneSegmentation\" ][ \"name\" ], description = self . ophys_information [ \"PlaneSegmentation\" ][ \"description\" ], imaging_plane = imaging_plane , # reference_images=image_series, ) ophys_module = nwbfile . create_processing_module ( name = \"ophys\" , description = \"optical physiology processed data\" ) ophys_module . add ( img_seg ) file_strs = [ \"F.npy\" , \"Fneu.npy\" , \"spks.npy\" ] traces = [] ncells = np . zeros ( len ( self . pops ), dtype = np . int_ ) Nfr = np . array ([ self . pops [ k ][ \"nframes\" ] for k in self . pops . keys ()]) . max () for iplane , ops in self . pops . items (): if iplane == 0 : iscell = self . iscells [ iplane ] for fstr in file_strs : traces . append ( np . load ( os . path . join ( ops [ \"save_path\" ], fstr ))) PlaneCellsIdx = iplane * np . ones ( len ( iscell )) else : iscell = np . append ( iscell , self . iscells [ iplane ], axis = 0 , ) for i , fstr in enumerate ( file_strs ): trace = np . load ( os . path . join ( ops [ \"save_path\" ], fstr )) if trace . shape [ 1 ] < Nfr : fcat = np . zeros ( ( trace . shape [ 0 ], Nfr - trace . shape [ 1 ]), \"float32\" ) trace = np . concatenate (( trace , fcat ), axis = 1 ) traces [ i ] = np . append ( traces [ i ], trace , axis = 0 ) PlaneCellsIdx = np . append ( PlaneCellsIdx , iplane * np . ones ( len ( iscell ) - len ( PlaneCellsIdx )) ) stat = self . stats [ iplane ] ncells [ iplane ] = len ( stat ) for n in range ( ncells [ iplane ]): if multiplane : pixel_mask = np . array ( [ stat [ n ][ \"ypix\" ], stat [ n ][ \"xpix\" ], iplane * np . ones ( stat [ n ][ \"npix\" ]), stat [ n ][ \"lam\" ], ] ) ps . add_roi ( voxel_mask = pixel_mask . T ) else : pixel_mask = np . array ( [ stat [ n ][ \"ypix\" ], stat [ n ][ \"xpix\" ], stat [ n ][ \"lam\" ]] ) ps . add_roi ( pixel_mask = pixel_mask . T ) ps . add_column ( \"iscell\" , \"two columns - iscell & probcell\" , iscell ) rt_region = [] for iplane , ops in self . pops . items (): if iplane == 0 : rt_region . append ( ps . create_roi_table_region ( region = list ( np . arange ( 0 , ncells [ iplane ]), ), description = f \"ROIs for plane { int ( iplane ) } \" , ) ) else : rt_region . append ( ps . create_roi_table_region ( region = list ( np . arange ( np . sum ( ncells [: iplane ]), ncells [ iplane ] + np . sum ( ncells [: iplane ]), ) ), description = f \"ROIs for plane { int ( iplane ) } \" , ) ) # FLUORESCENCE (all are required) name_strs = [ \"Fluorescence\" , \"Neuropil\" , \"Deconvolved\" ] for i , ( fstr , nstr ) in enumerate ( zip ( file_strs , name_strs )): for iplane , ops in self . pops . items (): roi_resp_series = RoiResponseSeries ( name = f \"plane { int ( iplane ) } \" , data = traces [ i ][ PlaneCellsIdx == iplane ], rois = rt_region [ iplane ], unit = \"lumens\" , rate = ops [ \"fs\" ], ) if iplane == 0 : fl = Fluorescence ( roi_response_series = roi_resp_series , name = nstr ) else : fl . add_roi_response_series ( roi_response_series = roi_resp_series ) ophys_module . add ( fl ) io . write ( nwbfile ) io . close () return load_suite2p_nwb ( self , path ) Load suite2p data from NWB Parameters: Name Type Description Default path str Path to the session required Source code in pynapple/io/suite2p.py def load_suite2p_nwb ( self , path ): \"\"\" Load suite2p data from NWB Parameters ---------- path : str Path to the session \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r\" ) nwbfile = io . read () if \"ophys\" in nwbfile . processing . keys (): ophys = nwbfile . processing [ \"ophys\" ] ################################################################# # STATS, OPS and ISCELL ################################################################# dims = nwbfile . acquisition [ \"TwoPhotonSeries\" ] . dimension [:] self . ops = { \"Ly\" : dims [ 0 ], \"Lx\" : dims [ 1 ]} self . rate = nwbfile . acquisition [ \"TwoPhotonSeries\" ] . imaging_plane . imaging_rate self . stats = { 0 : {}} self . iscell = ophys [ \"ImageSegmentation\" ][ \"PlaneSegmentation\" ][ \"iscell\" ] . data [:] info = pd . DataFrame ( data = self . iscell [:, 0 ] . astype ( \"int\" ), columns = [ \"iscell\" ] ) ################################################################# # ROIS ################################################################# try : rois = nwbfile . processing [ \"ophys\" ][ \"ImageSegmentation\" ][ \"PlaneSegmentation\" ][ \"pixel_mask\" ] multiplane = False except Exception : rois = nwbfile . processing [ \"ophys\" ][ \"ImageSegmentation\" ][ \"PlaneSegmentation\" ][ \"voxel_mask\" ] multiplane = True idx = np . where ( self . iscell [:, 0 ])[ 0 ] info [ \"plane\" ] = 0 for n in range ( len ( rois )): roi = pd . DataFrame ( rois [ n ]) if \"z\" in roi . columns : pl = roi [ \"z\" ][ 0 ] else : pl = 0 info . loc [ n , \"plane\" ] = pl if pl not in self . stats . keys (): self . stats [ pl ] = {} if n in idx : self . stats [ pl ][ n ] = { \"xpix\" : roi [ \"y\" ] . values , \"ypix\" : roi [ \"x\" ] . values , \"lam\" : roi [ \"weight\" ] . values , } ################################################################# # Time Series ################################################################# fields = np . intersect1d ( [ \"Fluorescence\" , \"Neuropil\" , \"Deconvolved\" ], list ( ophys . fields [ \"data_interfaces\" ] . keys ()), ) if len ( fields ) == 0 : print ( \"No \" + \" or \" . join ([ \"Fluorescence\" , \"Neuropil\" , \"Deconvolved\" ]), \"found in nwb {} \" . format ( self . nwbfilepath ), ) return False keys = ophys [ fields [ 0 ]] . roi_response_series . keys () planes = [ int ( k [ - 1 ]) for k in keys if \"plane\" in k ] data = {} if multiplane : keys = ophys [ fields [ 0 ]] . roi_response_series . keys () planes = [ int ( k [ - 1 ]) for k in keys if \"plane\" in k ] else : planes = [ 0 ] for k , name in zip ( [ \"F\" , \"Fneu\" , \"spks\" ], [ \"Fluorescence\" , \"Neuropil\" , \"Deconvolved\" ] ): tmp = [] timestamps = [] for i , n in enumerate ( planes ): if multiplane : pl = \"plane {} \" . format ( n ) else : pl = name # This doesn't make sense tokeep = info [ \"iscell\" ][ info [ \"plane\" ] == n ] . values == 1 d = np . transpose ( ophys [ name ][ pl ] . data [:][ tokeep ]) if ophys [ name ][ pl ] . timestamps is not None : t = ophys [ name ][ pl ] . timestamps [:] else : t = ( np . arange ( 0 , len ( d )) / self . rate ) + ophys [ name ][ pl ] . starting_time tmp . append ( d ) timestamps . append ( t ) data [ k ] = nap . TsdFrame ( t = timestamps [ 0 ], d = np . hstack ( tmp )) if \"F\" in data . keys (): self . F = data [ \"F\" ] if \"Fneu\" in data . keys (): self . Fneu = data [ \"Fneu\" ] if \"spks\" in data . keys (): self . spks = data [ \"spks\" ] self . plane_info = pd . DataFrame ( data = info [ \"plane\" ][ info [ \"iscell\" ] == 1 ] . values , columns = [ \"plane\" ] ) io . close () return True else : io . close () return False","title":"Suite2p"},{"location":"io.suite2p/#pynapple.io.suite2p.Suite2P","text":"Loader for data processed with Suite2P. Pynapple will try to look for data in this order : pynapplenwb/session_name.nwb suite2p/plane / .npy Attributes: Name Type Description F TsdFrame Fluorescence traces (timepoints x ROIs) for all planes Fneu TsdFrame Neuropil fluorescence traces (timepoints x ROIs) for all planes spks TsdFrame Deconvolved traces (timepoints x ROIS) for all planes plane_info pandas.DataFrame Contains plane identity of each cell stats dict dictionnay of statistics from stat.npy for each planes only for the neurons that were classified as cells (Can be smaller when loading from the NWB file) ops dict Parameters from Suite2p. (Can be smaller when loading from the NWB file) iscell numpy.ndarray Cell classification Source code in pynapple/io/suite2p.py class Suite2P ( BaseLoader ): \"\"\"Loader for data processed with Suite2P. Pynapple will try to look for data in this order : 1. pynapplenwb/session_name.nwb 2. suite2p/plane*/*.npy Attributes ---------- F : TsdFrame Fluorescence traces (timepoints x ROIs) for all planes Fneu : TsdFrame Neuropil fluorescence traces (timepoints x ROIs) for all planes spks : TsdFrame Deconvolved traces (timepoints x ROIS) for all planes plane_info : pandas.DataFrame Contains plane identity of each cell stats : dict dictionnay of statistics from stat.npy for each planes only for the neurons that were classified as cells (Can be smaller when loading from the NWB file) ops : dict Parameters from Suite2p. (Can be smaller when loading from the NWB file) iscell : numpy.ndarray Cell classification \"\"\" def __init__ ( self , path ): \"\"\" Parameters ---------- path : str The path of the session \"\"\" self . basename = os . path . basename ( path ) super () . __init__ ( path ) # Need to check if nwb file exists and if data are there loading_my_data = True if self . path is not None : nwb_path = os . path . join ( self . path , \"pynapplenwb\" ) if os . path . exists ( nwb_path ): files = os . listdir ( nwb_path ) if len ([ f for f in files if f . endswith ( \".nwb\" )]): success = self . load_suite2p_nwb ( path ) if success : loading_my_data = False # Bypass if data have already been transfered to nwb if loading_my_data : app = App () window = OphysGUI ( app , path = path ) app . mainloop () try : app . update () except Exception : pass if window . status : self . ophys_information = window . ophys_information self . load_suite2p ( path ) self . save_suite2p_nwb ( path ) def load_suite2p ( self , path ): \"\"\" Looking for suite2/plane* Parameters ---------- path : str The path of the session \"\"\" self . path_suite2p = os . path . join ( path , \"suite2p\" ) self . sampling_rate = float ( self . ophys_information [ \"ImagingPlane\" ][ \"imaging_rate\" ] ) data = { \"F\" : [], \"Fneu\" : [], \"spks\" : [], } plane_info = [] self . stats = {} self . pops = {} self . iscells = {} self . planes = [] if os . path . exists ( self . path_suite2p ): planes = glob . glob ( os . path . join ( self . path_suite2p , \"plane*\" )) if len ( planes ): # count = 0 for plane_dir in planes : n = int ( os . path . basename ( plane_dir )[ - 1 ]) self . planes . append ( n ) # Loading iscell.npy try : iscell = np . load ( os . path . join ( plane_dir , \"iscell.npy\" ), allow_pickle = True ) idx = np . where ( iscell . astype ( \"int\" )[:, 0 ])[ 0 ] plane_info . append ( np . ones ( len ( idx ), dtype = \"int\" ) * n ) except OSError as e : print ( e ) sys . exit () # Loading F.npy, Fneu.py and spks.npy for obj in [ \"F.npy\" , \"Fneu.npy\" , \"spks.npy\" ]: try : name = obj . split ( \".\" )[ 0 ] tmp = np . load ( os . path . join ( plane_dir , obj ), allow_pickle = True ) data [ name ] . append ( tmp [ idx ]) except OSError as e : print ( e ) sys . exit () # Loading stat.npy and ops.npy try : stat = np . load ( os . path . join ( plane_dir , \"stat.npy\" ), allow_pickle = True ) ops = np . load ( os . path . join ( plane_dir , \"ops.npy\" ), allow_pickle = True ) . item () except OSError as e : print ( e ) sys . exit () # Saving stat, ops and iscell self . stats [ n ] = stat self . pops [ n ] = ops self . iscells [ n ] = iscell # count += len(idx) else : warnings . warn ( \"Couldn't find planes in %s \" % self . path_suite2p , stacklevel = 2 ) sys . exit () else : warnings . warn ( \"No suite2p folder in %s \" % path , stacklevel = 2 ) sys . exit () # Calcium transients data [ \"F\" ] = np . transpose ( np . vstack ( data [ \"F\" ])) data [ \"Fneu\" ] = np . transpose ( np . vstack ( data [ \"Fneu\" ])) data [ \"spks\" ] = np . transpose ( np . vstack ( data [ \"spks\" ])) time_index = np . arange ( 0 , len ( data [ \"F\" ])) / self . sampling_rate self . F = nap . TsdFrame ( t = time_index , d = data [ \"F\" ]) self . Fneu = nap . TsdFrame ( t = time_index , d = data [ \"Fneu\" ]) self . spks = nap . TsdFrame ( t = time_index , d = data [ \"spks\" ]) self . ops = self . pops [ 0 ] self . iscell = np . vstack ([ self . iscells [ k ] for k in self . iscells . keys ()]) # Metadata self . plane_info = pd . DataFrame . from_dict ({ \"plane\" : np . hstack ( plane_info )}) return def save_suite2p_nwb ( self , path ): \"\"\" Save the data to NWB. To ensure continuity, this function is based on : https://github.com/MouseLand/suite2p/blob/main/suite2p/io/nwb.py. Parameters ---------- path : str The path of the session \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) multiplane = True if len ( self . planes ) > 1 else False ops = self . pops [ list ( self . pops . keys ())[ 0 ]] io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () device = nwbfile . create_device ( name = self . ophys_information [ \"device\" ][ \"name\" ], description = self . ophys_information [ \"device\" ][ \"description\" ], manufacturer = self . ophys_information [ \"device\" ][ \"manufacturer\" ], ) imaging_plane = nwbfile . create_imaging_plane ( name = self . ophys_information [ \"ImagingPlane\" ][ \"name\" ], optical_channel = OpticalChannel ( name = self . ophys_information [ \"OpticalChannel\" ][ \"name\" ], description = self . ophys_information [ \"OpticalChannel\" ][ \"description\" ], emission_lambda = float ( self . ophys_information [ \"OpticalChannel\" ][ \"emission_lambda\" ] ), ), imaging_rate = self . sampling_rate , description = self . ophys_information [ \"ImagingPlane\" ][ \"description\" ], device = device , excitation_lambda = float ( self . ophys_information [ \"ImagingPlane\" ][ \"excitation_lambda\" ] ), indicator = self . ophys_information [ \"ImagingPlane\" ][ \"indicator\" ], location = self . ophys_information [ \"ImagingPlane\" ][ \"location\" ], grid_spacing = ([ 2.0 , 2.0 , 30.0 ] if multiplane else [ 2.0 , 2.0 ]), grid_spacing_unit = \"microns\" , ) # link to external data image_series = TwoPhotonSeries ( name = \"TwoPhotonSeries\" , dimension = [ ops [ \"Ly\" ], ops [ \"Lx\" ]], external_file = ( ops [ \"filelist\" ] if \"filelist\" in ops else [ \"\" ]), imaging_plane = imaging_plane , starting_frame = [ 0 ], format = \"external\" , starting_time = 0.0 , rate = ops [ \"fs\" ] * ops [ \"nplanes\" ], ) nwbfile . add_acquisition ( image_series ) # processing img_seg = ImageSegmentation () ps = img_seg . create_plane_segmentation ( name = self . ophys_information [ \"PlaneSegmentation\" ][ \"name\" ], description = self . ophys_information [ \"PlaneSegmentation\" ][ \"description\" ], imaging_plane = imaging_plane , # reference_images=image_series, ) ophys_module = nwbfile . create_processing_module ( name = \"ophys\" , description = \"optical physiology processed data\" ) ophys_module . add ( img_seg ) file_strs = [ \"F.npy\" , \"Fneu.npy\" , \"spks.npy\" ] traces = [] ncells = np . zeros ( len ( self . pops ), dtype = np . int_ ) Nfr = np . array ([ self . pops [ k ][ \"nframes\" ] for k in self . pops . keys ()]) . max () for iplane , ops in self . pops . items (): if iplane == 0 : iscell = self . iscells [ iplane ] for fstr in file_strs : traces . append ( np . load ( os . path . join ( ops [ \"save_path\" ], fstr ))) PlaneCellsIdx = iplane * np . ones ( len ( iscell )) else : iscell = np . append ( iscell , self . iscells [ iplane ], axis = 0 , ) for i , fstr in enumerate ( file_strs ): trace = np . load ( os . path . join ( ops [ \"save_path\" ], fstr )) if trace . shape [ 1 ] < Nfr : fcat = np . zeros ( ( trace . shape [ 0 ], Nfr - trace . shape [ 1 ]), \"float32\" ) trace = np . concatenate (( trace , fcat ), axis = 1 ) traces [ i ] = np . append ( traces [ i ], trace , axis = 0 ) PlaneCellsIdx = np . append ( PlaneCellsIdx , iplane * np . ones ( len ( iscell ) - len ( PlaneCellsIdx )) ) stat = self . stats [ iplane ] ncells [ iplane ] = len ( stat ) for n in range ( ncells [ iplane ]): if multiplane : pixel_mask = np . array ( [ stat [ n ][ \"ypix\" ], stat [ n ][ \"xpix\" ], iplane * np . ones ( stat [ n ][ \"npix\" ]), stat [ n ][ \"lam\" ], ] ) ps . add_roi ( voxel_mask = pixel_mask . T ) else : pixel_mask = np . array ( [ stat [ n ][ \"ypix\" ], stat [ n ][ \"xpix\" ], stat [ n ][ \"lam\" ]] ) ps . add_roi ( pixel_mask = pixel_mask . T ) ps . add_column ( \"iscell\" , \"two columns - iscell & probcell\" , iscell ) rt_region = [] for iplane , ops in self . pops . items (): if iplane == 0 : rt_region . append ( ps . create_roi_table_region ( region = list ( np . arange ( 0 , ncells [ iplane ]), ), description = f \"ROIs for plane { int ( iplane ) } \" , ) ) else : rt_region . append ( ps . create_roi_table_region ( region = list ( np . arange ( np . sum ( ncells [: iplane ]), ncells [ iplane ] + np . sum ( ncells [: iplane ]), ) ), description = f \"ROIs for plane { int ( iplane ) } \" , ) ) # FLUORESCENCE (all are required) name_strs = [ \"Fluorescence\" , \"Neuropil\" , \"Deconvolved\" ] for i , ( fstr , nstr ) in enumerate ( zip ( file_strs , name_strs )): for iplane , ops in self . pops . items (): roi_resp_series = RoiResponseSeries ( name = f \"plane { int ( iplane ) } \" , data = traces [ i ][ PlaneCellsIdx == iplane ], rois = rt_region [ iplane ], unit = \"lumens\" , rate = ops [ \"fs\" ], ) if iplane == 0 : fl = Fluorescence ( roi_response_series = roi_resp_series , name = nstr ) else : fl . add_roi_response_series ( roi_response_series = roi_resp_series ) ophys_module . add ( fl ) io . write ( nwbfile ) io . close () return def load_suite2p_nwb ( self , path ): \"\"\" Load suite2p data from NWB Parameters ---------- path : str Path to the session \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r\" ) nwbfile = io . read () if \"ophys\" in nwbfile . processing . keys (): ophys = nwbfile . processing [ \"ophys\" ] ################################################################# # STATS, OPS and ISCELL ################################################################# dims = nwbfile . acquisition [ \"TwoPhotonSeries\" ] . dimension [:] self . ops = { \"Ly\" : dims [ 0 ], \"Lx\" : dims [ 1 ]} self . rate = nwbfile . acquisition [ \"TwoPhotonSeries\" ] . imaging_plane . imaging_rate self . stats = { 0 : {}} self . iscell = ophys [ \"ImageSegmentation\" ][ \"PlaneSegmentation\" ][ \"iscell\" ] . data [:] info = pd . DataFrame ( data = self . iscell [:, 0 ] . astype ( \"int\" ), columns = [ \"iscell\" ] ) ################################################################# # ROIS ################################################################# try : rois = nwbfile . processing [ \"ophys\" ][ \"ImageSegmentation\" ][ \"PlaneSegmentation\" ][ \"pixel_mask\" ] multiplane = False except Exception : rois = nwbfile . processing [ \"ophys\" ][ \"ImageSegmentation\" ][ \"PlaneSegmentation\" ][ \"voxel_mask\" ] multiplane = True idx = np . where ( self . iscell [:, 0 ])[ 0 ] info [ \"plane\" ] = 0 for n in range ( len ( rois )): roi = pd . DataFrame ( rois [ n ]) if \"z\" in roi . columns : pl = roi [ \"z\" ][ 0 ] else : pl = 0 info . loc [ n , \"plane\" ] = pl if pl not in self . stats . keys (): self . stats [ pl ] = {} if n in idx : self . stats [ pl ][ n ] = { \"xpix\" : roi [ \"y\" ] . values , \"ypix\" : roi [ \"x\" ] . values , \"lam\" : roi [ \"weight\" ] . values , } ################################################################# # Time Series ################################################################# fields = np . intersect1d ( [ \"Fluorescence\" , \"Neuropil\" , \"Deconvolved\" ], list ( ophys . fields [ \"data_interfaces\" ] . keys ()), ) if len ( fields ) == 0 : print ( \"No \" + \" or \" . join ([ \"Fluorescence\" , \"Neuropil\" , \"Deconvolved\" ]), \"found in nwb {} \" . format ( self . nwbfilepath ), ) return False keys = ophys [ fields [ 0 ]] . roi_response_series . keys () planes = [ int ( k [ - 1 ]) for k in keys if \"plane\" in k ] data = {} if multiplane : keys = ophys [ fields [ 0 ]] . roi_response_series . keys () planes = [ int ( k [ - 1 ]) for k in keys if \"plane\" in k ] else : planes = [ 0 ] for k , name in zip ( [ \"F\" , \"Fneu\" , \"spks\" ], [ \"Fluorescence\" , \"Neuropil\" , \"Deconvolved\" ] ): tmp = [] timestamps = [] for i , n in enumerate ( planes ): if multiplane : pl = \"plane {} \" . format ( n ) else : pl = name # This doesn't make sense tokeep = info [ \"iscell\" ][ info [ \"plane\" ] == n ] . values == 1 d = np . transpose ( ophys [ name ][ pl ] . data [:][ tokeep ]) if ophys [ name ][ pl ] . timestamps is not None : t = ophys [ name ][ pl ] . timestamps [:] else : t = ( np . arange ( 0 , len ( d )) / self . rate ) + ophys [ name ][ pl ] . starting_time tmp . append ( d ) timestamps . append ( t ) data [ k ] = nap . TsdFrame ( t = timestamps [ 0 ], d = np . hstack ( tmp )) if \"F\" in data . keys (): self . F = data [ \"F\" ] if \"Fneu\" in data . keys (): self . Fneu = data [ \"Fneu\" ] if \"spks\" in data . keys (): self . spks = data [ \"spks\" ] self . plane_info = pd . DataFrame ( data = info [ \"plane\" ][ info [ \"iscell\" ] == 1 ] . values , columns = [ \"plane\" ] ) io . close () return True else : io . close () return False","title":"Suite2P"},{"location":"io.suite2p/#pynapple.io.suite2p.Suite2P.__init__","text":"Parameters: Name Type Description Default path str The path of the session required Source code in pynapple/io/suite2p.py def __init__ ( self , path ): \"\"\" Parameters ---------- path : str The path of the session \"\"\" self . basename = os . path . basename ( path ) super () . __init__ ( path ) # Need to check if nwb file exists and if data are there loading_my_data = True if self . path is not None : nwb_path = os . path . join ( self . path , \"pynapplenwb\" ) if os . path . exists ( nwb_path ): files = os . listdir ( nwb_path ) if len ([ f for f in files if f . endswith ( \".nwb\" )]): success = self . load_suite2p_nwb ( path ) if success : loading_my_data = False # Bypass if data have already been transfered to nwb if loading_my_data : app = App () window = OphysGUI ( app , path = path ) app . mainloop () try : app . update () except Exception : pass if window . status : self . ophys_information = window . ophys_information self . load_suite2p ( path ) self . save_suite2p_nwb ( path )","title":"__init__()"},{"location":"io.suite2p/#pynapple.io.suite2p.Suite2P.load_suite2p","text":"Looking for suite2/plane* Parameters: Name Type Description Default path str The path of the session required Source code in pynapple/io/suite2p.py def load_suite2p ( self , path ): \"\"\" Looking for suite2/plane* Parameters ---------- path : str The path of the session \"\"\" self . path_suite2p = os . path . join ( path , \"suite2p\" ) self . sampling_rate = float ( self . ophys_information [ \"ImagingPlane\" ][ \"imaging_rate\" ] ) data = { \"F\" : [], \"Fneu\" : [], \"spks\" : [], } plane_info = [] self . stats = {} self . pops = {} self . iscells = {} self . planes = [] if os . path . exists ( self . path_suite2p ): planes = glob . glob ( os . path . join ( self . path_suite2p , \"plane*\" )) if len ( planes ): # count = 0 for plane_dir in planes : n = int ( os . path . basename ( plane_dir )[ - 1 ]) self . planes . append ( n ) # Loading iscell.npy try : iscell = np . load ( os . path . join ( plane_dir , \"iscell.npy\" ), allow_pickle = True ) idx = np . where ( iscell . astype ( \"int\" )[:, 0 ])[ 0 ] plane_info . append ( np . ones ( len ( idx ), dtype = \"int\" ) * n ) except OSError as e : print ( e ) sys . exit () # Loading F.npy, Fneu.py and spks.npy for obj in [ \"F.npy\" , \"Fneu.npy\" , \"spks.npy\" ]: try : name = obj . split ( \".\" )[ 0 ] tmp = np . load ( os . path . join ( plane_dir , obj ), allow_pickle = True ) data [ name ] . append ( tmp [ idx ]) except OSError as e : print ( e ) sys . exit () # Loading stat.npy and ops.npy try : stat = np . load ( os . path . join ( plane_dir , \"stat.npy\" ), allow_pickle = True ) ops = np . load ( os . path . join ( plane_dir , \"ops.npy\" ), allow_pickle = True ) . item () except OSError as e : print ( e ) sys . exit () # Saving stat, ops and iscell self . stats [ n ] = stat self . pops [ n ] = ops self . iscells [ n ] = iscell # count += len(idx) else : warnings . warn ( \"Couldn't find planes in %s \" % self . path_suite2p , stacklevel = 2 ) sys . exit () else : warnings . warn ( \"No suite2p folder in %s \" % path , stacklevel = 2 ) sys . exit () # Calcium transients data [ \"F\" ] = np . transpose ( np . vstack ( data [ \"F\" ])) data [ \"Fneu\" ] = np . transpose ( np . vstack ( data [ \"Fneu\" ])) data [ \"spks\" ] = np . transpose ( np . vstack ( data [ \"spks\" ])) time_index = np . arange ( 0 , len ( data [ \"F\" ])) / self . sampling_rate self . F = nap . TsdFrame ( t = time_index , d = data [ \"F\" ]) self . Fneu = nap . TsdFrame ( t = time_index , d = data [ \"Fneu\" ]) self . spks = nap . TsdFrame ( t = time_index , d = data [ \"spks\" ]) self . ops = self . pops [ 0 ] self . iscell = np . vstack ([ self . iscells [ k ] for k in self . iscells . keys ()]) # Metadata self . plane_info = pd . DataFrame . from_dict ({ \"plane\" : np . hstack ( plane_info )}) return","title":"load_suite2p()"},{"location":"io.suite2p/#pynapple.io.suite2p.Suite2P.save_suite2p_nwb","text":"Save the data to NWB. To ensure continuity, this function is based on : https://github.com/MouseLand/suite2p/blob/main/suite2p/io/nwb.py. Parameters: Name Type Description Default path str The path of the session required Source code in pynapple/io/suite2p.py def save_suite2p_nwb ( self , path ): \"\"\" Save the data to NWB. To ensure continuity, this function is based on : https://github.com/MouseLand/suite2p/blob/main/suite2p/io/nwb.py. Parameters ---------- path : str The path of the session \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) multiplane = True if len ( self . planes ) > 1 else False ops = self . pops [ list ( self . pops . keys ())[ 0 ]] io = NWBHDF5IO ( self . nwbfilepath , \"r+\" ) nwbfile = io . read () device = nwbfile . create_device ( name = self . ophys_information [ \"device\" ][ \"name\" ], description = self . ophys_information [ \"device\" ][ \"description\" ], manufacturer = self . ophys_information [ \"device\" ][ \"manufacturer\" ], ) imaging_plane = nwbfile . create_imaging_plane ( name = self . ophys_information [ \"ImagingPlane\" ][ \"name\" ], optical_channel = OpticalChannel ( name = self . ophys_information [ \"OpticalChannel\" ][ \"name\" ], description = self . ophys_information [ \"OpticalChannel\" ][ \"description\" ], emission_lambda = float ( self . ophys_information [ \"OpticalChannel\" ][ \"emission_lambda\" ] ), ), imaging_rate = self . sampling_rate , description = self . ophys_information [ \"ImagingPlane\" ][ \"description\" ], device = device , excitation_lambda = float ( self . ophys_information [ \"ImagingPlane\" ][ \"excitation_lambda\" ] ), indicator = self . ophys_information [ \"ImagingPlane\" ][ \"indicator\" ], location = self . ophys_information [ \"ImagingPlane\" ][ \"location\" ], grid_spacing = ([ 2.0 , 2.0 , 30.0 ] if multiplane else [ 2.0 , 2.0 ]), grid_spacing_unit = \"microns\" , ) # link to external data image_series = TwoPhotonSeries ( name = \"TwoPhotonSeries\" , dimension = [ ops [ \"Ly\" ], ops [ \"Lx\" ]], external_file = ( ops [ \"filelist\" ] if \"filelist\" in ops else [ \"\" ]), imaging_plane = imaging_plane , starting_frame = [ 0 ], format = \"external\" , starting_time = 0.0 , rate = ops [ \"fs\" ] * ops [ \"nplanes\" ], ) nwbfile . add_acquisition ( image_series ) # processing img_seg = ImageSegmentation () ps = img_seg . create_plane_segmentation ( name = self . ophys_information [ \"PlaneSegmentation\" ][ \"name\" ], description = self . ophys_information [ \"PlaneSegmentation\" ][ \"description\" ], imaging_plane = imaging_plane , # reference_images=image_series, ) ophys_module = nwbfile . create_processing_module ( name = \"ophys\" , description = \"optical physiology processed data\" ) ophys_module . add ( img_seg ) file_strs = [ \"F.npy\" , \"Fneu.npy\" , \"spks.npy\" ] traces = [] ncells = np . zeros ( len ( self . pops ), dtype = np . int_ ) Nfr = np . array ([ self . pops [ k ][ \"nframes\" ] for k in self . pops . keys ()]) . max () for iplane , ops in self . pops . items (): if iplane == 0 : iscell = self . iscells [ iplane ] for fstr in file_strs : traces . append ( np . load ( os . path . join ( ops [ \"save_path\" ], fstr ))) PlaneCellsIdx = iplane * np . ones ( len ( iscell )) else : iscell = np . append ( iscell , self . iscells [ iplane ], axis = 0 , ) for i , fstr in enumerate ( file_strs ): trace = np . load ( os . path . join ( ops [ \"save_path\" ], fstr )) if trace . shape [ 1 ] < Nfr : fcat = np . zeros ( ( trace . shape [ 0 ], Nfr - trace . shape [ 1 ]), \"float32\" ) trace = np . concatenate (( trace , fcat ), axis = 1 ) traces [ i ] = np . append ( traces [ i ], trace , axis = 0 ) PlaneCellsIdx = np . append ( PlaneCellsIdx , iplane * np . ones ( len ( iscell ) - len ( PlaneCellsIdx )) ) stat = self . stats [ iplane ] ncells [ iplane ] = len ( stat ) for n in range ( ncells [ iplane ]): if multiplane : pixel_mask = np . array ( [ stat [ n ][ \"ypix\" ], stat [ n ][ \"xpix\" ], iplane * np . ones ( stat [ n ][ \"npix\" ]), stat [ n ][ \"lam\" ], ] ) ps . add_roi ( voxel_mask = pixel_mask . T ) else : pixel_mask = np . array ( [ stat [ n ][ \"ypix\" ], stat [ n ][ \"xpix\" ], stat [ n ][ \"lam\" ]] ) ps . add_roi ( pixel_mask = pixel_mask . T ) ps . add_column ( \"iscell\" , \"two columns - iscell & probcell\" , iscell ) rt_region = [] for iplane , ops in self . pops . items (): if iplane == 0 : rt_region . append ( ps . create_roi_table_region ( region = list ( np . arange ( 0 , ncells [ iplane ]), ), description = f \"ROIs for plane { int ( iplane ) } \" , ) ) else : rt_region . append ( ps . create_roi_table_region ( region = list ( np . arange ( np . sum ( ncells [: iplane ]), ncells [ iplane ] + np . sum ( ncells [: iplane ]), ) ), description = f \"ROIs for plane { int ( iplane ) } \" , ) ) # FLUORESCENCE (all are required) name_strs = [ \"Fluorescence\" , \"Neuropil\" , \"Deconvolved\" ] for i , ( fstr , nstr ) in enumerate ( zip ( file_strs , name_strs )): for iplane , ops in self . pops . items (): roi_resp_series = RoiResponseSeries ( name = f \"plane { int ( iplane ) } \" , data = traces [ i ][ PlaneCellsIdx == iplane ], rois = rt_region [ iplane ], unit = \"lumens\" , rate = ops [ \"fs\" ], ) if iplane == 0 : fl = Fluorescence ( roi_response_series = roi_resp_series , name = nstr ) else : fl . add_roi_response_series ( roi_response_series = roi_resp_series ) ophys_module . add ( fl ) io . write ( nwbfile ) io . close () return","title":"save_suite2p_nwb()"},{"location":"io.suite2p/#pynapple.io.suite2p.Suite2P.load_suite2p_nwb","text":"Load suite2p data from NWB Parameters: Name Type Description Default path str Path to the session required Source code in pynapple/io/suite2p.py def load_suite2p_nwb ( self , path ): \"\"\" Load suite2p data from NWB Parameters ---------- path : str Path to the session \"\"\" self . nwb_path = os . path . join ( path , \"pynapplenwb\" ) if not os . path . exists ( self . nwb_path ): raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if \"nwb\" in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , \"r\" ) nwbfile = io . read () if \"ophys\" in nwbfile . processing . keys (): ophys = nwbfile . processing [ \"ophys\" ] ################################################################# # STATS, OPS and ISCELL ################################################################# dims = nwbfile . acquisition [ \"TwoPhotonSeries\" ] . dimension [:] self . ops = { \"Ly\" : dims [ 0 ], \"Lx\" : dims [ 1 ]} self . rate = nwbfile . acquisition [ \"TwoPhotonSeries\" ] . imaging_plane . imaging_rate self . stats = { 0 : {}} self . iscell = ophys [ \"ImageSegmentation\" ][ \"PlaneSegmentation\" ][ \"iscell\" ] . data [:] info = pd . DataFrame ( data = self . iscell [:, 0 ] . astype ( \"int\" ), columns = [ \"iscell\" ] ) ################################################################# # ROIS ################################################################# try : rois = nwbfile . processing [ \"ophys\" ][ \"ImageSegmentation\" ][ \"PlaneSegmentation\" ][ \"pixel_mask\" ] multiplane = False except Exception : rois = nwbfile . processing [ \"ophys\" ][ \"ImageSegmentation\" ][ \"PlaneSegmentation\" ][ \"voxel_mask\" ] multiplane = True idx = np . where ( self . iscell [:, 0 ])[ 0 ] info [ \"plane\" ] = 0 for n in range ( len ( rois )): roi = pd . DataFrame ( rois [ n ]) if \"z\" in roi . columns : pl = roi [ \"z\" ][ 0 ] else : pl = 0 info . loc [ n , \"plane\" ] = pl if pl not in self . stats . keys (): self . stats [ pl ] = {} if n in idx : self . stats [ pl ][ n ] = { \"xpix\" : roi [ \"y\" ] . values , \"ypix\" : roi [ \"x\" ] . values , \"lam\" : roi [ \"weight\" ] . values , } ################################################################# # Time Series ################################################################# fields = np . intersect1d ( [ \"Fluorescence\" , \"Neuropil\" , \"Deconvolved\" ], list ( ophys . fields [ \"data_interfaces\" ] . keys ()), ) if len ( fields ) == 0 : print ( \"No \" + \" or \" . join ([ \"Fluorescence\" , \"Neuropil\" , \"Deconvolved\" ]), \"found in nwb {} \" . format ( self . nwbfilepath ), ) return False keys = ophys [ fields [ 0 ]] . roi_response_series . keys () planes = [ int ( k [ - 1 ]) for k in keys if \"plane\" in k ] data = {} if multiplane : keys = ophys [ fields [ 0 ]] . roi_response_series . keys () planes = [ int ( k [ - 1 ]) for k in keys if \"plane\" in k ] else : planes = [ 0 ] for k , name in zip ( [ \"F\" , \"Fneu\" , \"spks\" ], [ \"Fluorescence\" , \"Neuropil\" , \"Deconvolved\" ] ): tmp = [] timestamps = [] for i , n in enumerate ( planes ): if multiplane : pl = \"plane {} \" . format ( n ) else : pl = name # This doesn't make sense tokeep = info [ \"iscell\" ][ info [ \"plane\" ] == n ] . values == 1 d = np . transpose ( ophys [ name ][ pl ] . data [:][ tokeep ]) if ophys [ name ][ pl ] . timestamps is not None : t = ophys [ name ][ pl ] . timestamps [:] else : t = ( np . arange ( 0 , len ( d )) / self . rate ) + ophys [ name ][ pl ] . starting_time tmp . append ( d ) timestamps . append ( t ) data [ k ] = nap . TsdFrame ( t = timestamps [ 0 ], d = np . hstack ( tmp )) if \"F\" in data . keys (): self . F = data [ \"F\" ] if \"Fneu\" in data . keys (): self . Fneu = data [ \"Fneu\" ] if \"spks\" in data . keys (): self . spks = data [ \"spks\" ] self . plane_info = pd . DataFrame ( data = info [ \"plane\" ][ info [ \"iscell\" ] == 1 ] . values , columns = [ \"plane\" ] ) io . close () return True else : io . close () return False","title":"load_suite2p_nwb()"},{"location":"nwbmatic-custom-io/","text":"Custom Loading IO This example shows how to construct a custom IO loader. # -*- coding: utf-8 -*- # @Author: gviejo # @Date: 2022-01-06 20:01:32 # @Last Modified by: gviejo # @Last Modified time: 2022-01-06 20:01:57 import os from nwbmatic.loader import BaseLoader from pynwb import NWBFile , NWBHDF5IO class MyCustomIO ( BaseLoader ): def __init__ ( self , path ): \"\"\" Parameters ---------- path : str The path to the data. \"\"\" self . basename = os . path . basename ( path ) super () . __init__ ( path ) # Need to check if nwb file exists and if data are there loading_my_data = True if self . path is not None : nwb_path = os . path . join ( self . path , 'pynapplenwb' ) if os . path . exists ( nwb_path ): files = os . listdir ( nwb_path ) if len ([ f for f in files if f . endswith ( '.nwb' )]): success = self . load_my_nwb ( path ) if success : loading_my_data = False # Bypass if data have already been transfered to nwb if loading_my_data : self . load_my_data ( path ) self . save_my_data_in_nwb ( path ) def load_my_data ( self , path ): \"\"\" This load the raw data Parameters ---------- path : str Path to the session \"\"\" ''' Load Raw data here ''' print ( path ) return None def save_my_data_in_nwb ( self , path ): \"\"\" Save the raw data to NWB Parameters ---------- path : TYPE Description \"\"\" self . nwb_path = os . path . join ( path , 'pynapplenwb' ) if os . path . exists ( self . nwb_path ): files = os . listdir ( self . nwb_path ) else : raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if 'nwb' in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , 'r+' ) ''' Save data in NWB here ''' io . close () return def load_my_nwb ( self , path ): \"\"\" This load the nwb that is already create by the base loader Parameters ---------- path : str Path to the session \"\"\" self . nwb_path = os . path . join ( path , 'pynapplenwb' ) if os . path . exists ( self . nwb_path ): files = os . listdir ( self . nwb_path ) else : raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if 'nwb' in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , 'r' ) nwbfile = io . read () ''' Add code to write to nwb file here ''' io . close () mydata = MyCustomIO ( '.' ) print ( type ( mydata ))","title":"Custom IO"},{"location":"nwbmatic-custom-io/#custom-loading-io","text":"This example shows how to construct a custom IO loader. # -*- coding: utf-8 -*- # @Author: gviejo # @Date: 2022-01-06 20:01:32 # @Last Modified by: gviejo # @Last Modified time: 2022-01-06 20:01:57 import os from nwbmatic.loader import BaseLoader from pynwb import NWBFile , NWBHDF5IO class MyCustomIO ( BaseLoader ): def __init__ ( self , path ): \"\"\" Parameters ---------- path : str The path to the data. \"\"\" self . basename = os . path . basename ( path ) super () . __init__ ( path ) # Need to check if nwb file exists and if data are there loading_my_data = True if self . path is not None : nwb_path = os . path . join ( self . path , 'pynapplenwb' ) if os . path . exists ( nwb_path ): files = os . listdir ( nwb_path ) if len ([ f for f in files if f . endswith ( '.nwb' )]): success = self . load_my_nwb ( path ) if success : loading_my_data = False # Bypass if data have already been transfered to nwb if loading_my_data : self . load_my_data ( path ) self . save_my_data_in_nwb ( path ) def load_my_data ( self , path ): \"\"\" This load the raw data Parameters ---------- path : str Path to the session \"\"\" ''' Load Raw data here ''' print ( path ) return None def save_my_data_in_nwb ( self , path ): \"\"\" Save the raw data to NWB Parameters ---------- path : TYPE Description \"\"\" self . nwb_path = os . path . join ( path , 'pynapplenwb' ) if os . path . exists ( self . nwb_path ): files = os . listdir ( self . nwb_path ) else : raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if 'nwb' in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , 'r+' ) ''' Save data in NWB here ''' io . close () return def load_my_nwb ( self , path ): \"\"\" This load the nwb that is already create by the base loader Parameters ---------- path : str Path to the session \"\"\" self . nwb_path = os . path . join ( path , 'pynapplenwb' ) if os . path . exists ( self . nwb_path ): files = os . listdir ( self . nwb_path ) else : raise RuntimeError ( \"Path {} does not exist.\" . format ( self . nwb_path )) self . nwbfilename = [ f for f in os . listdir ( self . nwb_path ) if 'nwb' in f ][ 0 ] self . nwbfilepath = os . path . join ( self . nwb_path , self . nwbfilename ) io = NWBHDF5IO ( self . nwbfilepath , 'r' ) nwbfile = io . read () ''' Add code to write to nwb file here ''' io . close () mydata = MyCustomIO ( '.' ) print ( type ( mydata ))","title":"Custom Loading IO"},{"location":"nwbmatic-neurosuite-optitrack/","text":"Tutorial In this example dataset , the data contains a sample recording from the anterodorsal nucleus of the thalamus and the hippocampus, with both a sleep and a wake phase. It contains both head-direction cells (i.e. cells that fire for a particular direction of the head in the horizontal plane) and place cells (i.e. cells that fire for a particular position in the environment). The example dataset looks like this: Preprocessing of the data was made with Kilosort 2.0 and spike sorting was made with Klusters . Tracking of the animal was done with Motive Optitrack. The file A2929-200711_1.csv contains the tracking data (both position and rotation of the head of the animal). The binary file A2929-200711_1_analogin.dat contains the TTL pulses tracking the camera frames. This tutorial demonstrates how to load data with nwbmatic. Import nwbmatic import nwbmatic as ntm Session loader The first step is to call the function load_session . It will then open a GUI for filling manually the information. The following screenshots show what parameters to use. data_directory = 'your/path/to/A2929-200711' data = ntm . load_session ( data_directory , 'neurosuite' ) Session Information The second step is to provides information about the session and the subject. All the fields shown are suggested by the NWB format (see here and here ). Epochs The epochs tab loads the epochs within the session (typically wake and sleep). In this case, we load the file Epoch_Ts.csv in the data folder. The first column contains the start of the epoch and the second column contains the end of the epoch. If the CSV file contains a third column with an epoch label, the loader will automatically write it in the label column. Otherwise, it is necessary to manually write the epoch labels. Tracking The tracking tab allows to load tracking data saved with a CSV file. Reading a CSV file is always a challenge when the header is unknown. The default csv file should contains only one row for the header with the column names. The first column should be the time index in seconds. Other formats are DeepLabCut and Optitrack. Frame alignement can vary as well. Pynapple offers three ways to align the tracking frames : Global timestamps The time column of the CSV file contains the timestamps aligned to the global timeframe of the session. Local timestamps The time column of the CSV file contains the timestamps aligned to one epoch. In this case, the user should select which epoch. TTL detection A binary file containing TTL pulses for each tracking frame is located within the folder and can be loaded. Alignement is made with TTL detection. In this example session, Tracking was made with Optitrack and TTL pulses were written to an analogin file recorded by an Intan RHD2000 recording system. The parameters for the tracking tab are shown below. Ephys loader The next step is specific to NeuroSuite. In this case, 2 electrophysiological probes were implanted, one to the ADN and another to the CA1. This step allows to label groups of electrodes as shown below. NWB file If successful, a NWB file should be created in session_folder/pynapplenwb/session_name.nwb Calling the function load_session should directly read the NWB file and bypass the GUI loader. data = ntm . load_session ( data_directory , 'neurosuite' ) In this case, the data that can be used for analysis are spikes , position and epochs . spikes = data . spikes position = data . position epochs = data . epochs print ( spikes , ' \\n ' ) print ( position , ' \\n ' ) print ( epochs , ' \\n ' ) Index Freq. (Hz) group ------- ------------ ------- 0 7.3 0 1 5.73 0 2 8.12 0 3 6.68 0 4 10.77 0 5 11 0 6 16.52 0 7 2.2 1 8 2.02 1 9 1.07 1 10 3.92 1 11 3.31 1 12 1.09 1 13 1.28 1 14 1.32 1 rx ry rz x y z Time (s) 670.64070 0.343163 5.207148 5.933598 -0.042857 0.425023 -0.195725 670.64900 0.346745 5.181029 5.917368 -0.043863 0.424850 -0.195110 670.65735 0.344035 5.155508 5.905679 -0.044853 0.424697 -0.194674 670.66565 0.322240 5.136537 5.892457 -0.045787 0.424574 -0.194342 670.67400 0.315836 5.120850 5.891577 -0.046756 0.424563 -0.194059 ... ... ... ... ... ... ... 1199.96160 6.009812 3.665954 0.230562 0.011241 0.037891 -0.001479 1199.96995 6.014660 3.634619 0.260742 0.010974 0.038677 -0.002370 1199.97825 6.031694 3.617849 0.276835 0.010786 0.039410 -0.003156 1199.98660 6.040435 3.609446 0.287006 0.010661 0.040064 -0.003821 1199.99495 6.050059 3.609375 0.293275 0.010624 0.040568 -0.004435 [63527 rows x 6 columns] {'sleep': start end 0 0.0 600.0, 'wake': start end 0 600.0 1200.0}","title":"Loading neurosuite-optitrack"},{"location":"nwbmatic-neurosuite-optitrack/#tutorial","text":"In this example dataset , the data contains a sample recording from the anterodorsal nucleus of the thalamus and the hippocampus, with both a sleep and a wake phase. It contains both head-direction cells (i.e. cells that fire for a particular direction of the head in the horizontal plane) and place cells (i.e. cells that fire for a particular position in the environment). The example dataset looks like this: Preprocessing of the data was made with Kilosort 2.0 and spike sorting was made with Klusters . Tracking of the animal was done with Motive Optitrack. The file A2929-200711_1.csv contains the tracking data (both position and rotation of the head of the animal). The binary file A2929-200711_1_analogin.dat contains the TTL pulses tracking the camera frames. This tutorial demonstrates how to load data with nwbmatic.","title":"Tutorial"},{"location":"nwbmatic-neurosuite-optitrack/#import-nwbmatic","text":"import nwbmatic as ntm","title":"Import nwbmatic"},{"location":"nwbmatic-neurosuite-optitrack/#session-loader","text":"The first step is to call the function load_session . It will then open a GUI for filling manually the information. The following screenshots show what parameters to use. data_directory = 'your/path/to/A2929-200711' data = ntm . load_session ( data_directory , 'neurosuite' )","title":"Session loader"},{"location":"nwbmatic-neurosuite-optitrack/#session-information","text":"The second step is to provides information about the session and the subject. All the fields shown are suggested by the NWB format (see here and here ).","title":"Session Information"},{"location":"nwbmatic-neurosuite-optitrack/#epochs","text":"The epochs tab loads the epochs within the session (typically wake and sleep). In this case, we load the file Epoch_Ts.csv in the data folder. The first column contains the start of the epoch and the second column contains the end of the epoch. If the CSV file contains a third column with an epoch label, the loader will automatically write it in the label column. Otherwise, it is necessary to manually write the epoch labels.","title":"Epochs"},{"location":"nwbmatic-neurosuite-optitrack/#tracking","text":"The tracking tab allows to load tracking data saved with a CSV file. Reading a CSV file is always a challenge when the header is unknown. The default csv file should contains only one row for the header with the column names. The first column should be the time index in seconds. Other formats are DeepLabCut and Optitrack. Frame alignement can vary as well. Pynapple offers three ways to align the tracking frames :","title":"Tracking"},{"location":"nwbmatic-neurosuite-optitrack/#global-timestamps","text":"The time column of the CSV file contains the timestamps aligned to the global timeframe of the session.","title":"Global timestamps"},{"location":"nwbmatic-neurosuite-optitrack/#local-timestamps","text":"The time column of the CSV file contains the timestamps aligned to one epoch. In this case, the user should select which epoch.","title":"Local timestamps"},{"location":"nwbmatic-neurosuite-optitrack/#ttl-detection","text":"A binary file containing TTL pulses for each tracking frame is located within the folder and can be loaded. Alignement is made with TTL detection. In this example session, Tracking was made with Optitrack and TTL pulses were written to an analogin file recorded by an Intan RHD2000 recording system. The parameters for the tracking tab are shown below.","title":"TTL detection"},{"location":"nwbmatic-neurosuite-optitrack/#ephys-loader","text":"The next step is specific to NeuroSuite. In this case, 2 electrophysiological probes were implanted, one to the ADN and another to the CA1. This step allows to label groups of electrodes as shown below.","title":"Ephys loader"},{"location":"nwbmatic-neurosuite-optitrack/#nwb-file","text":"If successful, a NWB file should be created in session_folder/pynapplenwb/session_name.nwb Calling the function load_session should directly read the NWB file and bypass the GUI loader. data = ntm . load_session ( data_directory , 'neurosuite' ) In this case, the data that can be used for analysis are spikes , position and epochs . spikes = data . spikes position = data . position epochs = data . epochs print ( spikes , ' \\n ' ) print ( position , ' \\n ' ) print ( epochs , ' \\n ' ) Index Freq. (Hz) group ------- ------------ ------- 0 7.3 0 1 5.73 0 2 8.12 0 3 6.68 0 4 10.77 0 5 11 0 6 16.52 0 7 2.2 1 8 2.02 1 9 1.07 1 10 3.92 1 11 3.31 1 12 1.09 1 13 1.28 1 14 1.32 1 rx ry rz x y z Time (s) 670.64070 0.343163 5.207148 5.933598 -0.042857 0.425023 -0.195725 670.64900 0.346745 5.181029 5.917368 -0.043863 0.424850 -0.195110 670.65735 0.344035 5.155508 5.905679 -0.044853 0.424697 -0.194674 670.66565 0.322240 5.136537 5.892457 -0.045787 0.424574 -0.194342 670.67400 0.315836 5.120850 5.891577 -0.046756 0.424563 -0.194059 ... ... ... ... ... ... ... 1199.96160 6.009812 3.665954 0.230562 0.011241 0.037891 -0.001479 1199.96995 6.014660 3.634619 0.260742 0.010974 0.038677 -0.002370 1199.97825 6.031694 3.617849 0.276835 0.010786 0.039410 -0.003156 1199.98660 6.040435 3.609446 0.287006 0.010661 0.040064 -0.003821 1199.99495 6.050059 3.609375 0.293275 0.010624 0.040568 -0.004435 [63527 rows x 6 columns] {'sleep': start end 0 0.0 600.0, 'wake': start end 0 600.0 1200.0}","title":"NWB file"}]}