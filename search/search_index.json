{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"nwbmatic","text":"<p>NWB creator from various data streams</p> <p>TODO badges</p>"},{"location":"#overview","title":"Overview","text":"<p>This package started as a main feature of pynapple IO module. It is now a standalone package to help create NWB from various data streams from electrophysiological and calcium imaging pipelines. It supports outputs from :</p> Electrophysiology Calcium imaging Behavior Phy matlab CNMF-E DeepLabCut Neurosuite Inscopix CNMF-E Optitrack Minian Suite2P <p>Warning A larger choice of data format is available from neuroconv</p>"},{"location":"#usage","title":"Usage","text":"<p>The general workflow of loading a session is described by the infographic below. As it is challenging to accomodate all possible types of format, we aimed to keep the IO of nwbmatic minimal while allowing the user to inherit the base loader and import their own custom io functions. </p> <p>The base loader is thus responsible for initializing the NWB file containing the tracking data, the epochs and the session informations.</p> <p></p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#installation","title":"Installation","text":"<p>The best way to install nwbmatic is with pip within a new conda environment :</p> <pre><code>$ conda create --name nwbmatic pip python=3.8\n$ conda activate nwbmatic\n$ pip install nwbmatic\n</code></pre> <p>or directly from the source code:</p> <pre><code>$ conda create --name nwbmatic pip python=3.8\n$ conda activate nwbmatic\n$ # clone the repository\n$ git clone https://github.com/pynapple-org/nwbmatic.git\n$ cd nwbmatic\n$ # Install in editable mode with `-e` or, equivalently, `--editable`\n$ pip install -e .\n</code></pre> <p>This procedure will install all the dependencies including </p> <ul> <li>pynapple</li> <li>pandas</li> <li>numpy</li> <li>pynwb 2.0</li> <li>h5py</li> </ul>"},{"location":"#example","title":"Example","text":"<p>In this example, a session preprocessed with phy will be copied to NWB and loaded.</p> <pre><code>import nwbmatic as ntm\ndata = ntm.load_session(\"path/to/my/session\", \"phy\")\n</code></pre>"},{"location":"#credits","title":"Credits","text":"<p>Thanks to Selen Calgin, Sara Mahallati and Luigi Petrucco for their contributions.</p>"},{"location":"AUTHORS/","title":"Credits","text":""},{"location":"AUTHORS/#development-lead","title":"Development Lead","text":"<ul> <li>Guillaume Viejo guillaume.viejo@gmail.com</li> </ul>"},{"location":"AUTHORS/#contributors","title":"Contributors","text":"<ul> <li>Sara Mahallati</li> <li>Luigi Petrucco</li> <li>Selen Calgi</li> </ul>"},{"location":"CONTRIBUTING/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"CONTRIBUTING/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"CONTRIBUTING/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/pynapple-org/nwbmatic/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in     troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"CONTRIBUTING/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \\\"bug\\\" and \\\"help wanted\\\" is open to whoever wants to implement it.</p>"},{"location":"CONTRIBUTING/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \\\"enhancement\\\" and \\\"help wanted\\\" is open to whoever wants to implement it.</p>"},{"location":"CONTRIBUTING/#write-documentation","title":"Write Documentation","text":"<p>nwbmatic could always use more documentation, whether as part of the official nwbmatic docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"CONTRIBUTING/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/pynapple-org/nwbmatic/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to     implement.</li> <li>Remember that this is a volunteer-driven project, and that     contributions are welcome :)</li> </ul>"},{"location":"CONTRIBUTING/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up [nwbmatic]{https://github.com/pynapple-org/nwbmatic} for local development.</p> <ol> <li>Fork the [nwbmatic]{https://github.com/pynapple-org/nwbmatic} repo on GitHub.</li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/nwbmatic.git\n</code></pre> </li> <li> <p>Install your local copy with pip. </p> <pre><code>$ cd nwbmatic/\n$ pip install -e .\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> </ol> <ol> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"CONTRIBUTING/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.nd.</li> <li>The pull request should work for Python 3.8, 3.9 and 3.10, and     for PyPy.      </li> </ol>"},{"location":"io.cnmfe/","title":"CNMF-E","text":"<p>Loaders for calcium imaging data with miniscope. Support CNMF-E in matlab, inscopix-cnmfe and minian.</p>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.CNMF_E","title":"<code>CNMF_E</code>","text":"<p>         Bases: <code>BaseLoader</code></p> <p>Loader for data processed with matlab CNMF-E(https://github.com/zhoupc/CNMF_E). The path folder should contain a file ending in .mat when calling Source2d.save_neurons</p> <p>Attributes:</p> Name Type Description <code>A</code> <code>numpy.ndarray</code> <p>Spatial footprints</p> <code>C</code> <code>TsdFrame</code> <p>The calcium transients</p> <code>sampling_rate</code> <code>float</code> <p>Sampling rate of the data (default is 30 Hz).</p> Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/cnmfe.py</code> <pre><code>class CNMF_E(BaseLoader):\n\"\"\"Loader for data processed with matlab CNMF-E(https://github.com/zhoupc/CNMF_E).\n    The path folder should contain a file ending in .mat\n    when calling Source2d.save_neurons\n    Attributes\n    ----------\n    A : numpy.ndarray\n        Spatial footprints\n    C : TsdFrame\n        The calcium transients\n    sampling_rate : float\n        Sampling rate of the data (default is 30 Hz).\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Parameters\n        ----------\n        path : str\n            The path to the data.\n        \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_cnmf_e(path)\nself.save_cnmfe_nwb(path)\ndef load_cnmf_e(self, path):\n\"\"\"\n        Load the calcium transients and the spatial footprints.\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nfiles = os.listdir(path)\nmatfiles = [f for f in files if f.endswith(\".mat\")]\nif len(matfiles):\ndata = loadmat(os.path.join(path, matfiles[0]), struct_as_record=False)\nelse:\nraise RuntimeError(\"No mat file found in {}\".format(path))\nself.struct = data[\"neuron_results\"][0][0]\nC = self.struct.C.T\nself.A = self.struct.A.T\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C)\nreturn None\ndef save_cnmfe_nwb(self, path):\n\"\"\"\n        Save the data to NWB.\n        Since there is no one-photon field in nwb, it uses the two-photon field.\n        Parameters\n        ----------\n        path : TYPE\n            Description\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = np.atleast_2d(self.A[i])\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_cnmfe_nwb(self, path):\n\"\"\"\n        Load the calcium transient and spatial footprint from nwb\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.CNMF_E.__init__","title":"<code>__init__(path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data.</p> required Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/cnmfe.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Parameters\n    ----------\n    path : str\n        The path to the data.\n    \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_cnmf_e(path)\nself.save_cnmfe_nwb(path)\n</code></pre>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.CNMF_E.load_cnmf_e","title":"<code>load_cnmf_e(path)</code>","text":"<p>Load the calcium transients and the spatial footprints.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/cnmfe.py</code> <pre><code>def load_cnmf_e(self, path):\n\"\"\"\n    Load the calcium transients and the spatial footprints.\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nfiles = os.listdir(path)\nmatfiles = [f for f in files if f.endswith(\".mat\")]\nif len(matfiles):\ndata = loadmat(os.path.join(path, matfiles[0]), struct_as_record=False)\nelse:\nraise RuntimeError(\"No mat file found in {}\".format(path))\nself.struct = data[\"neuron_results\"][0][0]\nC = self.struct.C.T\nself.A = self.struct.A.T\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C)\nreturn None\n</code></pre>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.CNMF_E.save_cnmfe_nwb","title":"<code>save_cnmfe_nwb(path)</code>","text":"<p>Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>TYPE</code> <p>Description</p> required Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/cnmfe.py</code> <pre><code>def save_cnmfe_nwb(self, path):\n\"\"\"\n    Save the data to NWB.\n    Since there is no one-photon field in nwb, it uses the two-photon field.\n    Parameters\n    ----------\n    path : TYPE\n        Description\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = np.atleast_2d(self.A[i])\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.CNMF_E.load_cnmfe_nwb","title":"<code>load_cnmfe_nwb(path)</code>","text":"<p>Load the calcium transient and spatial footprint from nwb</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/cnmfe.py</code> <pre><code>def load_cnmfe_nwb(self, path):\n\"\"\"\n    Load the calcium transient and spatial footprint from nwb\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.Minian","title":"<code>Minian</code>","text":"<p>         Bases: <code>BaseLoader</code></p> <p>Loader for data processed with Minian (https://github.com/denisecailab/minian). The path folder should contain a subfolder name minian.</p> <p>Attributes:</p> Name Type Description <code>A</code> <code>numpy.ndarray</code> <p>Spatial footprints</p> <code>C</code> <code>TsdFrame</code> <p>The calcium transients</p> <code>sampling_rate</code> <code>float</code> <p>Sampling rate of the data (default is 30 Hz).</p> Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/cnmfe.py</code> <pre><code>class Minian(BaseLoader):\n\"\"\"Loader for data processed with Minian (https://github.com/denisecailab/minian).\n    The path folder should contain a subfolder name minian.\n    Attributes\n    ----------\n    A : numpy.ndarray\n        Spatial footprints\n    C : TsdFrame\n        The calcium transients\n    sampling_rate : float\n        Sampling rate of the data (default is 30 Hz).\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Parameters\n        ----------\n        path : str\n            The path to the data.\n        \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_minian(path)\nself.save_cnmfe_nwb(path)\ndef load_minian(self, path):\n\"\"\"\n        Load the calcium transients and the spatial footprints.\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nminian_folder = os.path.join(path, \"minian\")\nif not os.path.exists(minian_folder):\nraise RuntimeError(\"Path {} does not contain a minian folder\".format(path))\ntry:\nimport zarr\nexcept ImportError as ie:\nprint(\"Please install module zarr for loading minian data\", ie)\nsys.exit()\ndata = zarr.open(minian_folder, \"r\")\nC = data[\"C.zarr\"][\"C\"][:]\nC = C.T\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C)\nself.A = data[\"A.zarr\"][\"A\"][:]\nreturn None\ndef save_cnmfe_nwb(self, path):\n\"\"\"\n        Save the data to NWB.\n        Since there is no one-photon field in nwb, it uses the two-photon field.\n        Parameters\n        ----------\n        path : TYPE\n            Description\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = self.A[i]\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_cnmfe_nwb(self, path):\n\"\"\"\n        Load the calcium transient and spatial footprint from nwb\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.Minian.__init__","title":"<code>__init__(path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data.</p> required Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/cnmfe.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Parameters\n    ----------\n    path : str\n        The path to the data.\n    \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_minian(path)\nself.save_cnmfe_nwb(path)\n</code></pre>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.Minian.load_minian","title":"<code>load_minian(path)</code>","text":"<p>Load the calcium transients and the spatial footprints.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/cnmfe.py</code> <pre><code>def load_minian(self, path):\n\"\"\"\n    Load the calcium transients and the spatial footprints.\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nminian_folder = os.path.join(path, \"minian\")\nif not os.path.exists(minian_folder):\nraise RuntimeError(\"Path {} does not contain a minian folder\".format(path))\ntry:\nimport zarr\nexcept ImportError as ie:\nprint(\"Please install module zarr for loading minian data\", ie)\nsys.exit()\ndata = zarr.open(minian_folder, \"r\")\nC = data[\"C.zarr\"][\"C\"][:]\nC = C.T\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C)\nself.A = data[\"A.zarr\"][\"A\"][:]\nreturn None\n</code></pre>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.Minian.save_cnmfe_nwb","title":"<code>save_cnmfe_nwb(path)</code>","text":"<p>Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>TYPE</code> <p>Description</p> required Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/cnmfe.py</code> <pre><code>def save_cnmfe_nwb(self, path):\n\"\"\"\n    Save the data to NWB.\n    Since there is no one-photon field in nwb, it uses the two-photon field.\n    Parameters\n    ----------\n    path : TYPE\n        Description\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = self.A[i]\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.Minian.load_cnmfe_nwb","title":"<code>load_cnmfe_nwb(path)</code>","text":"<p>Load the calcium transient and spatial footprint from nwb</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/cnmfe.py</code> <pre><code>def load_cnmfe_nwb(self, path):\n\"\"\"\n    Load the calcium transient and spatial footprint from nwb\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE","title":"<code>InscopixCNMFE</code>","text":"<p>         Bases: <code>BaseLoader</code></p> <p>Loader for Inscopix-cnmfe (https://github.com/inscopix/inscopix-cnmfe). The folder should contain a file ending with '_traces.csv' and a tiff file for spatial footprints.</p> <p>Attributes:</p> Name Type Description <code>A</code> <code>np.ndarray</code> <p>The spatial footprints</p> <code>C</code> <code>TsdFrame</code> <p>The calcium transients</p> <code>sampling_rate</code> <code>float</code> <p>Sampling rate of the data (default is 30 Hz).</p> Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/cnmfe.py</code> <pre><code>class InscopixCNMFE(BaseLoader):\n\"\"\"Loader for Inscopix-cnmfe (https://github.com/inscopix/inscopix-cnmfe).\n    The folder should contain a file ending with '_traces.csv'\n    and a tiff file for spatial footprints.\n    Attributes\n    ----------\n    A : np.ndarray\n        The spatial footprints\n    C : TsdFrame\n        The calcium transients\n    sampling_rate : float\n        Sampling rate of the data (default is 30 Hz).\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Parameters\n        ----------\n        path : str\n            The path to the data.\n        \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_inscopix_cnmfe(path)\nself.save_cnmfe_nwb(path)\ndef load_inscopix_cnmfe(self, path):\n\"\"\"\n        Load the calcium transients and the spatial footprints.\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nfiles = os.listdir(path)\ntracefile = [f for f in files if f.endswith(\"_traces.csv\")]\nif len(tracefile):\nC = pd.read_csv(os.path.join(path, tracefile[0]), index_col=0)\nelse:\nraise RuntimeError(\n\"Path {} does not contain the file {}\".format(path, \"*_traces.csv\")\n)\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C.values)\ntry:\nimport tifffile as tiff\nexcept ImportError as ie:\nprint(\"Please install module tifffile for loading inscopix-cnmfe data\", ie)\nsys.exit()\ntifffile = [f for f in files if f.endswith(\".tiff\")]\nif len(tifffile):\nself.A = tiff.imread(os.path.join(path, tifffile[0]))\nelse:\nraise RuntimeError(\n\"Path {} does not contain the file {}\".format(path, \"*.tiff\")\n)\nreturn None\ndef save_cnmfe_nwb(self, path):\n\"\"\"\n        Save the data to NWB.\n        Since there is no one-photon field in nwb, it uses the two-photon field.\n        Parameters\n        ----------\n        path : TYPE\n            Description\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = self.A[i]\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_cnmfe_nwb(self, path):\n\"\"\"\n        Load the calcium transient and spatial footprint from nwb\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.__init__","title":"<code>__init__(path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data.</p> required Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/cnmfe.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Parameters\n    ----------\n    path : str\n        The path to the data.\n    \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_inscopix_cnmfe(path)\nself.save_cnmfe_nwb(path)\n</code></pre>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.load_inscopix_cnmfe","title":"<code>load_inscopix_cnmfe(path)</code>","text":"<p>Load the calcium transients and the spatial footprints.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/cnmfe.py</code> <pre><code>def load_inscopix_cnmfe(self, path):\n\"\"\"\n    Load the calcium transients and the spatial footprints.\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nfiles = os.listdir(path)\ntracefile = [f for f in files if f.endswith(\"_traces.csv\")]\nif len(tracefile):\nC = pd.read_csv(os.path.join(path, tracefile[0]), index_col=0)\nelse:\nraise RuntimeError(\n\"Path {} does not contain the file {}\".format(path, \"*_traces.csv\")\n)\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C.values)\ntry:\nimport tifffile as tiff\nexcept ImportError as ie:\nprint(\"Please install module tifffile for loading inscopix-cnmfe data\", ie)\nsys.exit()\ntifffile = [f for f in files if f.endswith(\".tiff\")]\nif len(tifffile):\nself.A = tiff.imread(os.path.join(path, tifffile[0]))\nelse:\nraise RuntimeError(\n\"Path {} does not contain the file {}\".format(path, \"*.tiff\")\n)\nreturn None\n</code></pre>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.save_cnmfe_nwb","title":"<code>save_cnmfe_nwb(path)</code>","text":"<p>Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>TYPE</code> <p>Description</p> required Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/cnmfe.py</code> <pre><code>def save_cnmfe_nwb(self, path):\n\"\"\"\n    Save the data to NWB.\n    Since there is no one-photon field in nwb, it uses the two-photon field.\n    Parameters\n    ----------\n    path : TYPE\n        Description\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = self.A[i]\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.load_cnmfe_nwb","title":"<code>load_cnmfe_nwb(path)</code>","text":"<p>Load the calcium transient and spatial footprint from nwb</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/cnmfe.py</code> <pre><code>def load_cnmfe_nwb(self, path):\n\"\"\"\n    Load the calcium transient and spatial footprint from nwb\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"io.loader/","title":"Basic IO","text":"<p>BaseLoader is the general class for loading session with pynapple.</p> <p>@author: Guillaume Viejo</p>"},{"location":"io.loader/#pynapple.io.loader.BaseLoader","title":"<code>BaseLoader</code>","text":"<p>         Bases: <code>object</code></p> <p>General loader for epochs and tracking data</p> Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/loader.py</code> <pre><code>class BaseLoader(object):\n\"\"\"\n    General loader for epochs and tracking data\n    \"\"\"\ndef __init__(self, path=None):\nself.path = path\nstart_gui = True\n# Check if a pynapplenwb folder exist to bypass GUI\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nstart_gui = False\nself.load_data(path)\n# Starting the GUI\nif start_gui:\napp = App()\nwindow = BaseLoaderGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\n# Extracting all the information from gui loader\nif window.status:\nself.session_information = window.session_information\nself.subject_information = window.subject_information\nself.name = self.session_information[\"name\"]\nself.tracking_frequency = window.tracking_frequency\nself.position = self._make_position(\nwindow.tracking_parameters,\nwindow.tracking_method,\nwindow.tracking_frequency,\nwindow.epochs,\nwindow.time_units_epochs,\nwindow.tracking_alignment,\n)\nself.epochs = self._make_epochs(window.epochs, window.time_units_epochs)\nself.time_support = self._join_epochs(\nwindow.epochs, window.time_units_epochs\n)\n# Save the data\nself.create_nwb_file(path)\ndef load_default_csv(self, csv_file):\n\"\"\"\n        Load tracking data. The default csv should have the time index in the first column in seconds.\n        If no header is provided, the column names will be the column index.\n        Parameters\n        ----------\n        csv_file : str\n            path to the csv file\n        Returns\n        -------\n        pandas.DataFrame\n            _\n        \"\"\"\nposition = pd.read_csv(csv_file, header=[0], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nreturn position\ndef load_optitrack_csv(self, csv_file):\n\"\"\"\n        Load tracking data exported with Optitrack.\n        By default, the function reads rows 4 and 5 to build the column names.\n        Parameters\n        ----------\n        csv_file : str\n            path to the csv file\n        Raises\n        ------\n        RuntimeError\n            If header names are unknown. Should be 'Position' and 'Rotation'\n        Returns\n        -------\n        pandas.DataFrame\n            _\n        \"\"\"\nposition = pd.read_csv(csv_file, header=[4, 5], index_col=1)\nif 1 in position.columns:\nposition = position.drop(labels=1, axis=1)\nposition = position[~position.index.duplicated(keep=\"first\")]\norder = []\ncols = []\nfor n in position.columns:\nif n[0] == \"Rotation\":\norder.append(\"r\" + n[1].lower())\ncols.append(n)\nelif n[0] == \"Position\":\norder.append(n[1].lower())\ncols.append(n)\nif len(order) == 0:\nraise RuntimeError(\n\"Unknow tracking format for csv file {}\".format(csv_file)\n)\nposition = position[cols]\nposition.columns = order\nreturn position\ndef load_dlc_csv(self, csv_file):\n\"\"\"\n        Load tracking data exported with DeepLabCut\n        Parameters\n        ----------\n        csv_file : str\n            path to the csv file\n        Returns\n        -------\n        pandas.DataFrame\n            _\n        \"\"\"\nposition = pd.read_csv(csv_file, header=[1, 2], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nposition.columns = list(map(lambda x: \"_\".join(x), position.columns.values))\nreturn position\ndef load_ttl_pulse(\nself,\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n):\n\"\"\"\n        Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.\n        Parameters\n        ----------\n        ttl_file : str\n            File name\n        n_channels : int, optional\n            The number of channels in the binary file.\n        channel : int, optional\n            Which channel contains the TTL\n        bytes_size : int, optional\n            Bytes size of the binary file.\n        fs : float, optional\n            Sampling frequency of the binary file\n        Returns\n        -------\n        pd.Series\n            A series containing the time index of the TTL.\n        \"\"\"\nf = open(ttl_file, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\nwith open(ttl_file, \"rb\") as f:\ndata = np.fromfile(f, np.uint16).reshape((n_samples, n_channels))\nif n_channels == 1:\ndata = data.flatten().astype(np.int32)\nelse:\ndata = data[:, channel].flatten().astype(np.int32)\ndata = data / data.max()\npeaks, _ = scipy.signal.find_peaks(\nnp.diff(data), height=threshold, distance=int(fs / (tracking_frequency * 2))\n)\ntimestep = np.arange(0, len(data)) / fs\npeaks += 1\nttl = pd.Series(index=timestep[peaks], data=data[peaks])\nreturn ttl\ndef _make_position(\nself, parameters, method, frequency, epochs, time_units, alignment\n):\n\"\"\"\n        Make the position TSDFrame with the parameters extracted from the GUI.\n        \"\"\"\nif len(parameters.index) == 0:\nreturn None\nelse:\nif len(epochs) == 0:\nepochs.loc[0, \"start\"] = 0.0\nframes = []\ntime_supports_starts = []\ntime_support_ends = []\nfor i in range(len(parameters)):\nif method.lower() == \"optitrack\":\nposition = self.load_optitrack_csv(parameters.loc[i, \"csv\"])\nelif method.lower() == \"deep lab cut\":\nposition = self.load_dlc_csv(parameters.loc[i, \"csv\"])\nelif method.lower() == \"default\":\nposition = self.load_default_csv(parameters.loc[i, \"csv\"])\nif alignment.lower() == \"local\":\nstart_epoch = nap.format_timestamps(\nepochs.loc[int(parameters.loc[i, \"epoch\"]), \"start\"], time_units\n)\nend_epoch = nap.format_timestamps(\nepochs.loc[int(parameters.loc[i, \"epoch\"]), \"end\"], time_units\n)\ntimestamps = (\nposition.index.values\n+ nap.return_timestamps(start_epoch, \"s\")[0]\n)\n# Make sure timestamps are within the epochs\nidx = np.where(timestamps &lt; end_epoch)[0]\nposition = position.iloc[idx]\nposition.index = pd.Index(timestamps[idx])\nif alignment.lower() == \"ttl\":\nttl = self.load_ttl_pulse(\nttl_file=parameters.loc[i, \"ttl\"],\ntracking_frequency=frequency,\nn_channels=int(parameters.loc[i, \"n_channels\"]),\nchannel=int(parameters.loc[i, \"tracking_channel\"]),\nbytes_size=int(parameters.loc[i, \"bytes_size\"]),\nfs=float(parameters.loc[i, \"fs\"]),\nthreshold=float(parameters.loc[i, \"threshold\"]),\n)\nif len(ttl):\nlength = np.minimum(len(ttl), len(position))\nttl = ttl.iloc[0:length]\nposition = position.iloc[0:length]\nelse:\nraise RuntimeError(\n\"No ttl detected for {}\".format(parameters.loc[i, \"ttl\"])\n)\n# Make sure start epochs in seconds\n# start_epoch = format_timestamp(\n#     epochs.loc[parameters.loc[f, \"epoch\"], \"start\"], time_units\n# )\nstart_epoch = nap.format_timestamps(\nepochs.loc[int(parameters.loc[i, \"epoch\"]), \"start\"], time_units\n)\ntimestamps = start_epoch + ttl.index.values\nposition.index = pd.Index(timestamps)\nframes.append(position)\ntime_supports_starts.append(position.index[0])\ntime_support_ends.append(position.index[-1])\nposition = pd.concat(frames)\ntime_supports = nap.IntervalSet(\nstart=time_supports_starts, end=time_support_ends, time_units=\"s\"\n)\n# Specific to optitrACK\nif set([\"rx\", \"ry\", \"rz\"]).issubset(position.columns):\nposition[[\"ry\", \"rx\", \"rz\"]] *= np.pi / 180\nposition[[\"ry\", \"rx\", \"rz\"]] += 2 * np.pi\nposition[[\"ry\", \"rx\", \"rz\"]] %= 2 * np.pi\nposition = nap.TsdFrame(\nt=position.index.values,\nd=position.values,\ncolumns=position.columns.values,\ntime_support=time_supports,\ntime_units=\"s\",\n)\nreturn position\ndef _make_epochs(self, epochs, time_units=\"s\"):\n\"\"\"\n        Split GUI epochs into dict of epochs\n        \"\"\"\nlabels = epochs.groupby(\"label\").groups\nisets = {}\nfor lbs in labels.keys():\ntmp = epochs.loc[labels[lbs]]\nisets[lbs] = nap.IntervalSet(\nstart=tmp[\"start\"], end=tmp[\"end\"], time_units=time_units\n)\nreturn isets\ndef _join_epochs(self, epochs, time_units=\"s\"):\n\"\"\"\n        To create the global time support of the data\n        \"\"\"\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\nisets = nap.IntervalSet(\nstart=epochs[\"start\"].sort_values(),\nend=epochs[\"end\"].sort_values(),\ntime_units=time_units,\n)\niset = isets.merge_close_intervals(1, time_units=\"us\")\nif len(iset):\nreturn iset\nelse:\nreturn None\ndef create_nwb_file(self, path):\n\"\"\"\n        Initialize the NWB file in the folder pynapplenwb within the data folder.\n        Parameters\n        ----------\n        path : str\n            The path to save the data\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nos.makedirs(self.nwb_path)\nself.nwbfilepath = os.path.join(\nself.nwb_path, self.session_information[\"name\"] + \".nwb\"\n)\nself.subject_information[\"date_of_birth\"] = None\nnwbfile = NWBFile(\nsession_description=self.session_information[\"description\"],\nidentifier=self.session_information[\"name\"],\nsession_start_time=datetime.datetime.now(datetime.timezone.utc),\nexperimenter=self.session_information[\"experimenter\"],\nlab=self.session_information[\"lab\"],\ninstitution=self.session_information[\"institution\"],\nsubject=Subject(**self.subject_information),\n)\n# Tracking\nif self.position is not None:\ndata = self.position.as_units(\"s\")\n# specific to optitrack\nif set([\"x\", \"y\", \"z\", \"rx\", \"ry\", \"rz\"]).issubset(data.columns):\nposition = Position()\nfor c in [\"x\", \"y\", \"z\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\ndirection = CompassDirection()\nfor c in [\"rx\", \"ry\", \"rz\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"radian\",\nreference_frame=\"\",\n)\ndirection.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\nnwbfile.add_acquisition(direction)\n# Other types\nelse:\nposition = Position()\nfor c in data.columns:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\n# Adding time support of position as TimeIntervals\nepochs = self.position.time_support.as_units(\"s\")\nposition_time_support = TimeIntervals(\nname=\"position_time_support\",\ndescription=\"The time support of the position i.e the real start and end of the tracking\",\n)\nfor i in self.position.time_support.index:\nposition_time_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(position_time_support)\n# Epochs\nfor ep in self.epochs.keys():\nepochs = self.epochs[ep].as_units(\"s\")\nfor i in self.epochs[ep].index:\nnwbfile.add_epoch(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=[ep],  # This is stupid nwb who tries to parse the string\n)\nwith NWBHDF5IO(self.nwbfilepath, \"w\") as io:\nio.write(nwbfile)\nreturn\ndef load_data(self, path):\n\"\"\"\n        Load NWB data save with pynapple in the pynapplenwb folder\n        Parameters\n        ----------\n        path : str\n            Path to the session folder\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nposition = {}\nacq_keys = nwbfile.acquisition.keys()\nif \"CompassDirection\" in acq_keys:\ncompass = nwbfile.acquisition[\"CompassDirection\"]\nfor k in compass.spatial_series.keys():\nposition[k] = pd.Series(\nindex=compass.get_spatial_series(k).timestamps[:],\ndata=compass.get_spatial_series(k).data[:],\n)\nif \"Position\" in acq_keys:\ntracking = nwbfile.acquisition[\"Position\"]\nfor k in tracking.spatial_series.keys():\nposition[k] = pd.Series(\nindex=tracking.get_spatial_series(k).timestamps[:],\ndata=tracking.get_spatial_series(k).data[:],\n)\nif len(position):\nposition = pd.DataFrame.from_dict(position)\n# retrieveing time support position if in epochs\nif \"position_time_support\" in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[\"position_time_support\"].to_dataframe()\ntime_support = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nself.position = nap.TsdFrame(\nposition, time_units=\"s\", time_support=time_support\n)\nif nwbfile.epochs is not None:\nepochs = nwbfile.epochs.to_dataframe()\n# NWB is dumb and cannot take a single string for labels\nepochs[\"label\"] = [epochs.loc[i, \"tags\"][0] for i in epochs.index]\nepochs = epochs.drop(labels=\"tags\", axis=1)\nepochs = epochs.rename(columns={\"start_time\": \"start\", \"stop_time\": \"end\"})\nself.epochs = self._make_epochs(epochs)\nself.time_support = self._join_epochs(epochs, \"s\")\nio.close()\nreturn\ndef save_nwb_intervals(self, iset, name, description=\"\"):\n\"\"\"\n        Add epochs to the NWB file (e.g. ripples epochs)\n        See pynwb.epoch.TimeIntervals\n        Parameters\n        ----------\n        iset : IntervalSet\n            The intervalSet to save\n        name : str\n            The name in the nwb file\n        \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nepochs = iset.as_units(\"s\")\ntime_intervals = TimeIntervals(name=name, description=description)\nfor i in epochs.index:\ntime_intervals.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_intervals)\nio.write(nwbfile)\nio.close()\nreturn\ndef save_nwb_timeseries(self, tsd, name, description=\"\"):\n\"\"\"\n        Save timestamps in the NWB file (e.g. ripples time) with the time support.\n        See pynwb.base.TimeSeries\n        Parameters\n        ----------\n        tsd : TsdFrame\n            _\n        name : str\n            _\n        description : str, optional\n            _\n        \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nts = TimeSeries(\nname=name,\nunit=\"s\",\ndata=tsd.values,\ntimestamps=tsd.as_units(\"s\").index.values,\n)\ntime_support = TimeIntervals(\nname=name + \"_timesupport\", description=\"The time support of the object\"\n)\nepochs = tsd.time_support.as_units(\"s\")\nfor i in epochs.index:\ntime_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_support)\nnwbfile.add_acquisition(ts)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_nwb_intervals(self, name):\n\"\"\"\n        Load epochs from the NWB file (e.g. 'ripples')\n        Parameters\n        ----------\n        name : str\n            The name in the nwb file\n        \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif name in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[name].to_dataframe()\nisets = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nio.close()\nreturn isets\nelse:\nio.close()\nreturn\ndef load_nwb_timeseries(self, name):\n\"\"\"\n        Load timestamps in the NWB file (e.g. ripples time)\n        Parameters\n        ----------\n        name : str\n            _\n        Returns\n        -------\n        Tsd\n            _\n        \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nts = nwbfile.acquisition[name]\ntime_support = self.load_nwb_intervals(name + \"_timesupport\")\ntsd = nap.Tsd(\nt=ts.timestamps[:], d=ts.data[:], time_units=\"s\", time_support=time_support\n)\nio.close()\nreturn tsd\n</code></pre>"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.load_default_csv","title":"<code>load_default_csv(csv_file)</code>","text":"<p>Load tracking data. The default csv should have the time index in the first column in seconds. If no header is provided, the column names will be the column index.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>_</p> Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/loader.py</code> <pre><code>def load_default_csv(self, csv_file):\n\"\"\"\n    Load tracking data. The default csv should have the time index in the first column in seconds.\n    If no header is provided, the column names will be the column index.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[0], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nreturn position\n</code></pre>"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.load_optitrack_csv","title":"<code>load_optitrack_csv(csv_file)</code>","text":"<p>Load tracking data exported with Optitrack. By default, the function reads rows 4 and 5 to build the column names.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If header names are unknown. Should be 'Position' and 'Rotation'</p> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>_</p> Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/loader.py</code> <pre><code>def load_optitrack_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with Optitrack.\n    By default, the function reads rows 4 and 5 to build the column names.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Raises\n    ------\n    RuntimeError\n        If header names are unknown. Should be 'Position' and 'Rotation'\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[4, 5], index_col=1)\nif 1 in position.columns:\nposition = position.drop(labels=1, axis=1)\nposition = position[~position.index.duplicated(keep=\"first\")]\norder = []\ncols = []\nfor n in position.columns:\nif n[0] == \"Rotation\":\norder.append(\"r\" + n[1].lower())\ncols.append(n)\nelif n[0] == \"Position\":\norder.append(n[1].lower())\ncols.append(n)\nif len(order) == 0:\nraise RuntimeError(\n\"Unknow tracking format for csv file {}\".format(csv_file)\n)\nposition = position[cols]\nposition.columns = order\nreturn position\n</code></pre>"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.load_dlc_csv","title":"<code>load_dlc_csv(csv_file)</code>","text":"<p>Load tracking data exported with DeepLabCut</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>_</p> Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/loader.py</code> <pre><code>def load_dlc_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with DeepLabCut\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[1, 2], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nposition.columns = list(map(lambda x: \"_\".join(x), position.columns.values))\nreturn position\n</code></pre>"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.load_ttl_pulse","title":"<code>load_ttl_pulse(ttl_file, tracking_frequency, n_channels=1, channel=0, bytes_size=2, fs=20000.0, threshold=0.3)</code>","text":"<p>Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.</p> <p>Parameters:</p> Name Type Description Default <code>ttl_file</code> <code>str</code> <p>File name</p> required <code>n_channels</code> <code>int, optional</code> <p>The number of channels in the binary file.</p> <code>1</code> <code>channel</code> <code>int, optional</code> <p>Which channel contains the TTL</p> <code>0</code> <code>bytes_size</code> <code>int, optional</code> <p>Bytes size of the binary file.</p> <code>2</code> <code>fs</code> <code>float, optional</code> <p>Sampling frequency of the binary file</p> <code>20000.0</code> <p>Returns:</p> Type Description <code>pd.Series</code> <p>A series containing the time index of the TTL.</p> Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/loader.py</code> <pre><code>def load_ttl_pulse(\nself,\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n):\n\"\"\"\n    Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.\n    Parameters\n    ----------\n    ttl_file : str\n        File name\n    n_channels : int, optional\n        The number of channels in the binary file.\n    channel : int, optional\n        Which channel contains the TTL\n    bytes_size : int, optional\n        Bytes size of the binary file.\n    fs : float, optional\n        Sampling frequency of the binary file\n    Returns\n    -------\n    pd.Series\n        A series containing the time index of the TTL.\n    \"\"\"\nf = open(ttl_file, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\nwith open(ttl_file, \"rb\") as f:\ndata = np.fromfile(f, np.uint16).reshape((n_samples, n_channels))\nif n_channels == 1:\ndata = data.flatten().astype(np.int32)\nelse:\ndata = data[:, channel].flatten().astype(np.int32)\ndata = data / data.max()\npeaks, _ = scipy.signal.find_peaks(\nnp.diff(data), height=threshold, distance=int(fs / (tracking_frequency * 2))\n)\ntimestep = np.arange(0, len(data)) / fs\npeaks += 1\nttl = pd.Series(index=timestep[peaks], data=data[peaks])\nreturn ttl\n</code></pre>"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.create_nwb_file","title":"<code>create_nwb_file(path)</code>","text":"<p>Initialize the NWB file in the folder pynapplenwb within the data folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the data</p> required Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/loader.py</code> <pre><code>def create_nwb_file(self, path):\n\"\"\"\n    Initialize the NWB file in the folder pynapplenwb within the data folder.\n    Parameters\n    ----------\n    path : str\n        The path to save the data\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nos.makedirs(self.nwb_path)\nself.nwbfilepath = os.path.join(\nself.nwb_path, self.session_information[\"name\"] + \".nwb\"\n)\nself.subject_information[\"date_of_birth\"] = None\nnwbfile = NWBFile(\nsession_description=self.session_information[\"description\"],\nidentifier=self.session_information[\"name\"],\nsession_start_time=datetime.datetime.now(datetime.timezone.utc),\nexperimenter=self.session_information[\"experimenter\"],\nlab=self.session_information[\"lab\"],\ninstitution=self.session_information[\"institution\"],\nsubject=Subject(**self.subject_information),\n)\n# Tracking\nif self.position is not None:\ndata = self.position.as_units(\"s\")\n# specific to optitrack\nif set([\"x\", \"y\", \"z\", \"rx\", \"ry\", \"rz\"]).issubset(data.columns):\nposition = Position()\nfor c in [\"x\", \"y\", \"z\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\ndirection = CompassDirection()\nfor c in [\"rx\", \"ry\", \"rz\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"radian\",\nreference_frame=\"\",\n)\ndirection.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\nnwbfile.add_acquisition(direction)\n# Other types\nelse:\nposition = Position()\nfor c in data.columns:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\n# Adding time support of position as TimeIntervals\nepochs = self.position.time_support.as_units(\"s\")\nposition_time_support = TimeIntervals(\nname=\"position_time_support\",\ndescription=\"The time support of the position i.e the real start and end of the tracking\",\n)\nfor i in self.position.time_support.index:\nposition_time_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(position_time_support)\n# Epochs\nfor ep in self.epochs.keys():\nepochs = self.epochs[ep].as_units(\"s\")\nfor i in self.epochs[ep].index:\nnwbfile.add_epoch(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=[ep],  # This is stupid nwb who tries to parse the string\n)\nwith NWBHDF5IO(self.nwbfilepath, \"w\") as io:\nio.write(nwbfile)\nreturn\n</code></pre>"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.load_data","title":"<code>load_data(path)</code>","text":"<p>Load NWB data save with pynapple in the pynapplenwb folder</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session folder</p> required Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/loader.py</code> <pre><code>def load_data(self, path):\n\"\"\"\n    Load NWB data save with pynapple in the pynapplenwb folder\n    Parameters\n    ----------\n    path : str\n        Path to the session folder\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nposition = {}\nacq_keys = nwbfile.acquisition.keys()\nif \"CompassDirection\" in acq_keys:\ncompass = nwbfile.acquisition[\"CompassDirection\"]\nfor k in compass.spatial_series.keys():\nposition[k] = pd.Series(\nindex=compass.get_spatial_series(k).timestamps[:],\ndata=compass.get_spatial_series(k).data[:],\n)\nif \"Position\" in acq_keys:\ntracking = nwbfile.acquisition[\"Position\"]\nfor k in tracking.spatial_series.keys():\nposition[k] = pd.Series(\nindex=tracking.get_spatial_series(k).timestamps[:],\ndata=tracking.get_spatial_series(k).data[:],\n)\nif len(position):\nposition = pd.DataFrame.from_dict(position)\n# retrieveing time support position if in epochs\nif \"position_time_support\" in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[\"position_time_support\"].to_dataframe()\ntime_support = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nself.position = nap.TsdFrame(\nposition, time_units=\"s\", time_support=time_support\n)\nif nwbfile.epochs is not None:\nepochs = nwbfile.epochs.to_dataframe()\n# NWB is dumb and cannot take a single string for labels\nepochs[\"label\"] = [epochs.loc[i, \"tags\"][0] for i in epochs.index]\nepochs = epochs.drop(labels=\"tags\", axis=1)\nepochs = epochs.rename(columns={\"start_time\": \"start\", \"stop_time\": \"end\"})\nself.epochs = self._make_epochs(epochs)\nself.time_support = self._join_epochs(epochs, \"s\")\nio.close()\nreturn\n</code></pre>"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.save_nwb_intervals","title":"<code>save_nwb_intervals(iset, name, description='')</code>","text":"<p>Add epochs to the NWB file (e.g. ripples epochs) See pynwb.epoch.TimeIntervals</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>The intervalSet to save</p> required <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/loader.py</code> <pre><code>def save_nwb_intervals(self, iset, name, description=\"\"):\n\"\"\"\n    Add epochs to the NWB file (e.g. ripples epochs)\n    See pynwb.epoch.TimeIntervals\n    Parameters\n    ----------\n    iset : IntervalSet\n        The intervalSet to save\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nepochs = iset.as_units(\"s\")\ntime_intervals = TimeIntervals(name=name, description=description)\nfor i in epochs.index:\ntime_intervals.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_intervals)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.save_nwb_timeseries","title":"<code>save_nwb_timeseries(tsd, name, description='')</code>","text":"<p>Save timestamps in the NWB file (e.g. ripples time) with the time support. See pynwb.base.TimeSeries</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>TsdFrame</code> <p>_</p> required <code>name</code> <code>str</code> <p>_</p> required <code>description</code> <code>str, optional</code> <p>_</p> <code>''</code> Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/loader.py</code> <pre><code>def save_nwb_timeseries(self, tsd, name, description=\"\"):\n\"\"\"\n    Save timestamps in the NWB file (e.g. ripples time) with the time support.\n    See pynwb.base.TimeSeries\n    Parameters\n    ----------\n    tsd : TsdFrame\n        _\n    name : str\n        _\n    description : str, optional\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nts = TimeSeries(\nname=name,\nunit=\"s\",\ndata=tsd.values,\ntimestamps=tsd.as_units(\"s\").index.values,\n)\ntime_support = TimeIntervals(\nname=name + \"_timesupport\", description=\"The time support of the object\"\n)\nepochs = tsd.time_support.as_units(\"s\")\nfor i in epochs.index:\ntime_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_support)\nnwbfile.add_acquisition(ts)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.load_nwb_intervals","title":"<code>load_nwb_intervals(name)</code>","text":"<p>Load epochs from the NWB file (e.g. 'ripples')</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/loader.py</code> <pre><code>def load_nwb_intervals(self, name):\n\"\"\"\n    Load epochs from the NWB file (e.g. 'ripples')\n    Parameters\n    ----------\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif name in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[name].to_dataframe()\nisets = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nio.close()\nreturn isets\nelse:\nio.close()\nreturn\n</code></pre>"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.load_nwb_timeseries","title":"<code>load_nwb_timeseries(name)</code>","text":"<p>Load timestamps in the NWB file (e.g. ripples time)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>_</p> required <p>Returns:</p> Type Description <code>Tsd</code> <p>_</p> Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/loader.py</code> <pre><code>def load_nwb_timeseries(self, name):\n\"\"\"\n    Load timestamps in the NWB file (e.g. ripples time)\n    Parameters\n    ----------\n    name : str\n        _\n    Returns\n    -------\n    Tsd\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nts = nwbfile.acquisition[name]\ntime_support = self.load_nwb_intervals(name + \"_timesupport\")\ntsd = nap.Tsd(\nt=ts.timestamps[:], d=ts.data[:], time_units=\"s\", time_support=time_support\n)\nio.close()\nreturn tsd\n</code></pre>"},{"location":"io/","title":"Miscellaneous","text":""},{"location":"io/#pynapple.io.misc","title":"<code>pynapple.io.misc</code>","text":"<p>Various io functions</p> <p>@author: Guillaume Viejo</p>"},{"location":"io/#pynapple.io.misc-classes","title":"Classes","text":""},{"location":"io/#pynapple.io.misc-functions","title":"Functions","text":""},{"location":"io/#pynapple.io.misc.load_session","title":"<code>load_session(path=None, session_type=None)</code>","text":"<p>General Loader for</p> <ul> <li> <p>Neurosuite</p> </li> <li> <p>Phy</p> </li> <li> <p>Minian</p> </li> <li> <p>Inscopix-cnmfe</p> </li> <li> <p>Matlab-cnmfe</p> </li> <li> <p>Suite2p</p> </li> <li>None for default session.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str, optional</code> <p>The path to load the data</p> <code>None</code> <code>session_type</code> <code>str, optional</code> <p>Can be 'neurosuite', 'phy', 'minian', 'inscopix-cnmfe', 'cnmfe-matlab', 'suite2p' or None for default loader.</p> <code>None</code> <p>Returns:</p> Type Description <code>Session</code> <p>A class holding all the data from the session.</p> Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/misc.py</code> <pre><code>def load_session(path=None, session_type=None):\n\"\"\"\n    General Loader for\n    - Neurosuite\\n\n    - Phy\\n\n    - Minian\\n\n    - Inscopix-cnmfe\\n\n    - Matlab-cnmfe\\n\n    - Suite2p\n    - None for default session.\n    Parameters\n    ----------\n    path : str, optional\n        The path to load the data\n    session_type : str, optional\n        Can be 'neurosuite', 'phy',\n        'minian', 'inscopix-cnmfe', 'cnmfe-matlab',\n        'suite2p' or None for default loader.\n    Returns\n    -------\n    Session\n        A class holding all the data from the session.\n    \"\"\"\nif path:\nif not os.path.isdir(path):\nraise RuntimeError(\"Path {} is not found.\".format(path))\nif isinstance(session_type, str):\nsession_type = session_type.lower()\nif session_type == \"neurosuite\":\nreturn NeuroSuite(path)\nelif session_type == \"phy\":\nreturn Phy(path)\nelif session_type == \"inscopix-cnmfe\":\nreturn InscopixCNMFE(path)\nelif session_type == \"minian\":\nreturn Minian(path)\nelif session_type == \"cnmfe-matlab\":\nreturn CNMF_E(path)\nelif session_type == \"suite2p\":\nreturn Suite2P(path)\nelse:\nreturn BaseLoader(path)\n</code></pre>"},{"location":"io/#pynapple.io.misc.load_eeg","title":"<code>load_eeg(filepath, channel=None, n_channels=None, frequency=None, precision='int16', bytes_size=2)</code>","text":"<p>Standalone function to load eeg/lfp/dat file in binary format.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path to the eeg file</p> required <code>channel</code> <code>int or list of int, optional</code> <p>The channel(s) to load. If None return a memory map of the dat file to avoid memory error</p> <code>None</code> <code>n_channels</code> <code>int, optional</code> <p>Number of channels</p> <code>None</code> <code>frequency</code> <code>float, optional</code> <p>Sampling rate of the file</p> <code>None</code> <code>precision</code> <code>str, optional</code> <p>The precision of the binary file</p> <code>'int16'</code> <code>bytes_size</code> <code>int, optional</code> <p>Bytes size of the binary file</p> <code>2</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If can't find the lfp/eeg/dat file</p> <p>Returns:</p> Type Description <code>Tsd or TsdFrame</code> <p>The lfp in a time series format</p>"},{"location":"io/#pynapple.io.misc.load_eeg--deleted-parameters","title":"Deleted Parameters","text":"<p>extension : str, optional     The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match</p> Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/misc.py</code> <pre><code>def load_eeg(\nfilepath,\nchannel=None,\nn_channels=None,\nfrequency=None,\nprecision=\"int16\",\nbytes_size=2,\n):\n\"\"\"\n    Standalone function to load eeg/lfp/dat file in binary format.\n    Parameters\n    ----------\n    filepath : str\n        The path to the eeg file\n    channel : int or list of int, optional\n        The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n    n_channels : int, optional\n        Number of channels\n    frequency : float, optional\n        Sampling rate of the file\n    precision : str, optional\n        The precision of the binary file\n    bytes_size : int, optional\n        Bytes size of the binary file\n    Raises\n    ------\n    RuntimeError\n        If can't find the lfp/eeg/dat file\n    Returns\n    -------\n    Tsd or TsdFrame\n        The lfp in a time series format\n    Deleted Parameters\n    ------------------\n    extension : str, optional\n        The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n    \"\"\"\n# Need to check if a xml file exists\npath = os.path.dirname(filepath)\nbasename = os.path.basename(filepath).split(\".\")[0]\nlistdir = os.listdir(path)\nif frequency is None or n_channels is None:\nif basename + \".xml\" in listdir:\nxmlpath = os.path.join(path, basename + \".xml\")\nxmldoc = minidom.parse(xmlpath)\nelse:\nraise RuntimeError(\n\"Can't find xml file; please specify sampling frequency or number of channels\"\n)\nif frequency is None:\nif filepath.endswith(\".dat\"):\nfs_dat = int(\nxmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"samplingRate\")[0]\n.firstChild.data\n)\nfrequency = fs_dat\nelif filepath.endswith((\".lfp\", \".eeg\")):\nfs_eeg = int(\nxmldoc.getElementsByTagName(\"fieldPotentials\")[0]\n.getElementsByTagName(\"lfpSamplingRate\")[0]\n.firstChild.data\n)\nfrequency = fs_eeg\nif n_channels is None:\nn_channels = int(\nxmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"nChannels\")[0]\n.firstChild.data\n)\nf = open(filepath, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nduration = n_samples / frequency\nf.close()\nfp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\ntimestep = np.arange(0, n_samples) / frequency\ntime_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\nif channel is None:\nreturn fp\nelif type(channel) is int:\nreturn nap.Tsd(\nt=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n)\nelif type(channel) is list:\nreturn nap.TsdFrame(\nt=timestep,\nd=fp[:, channel],\ntime_units=\"s\",\ntime_support=time_support,\ncolumns=channel,\n)\n</code></pre>"},{"location":"io/#pynapple.io.misc.append_NWB_LFP","title":"<code>append_NWB_LFP(path, lfp, channel=None)</code>","text":"<p>Standalone function for adding lfp/eeg to already existing nwb files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data. The function will looks for a nwb file in path or in path/pynapplenwb.</p> required <code>lfp</code> <code>Tsd or TsdFrame</code> <p>Description</p> required <code>channel</code> <code>None, optional</code> <p>channel number in int ff lfp is a Tsd</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If can't find the nwb file </p> <p>If no channel is specify when passing a Tsd</p> Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/misc.py</code> <pre><code>def append_NWB_LFP(path, lfp, channel=None):\n\"\"\"Standalone function for adding lfp/eeg to already existing nwb files.\n    Parameters\n    ----------\n    path : str\n        The path to the data. The function will looks for a nwb file in path\n        or in path/pynapplenwb.\n    lfp : Tsd or TsdFrame\n        Description\n    channel : None, optional\n        channel number in int ff lfp is a Tsd\n    Raises\n    ------\n    RuntimeError\n        If can't find the nwb file \\n\n        If no channel is specify when passing a Tsd\n    \"\"\"\nnew_path = os.path.join(path, \"pynapplenwb\")\nnwb_path = \"\"\nif os.path.exists(new_path):\nnwbfilename = [f for f in os.listdir(new_path) if f.endswith(\".nwb\")]\nif len(nwbfilename):\nnwb_path = os.path.join(path, \"pynapplenwb\", nwbfilename[0])\nelse:\nnwbfilename = [f for f in os.listdir(path) if f.endswith(\".nwb\")]\nif len(nwbfilename):\nnwb_path = os.path.join(path, \"pynapplenwb\", nwbfilename[0])\nif len(nwb_path) == 0:\nraise RuntimeError(\"Can't find nwb file in {}\".format(path))\nif isinstance(lfp, nap.TsdFrame):\nchannels = lfp.columns.values\nelif isinstance(lfp, nap.Tsd):\nif isinstance(channel, int):\nchannels = [channel]\nelse:\nraise RuntimeError(\"Please specify which channel it is.\")\nio = NWBHDF5IO(nwb_path, \"r+\")\nnwbfile = io.read()\nall_table_region = nwbfile.create_electrode_table_region(\nregion=channels, description=\"\", name=\"electrodes\"\n)\nlfp_electrical_series = ElectricalSeries(\nname=\"ElectricalSeries\",\ndata=lfp.values,\ntimestamps=lfp.index.values,\nelectrodes=all_table_region,\n)\nlfp = LFP(electrical_series=lfp_electrical_series)\necephys_module = nwbfile.create_processing_module(\nname=\"ecephys\", description=\"processed extracellular electrophysiology data\"\n)\necephys_module.add(lfp)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"io.neurosuite/","title":"Neurosuite","text":"<p>Class and functions for loading data processed with the Neurosuite (Klusters, Neuroscope, NDmanager)</p> <p>@author: Guillaume Viejo</p>"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite","title":"<code>NeuroSuite</code>","text":"<p>         Bases: <code>BaseLoader</code></p> <p>Loader for kluster data</p> Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/neurosuite.py</code> <pre><code>class NeuroSuite(BaseLoader):\n\"\"\"\n    Loader for kluster data\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Instantiate the data class from a neurosuite folder.\n        Parameters\n        ----------\n        path : str\n            The path to the data.\n        \"\"\"\nself.basename = os.path.basename(path)\nself.time_support = None\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_neurosuite = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_nwb_spikes(path)\nif success:\nloading_neurosuite = False\n# Bypass if data have already been transfered to nwb\nif loading_neurosuite:\nself.load_neurosuite_xml(path)\n# print(\"XML loaded\")\n# To label the electrodes groups\napp = App()\nwindow = EphysGUI(app, path=path, groups=self.group_to_channel)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\n# print(\"GUI DONE\")\nif window.status:\nself.ephys_information = window.ephys_information\nself.load_neurosuite_spikes(path, self.basename, self.time_support)\nself.save_data(path)\ndef load_neurosuite_spikes(self, path, basename, time_support=None, fs=20000.0):\n\"\"\"\n        Read the clus and res files and convert to NWB.\n        Instantiate automatically a TsGroup object.\n        Parameters\n        ----------\n        path : str\n            The path to the data\n        basename : str\n            Basename of the clu and res files.\n        time_support : IntevalSet, optional\n            The time support of the data\n        fs : float, optional\n            Sampling rate of the recording.\n        Raises\n        ------\n        RuntimeError\n            If number of clu and res are not equal.\n        \"\"\"\nfiles = os.listdir(path)\nclu_files = np.sort([f for f in files if \".clu.\" in f and f[0] != \".\"])\nres_files = np.sort([f for f in files if \".res.\" in f and f[0] != \".\"])\nclu1 = np.sort([int(f.split(\".\")[-1]) for f in clu_files])\nclu2 = np.sort([int(f.split(\".\")[-1]) for f in res_files])\nif len(clu_files) != len(res_files) or not (clu1 == clu2).any():\nraise RuntimeError(\n\"Not the same number of clu and res files in \" + path + \"; Exiting ...\"\n)\ncount = 0\nspikes = {}\ngroup = pd.Series(dtype=np.int32)\nfor i, s in zip(range(len(clu_files)), clu1):\nclu = np.genfromtxt(\nos.path.join(path, basename + \".clu.\" + str(s)), dtype=np.int32\n)[1:]\nif np.max(clu) &gt; 1:  # getting rid of mua and noise\nres = np.genfromtxt(os.path.join(path, basename + \".res.\" + str(s)))\ntmp = np.unique(clu).astype(int)\nidx_clu = tmp[tmp &gt; 1]\nidx_out = np.arange(count, count + len(idx_clu))\nfor j, k in zip(idx_clu, idx_out):\nt = res[clu == j] / fs\nspikes[k] = nap.Ts(t=t, time_units=\"s\")\ngroup.loc[k] = s\ncount += len(idx_clu)\ngroup = group - 1  # better to start it a 0\nself.spikes = nap.TsGroup(\nspikes, time_support=time_support, time_units=\"s\", group=group\n)\n# adding some information to help parse the neurons\nnames = pd.Series(\nindex=group.index,\ndata=[self.ephys_information[group.loc[i]][\"name\"] for i in group.index],\n)\nif ~np.all(names.values == \"\"):\nself.spikes.set_info(name=names)\nlocations = pd.Series(\nindex=group.index,\ndata=[\nself.ephys_information[group.loc[i]][\"location\"] for i in group.index\n],\n)\nif ~np.all(locations.values == \"\"):\nself.spikes.set_info(location=locations)\nreturn\ndef load_neurosuite_xml(self, path):\n\"\"\"\n        path should be the folder session containing the XML file\n        Function reads :\n        1. the number of channels\n        2. the sampling frequency of the dat file or the eeg file depending of what is present in the folder\n            eeg file first if both are present or both are absent\n        3. the mappings shanks to channels as a dict\n        Parameters\n        ----------\n        path: str\n            The path to the data\n        Raises\n        ------\n        RuntimeError\n            If path does not contain the xml file.\n        \"\"\"\nlistdir = os.listdir(path)\nxmlfiles = [f for f in listdir if f.endswith(\".xml\")]\nif not len(xmlfiles):\nraise RuntimeError(\"Path {} contains no xml files;\".format(path))\nsys.exit()\nnew_path = os.path.join(path, xmlfiles[0])\nself.xmldoc = minidom.parse(new_path)\nself.nChannels = int(\nself.xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"nChannels\")[0]\n.firstChild.data\n)\nself.fs_dat = int(\nself.xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"samplingRate\")[0]\n.firstChild.data\n)\nself.fs_eeg = int(\nself.xmldoc.getElementsByTagName(\"fieldPotentials\")[0]\n.getElementsByTagName(\"lfpSamplingRate\")[0]\n.firstChild.data\n)\nself.group_to_channel = {}\ngroups = (\nself.xmldoc.getElementsByTagName(\"anatomicalDescription\")[0]\n.getElementsByTagName(\"channelGroups\")[0]\n.getElementsByTagName(\"group\")\n)\nfor i in range(len(groups)):\nself.group_to_channel[i] = np.array(\n[\nint(child.firstChild.data)\nfor child in groups[i].getElementsByTagName(\"channel\")\n]\n)\nreturn\ndef save_data(self, path):\n\"\"\"\n        Save the data to NWB format.\n        Parameters\n        ----------\n        path : str\n            The path to save the data\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nelectrode_groups = {}\nfor g in self.group_to_channel:\ndevice = nwbfile.create_device(\nname=self.ephys_information[g][\"device\"][\"name\"] + \"-\" + str(g),\ndescription=self.ephys_information[g][\"device\"][\"description\"],\nmanufacturer=self.ephys_information[g][\"device\"][\"manufacturer\"],\n)\nif (\nlen(self.ephys_information[g][\"position\"])\nand type(self.ephys_information[g][\"position\"]) is str\n):\nself.ephys_information[g][\"position\"] = re.split(\n\";|,| \", self.ephys_information[g][\"position\"]\n)\nelif self.ephys_information[g][\"position\"] == \"\":\nself.ephys_information[g][\"position\"] = None\nelectrode_groups[g] = nwbfile.create_electrode_group(\nname=\"group\" + str(g) + \"_\" + self.ephys_information[g][\"name\"],\ndescription=self.ephys_information[g][\"description\"],\nposition=self.ephys_information[g][\"position\"],\nlocation=self.ephys_information[g][\"location\"],\ndevice=device,\n)\nfor idx in self.group_to_channel[g]:\nnwbfile.add_electrode(\nid=idx,\nx=0.0,\ny=0.0,\nz=0.0,\nimp=0.0,\nlocation=self.ephys_information[g][\"location\"],\nfiltering=\"none\",\ngroup=electrode_groups[g],\n)\n# Adding units\nnwbfile.add_unit_column(\"location\", \"the anatomical location of this unit\")\nnwbfile.add_unit_column(\"group\", \"the group of the unit\")\nfor u in self.spikes.keys():\nnwbfile.add_unit(\nid=u,\nspike_times=self.spikes[u].as_units(\"s\").index.values,\nelectrode_group=electrode_groups[self.spikes.get_info(\"group\").loc[u]],\nlocation=self.ephys_information[self.spikes.get_info(\"group\").loc[u]][\n\"location\"\n],\ngroup=self.spikes.get_info(\"group\").loc[u],\n)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_nwb_spikes(self, path):\n\"\"\"\n        Read the NWB spikes to extract the spike times.\n        Parameters\n        ----------\n        path : str\n            The path to the data\n        Returns\n        -------\n        TYPE\n            Description\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif nwbfile.units is None:\nio.close()\nreturn False\nelse:\nunits = nwbfile.units.to_dataframe()\nspikes = {\nn: nap.Ts(t=units.loc[n, \"spike_times\"], time_units=\"s\")\nfor n in units.index\n}\nself.spikes = nap.TsGroup(\nspikes,\ntime_support=self.time_support,\ntime_units=\"s\",\ngroup=units[\"group\"],\n)\nif ~np.all(units[\"location\"] == \"\"):\nself.spikes.set_info(location=units[\"location\"])\nio.close()\nreturn True\ndef load_lfp(\nself,\nfilename=None,\nchannel=None,\nextension=\".eeg\",\nfrequency=1250.0,\nprecision=\"int16\",\nbytes_size=2,\n):\n\"\"\"\n        Load the LFP.\n        Parameters\n        ----------\n        filename : str, optional\n            The filename of the lfp file.\n            It can be useful it multiple dat files are present in the data directory\n        channel : int or list of int, optional\n            The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n        extension : str, optional\n            The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n        frequency : float, optional\n            Default 1250 Hz for the eeg file\n        precision : str, optional\n            The precision of the binary file\n        bytes_size : int, optional\n            Bytes size of the lfp file\n        Raises\n        ------\n        RuntimeError\n            If can't find the lfp/eeg/dat file\n        Returns\n        -------\n        Tsd or TsdFrame\n            The lfp in a time series format\n        \"\"\"\nif filename is not None:\nfilepath = os.path.join(self.path, filename)\nelse:\nlistdir = os.listdir(self.path)\neegfile = [f for f in listdir if f.endswith(extension)]\nif not len(eegfile):\nraise RuntimeError(\n\"Path {} contains no {} files;\".format(self.path, extension)\n)\nfilepath = os.path.join(self.path, eegfile[0])\nself.load_neurosuite_xml(self.path)\nn_channels = int(self.nChannels)\nf = open(filepath, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nduration = n_samples / frequency\nf.close()\nfp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\ntimestep = np.arange(0, n_samples) / frequency\ntime_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\nif channel is None:\nreturn nap.TsdFrame(\nt=timestep, d=fp, time_units=\"s\", time_support=time_support\n)\nelif type(channel) is int:\nreturn nap.Tsd(\nt=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n)\nelif type(channel) is list:\nreturn nap.TsdFrame(\nt=timestep,\nd=fp[:, channel],\ntime_units=\"s\",\ntime_support=time_support,\ncolumns=channel,\n)\ndef read_neuroscope_intervals(self, name=None, path2file=None):\n\"\"\"\n        This function reads .evt files in which odd raws indicate the beginning\n        of the time series and the even raws are the ends.\n        If the file is present in the nwb, provide the just the name. If the file\n        is not present in the nwb, it loads the events from the nwb directory.\n        If just the path is provided but not the name, it takes the name from the file.\n        Parameters\n        ----------\n        name: str\n            name of the epoch in the nwb file, e.g. \"rem\" or desired name save\n            the data in the nwb.\n        path2file: str\n            Path of the file you want to load.\n        Returns\n        -------\n        IntervalSet\n            Contains two columns corresponding to the start and end of the intervals.\n        \"\"\"\nif name:\nisets = self.load_nwb_intervals(name)\nif isinstance(isets, nap.IntervalSet):\nreturn isets\nif name is not None and path2file is None:\npath2file = os.path.join(self.path, self.basename + \".\" + name + \".evt\")\nif path2file is not None:\ntry:\n# df = pd.read_csv(path2file, delimiter=' ', usecols = [0], header = None)\ntmp = np.genfromtxt(path2file)[:, 0]\ndf = tmp.reshape(len(tmp) // 2, 2)\nexcept ValueError:\nprint(\"specify a valid name\")\nisets = nap.IntervalSet(df[:, 0], df[:, 1], time_units=\"ms\")\nif name is None:\nname = path2file.split(\".\")[-2]\nprint(\"*** saving file in the nwb as\", name)\nself.save_nwb_intervals(isets, name)\nelse:\nraise ValueError(\"specify a valid path\")\nreturn isets\ndef write_neuroscope_intervals(self, extension, isets, name):\n\"\"\"Write events to load with neuroscope (e.g. ripples start and ends)\n        Parameters\n        ----------\n        extension : str\n            The extension of the file (e.g. basename.evt.py.rip)\n        isets : IntervalSet\n            The IntervalSet to write\n        name : str\n            The name of the events (e.g. Ripples)\n        \"\"\"\nstart = isets.as_units(\"ms\")[\"start\"].values\nends = isets.as_units(\"ms\")[\"end\"].values\ndatatowrite = np.vstack((start, ends)).T.flatten()\nn = len(isets)\ntexttowrite = np.vstack(\n(\n(np.repeat(np.array([name + \" start\"]), n)),\n(np.repeat(np.array([name + \" end\"]), n)),\n)\n).T.flatten()\nevt_file = os.path.join(self.path, self.basename + extension)\nf = open(evt_file, \"w\")\nfor t, n in zip(datatowrite, texttowrite):\nf.writelines(\"{:1.6f}\".format(t) + \"\\t\" + n + \"\\n\")\nf.close()\nreturn\ndef load_mean_waveforms(self, epoch=None, waveform_window=None, spike_count=1000):\n\"\"\"\n        Load the mean waveforms from a dat file.\n        Parameters\n        ----------\n        epoch : IntervalSet\n            default = None\n            Restrict spikes to an epoch.\n        waveform_window : IntervalSet\n            default interval nap.IntervalSet(start = -0.0005, end = 0.001, time_units = 'ms')\n            Limit waveform extraction before and after spike time\n        spike_count : int\n            default = 1000\n            Number of spikes used per neuron for the calculation of waveforms\n        Returns\n        -------\n        dictionary\n            the waveforms for all neurons\n        pandas.Series\n            the channel with the maximum waveform for each neuron\n        \"\"\"\nif not isinstance(waveform_window, nap.IntervalSet):\nwaveform_window = nap.IntervalSet(start=-0.5, end=1, time_units=\"ms\")\nspikes = self.spikes\nif not os.path.exists(self.path):  # check if path exists\nprint(\"The path \" + self.path + \" doesn't exist; Exiting ...\")\nsys.exit()\n# Load XML INFO\nself.load_neurosuite_xml(self.path)\nn_channels = self.nChannels\nfs = self.fs_dat\ngroup_to_channel = self.group_to_channel\ngroup = spikes.get_info(\"group\")\n# Check if there is an epoch, restrict spike times to epoch\nif epoch is not None:\nif type(epoch) is not nap.IntervalSet:\nprint(\"Epoch must be an IntervalSet\")\nsys.exit()\nelse:\nprint(\"Restricting spikes to epoch\")\nspikes = spikes.restrict(epoch)\nepstart = int(epoch.as_units(\"s\")[\"start\"].values[0] * fs)\nepend = int(epoch.as_units(\"s\")[\"end\"].values[0] * fs)\n# Find dat file\nfiles = os.listdir(self.path)\ndat_files = np.sort([f for f in files if \"dat\" in f and f[0] != \".\"])\n# Need n_samples collected in the entire recording from dat file to load\nfile = os.path.join(self.path, dat_files[0])\nf = open(\nfile, \"rb\"\n)  # open file to get number of samples collected in the entire recording\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\n# map to memory all samples for all channels, channels are numbered according to neuroscope number\nfp = np.memmap(file, np.int16, \"r\", shape=(n_samples, n_channels))\n# convert spike times to spikes in sample number\nsample_spikes = {\nneuron: (spikes[neuron].as_units(\"s\").index.values * fs).astype(\"int\")\nfor neuron in spikes\n}\n# prep for waveforms\noverlap = int(\nwaveform_window.tot_length(time_units=\"s\")\n)  # one spike's worth of overlap between windows\nwaveform_window = abs(np.array(waveform_window.as_units(\"s\"))[0] * fs).astype(\nint\n)  # convert time to sample number\nneuron_waveforms = {\nn: np.zeros([np.sum(waveform_window), len(group_to_channel[group[n]])])\nfor n in sample_spikes\n}\n# divide dat file into batches that slightly overlap for faster loading\nbatch_size = 3000000\nwindows = np.arange(0, int(endoffile / n_channels / bytes_size), batch_size)\nif epoch is not None:\nprint(\"Restricting dat file to epoch\")\nwindows = windows[(windows &gt;= epstart) &amp; (windows &lt;= epend)]\nbatches = []\nfor (\ni\n) in windows:  # make overlapping batches from the beginning to end of recording\nif i == windows[-1]:  # the last batch cannot overlap with the next one\nbatches.append([i, n_samples])\nelse:\nbatches.append([i, i + batch_size + overlap])\nbatches = [np.int32(batch) for batch in batches]\nsample_counted_spikes = {}\nfor index, neuron in enumerate(sample_spikes):\nif len(sample_spikes[neuron]) &gt;= spike_count:\nsample_counted_spikes[neuron] = np.array(\nnp.random.choice(list(sample_spikes[neuron]), spike_count)\n)\nelif len(sample_spikes[neuron]) &lt; spike_count:\nprint(\n\"Not enough spikes in neuron \" + str(index) + \"... using all spikes\"\n)\nsample_counted_spikes[neuron] = sample_spikes[neuron]\n# Make one array containing all selected spike times of all neurons - will be used to check for spikes before loading dat file\nspike_check = np.array(\n[\nint(spikes_neuron)\nfor spikes_neuron in sample_counted_spikes[neuron]\nfor neuron in sample_counted_spikes\n]\n)\nfor index, timestep in enumerate(batches):\nprint(\nf\"Extracting waveforms from dat file: window {index+1} / {len(windows)}\",\nend=\"\\r\",\n)\nif (\nlen(\nspike_check[\n(timestep[0] &lt; spike_check) &amp; (timestep[1] &gt; spike_check)\n]\n)\n== 0\n):\ncontinue  # if there are no spikes for any neurons in this batch, skip and go to the next one\n# Load dat file for timestep\ntmp = pd.DataFrame(\ndata=fp[timestep[0] : timestep[1], :],\ncolumns=np.arange(n_channels),\nindex=range(timestep[0], timestep[1]),\n)  # load dat file\n# Check if any spikes are present\nfor neuron in sample_counted_spikes:\nneurontmp = sample_counted_spikes[neuron]\ntmp2 = neurontmp[(timestep[0] &lt; neurontmp) &amp; (timestep[1] &gt; neurontmp)]\nif len(neurontmp) == 0:\ncontinue  # skip neuron if it has no spikes in this batch\ntmpn = tmp[\ngroup_to_channel[group[neuron]]\n]  # restrict dat file to the channel group of the neuron\nfor time in tmp2:  # add each spike waveform to neuron_waveform\nspikewindow = tmpn.loc[\ntime - waveform_window[0] : time + waveform_window[1] - 1\n]  # waveform for this spike time\ntry:\nneuron_waveforms[neuron] += spikewindow.values\nexcept (\nException\n):  # ignore if full waveform is not present in this batch\npass\nmeanwf = {\nn: pd.DataFrame(\ndata=np.array(neuron_waveforms[n]) / spike_count,\ncolumns=np.arange(len(group_to_channel[group[n]])),\nindex=np.array(np.arange(-waveform_window[0], waveform_window[1])) / fs,\n)\nfor n in sample_counted_spikes\n}\n# find the max channel for each neuron\nmaxch = pd.Series(\ndata=[meanwf[n][meanwf[n].loc[0].idxmin()].name for n in meanwf],\nindex=spikes.keys(),\n)\nreturn meanwf, maxch\n</code></pre>"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.__init__","title":"<code>__init__(path)</code>","text":"<p>Instantiate the data class from a neurosuite folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data.</p> required Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/neurosuite.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Instantiate the data class from a neurosuite folder.\n    Parameters\n    ----------\n    path : str\n        The path to the data.\n    \"\"\"\nself.basename = os.path.basename(path)\nself.time_support = None\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_neurosuite = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_nwb_spikes(path)\nif success:\nloading_neurosuite = False\n# Bypass if data have already been transfered to nwb\nif loading_neurosuite:\nself.load_neurosuite_xml(path)\n# print(\"XML loaded\")\n# To label the electrodes groups\napp = App()\nwindow = EphysGUI(app, path=path, groups=self.group_to_channel)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\n# print(\"GUI DONE\")\nif window.status:\nself.ephys_information = window.ephys_information\nself.load_neurosuite_spikes(path, self.basename, self.time_support)\nself.save_data(path)\n</code></pre>"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_neurosuite_spikes","title":"<code>load_neurosuite_spikes(path, basename, time_support=None, fs=20000.0)</code>","text":"<p>Read the clus and res files and convert to NWB. Instantiate automatically a TsGroup object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data</p> required <code>basename</code> <code>str</code> <p>Basename of the clu and res files.</p> required <code>time_support</code> <code>IntevalSet, optional</code> <p>The time support of the data</p> <code>None</code> <code>fs</code> <code>float, optional</code> <p>Sampling rate of the recording.</p> <code>20000.0</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If number of clu and res are not equal.</p> Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/neurosuite.py</code> <pre><code>def load_neurosuite_spikes(self, path, basename, time_support=None, fs=20000.0):\n\"\"\"\n    Read the clus and res files and convert to NWB.\n    Instantiate automatically a TsGroup object.\n    Parameters\n    ----------\n    path : str\n        The path to the data\n    basename : str\n        Basename of the clu and res files.\n    time_support : IntevalSet, optional\n        The time support of the data\n    fs : float, optional\n        Sampling rate of the recording.\n    Raises\n    ------\n    RuntimeError\n        If number of clu and res are not equal.\n    \"\"\"\nfiles = os.listdir(path)\nclu_files = np.sort([f for f in files if \".clu.\" in f and f[0] != \".\"])\nres_files = np.sort([f for f in files if \".res.\" in f and f[0] != \".\"])\nclu1 = np.sort([int(f.split(\".\")[-1]) for f in clu_files])\nclu2 = np.sort([int(f.split(\".\")[-1]) for f in res_files])\nif len(clu_files) != len(res_files) or not (clu1 == clu2).any():\nraise RuntimeError(\n\"Not the same number of clu and res files in \" + path + \"; Exiting ...\"\n)\ncount = 0\nspikes = {}\ngroup = pd.Series(dtype=np.int32)\nfor i, s in zip(range(len(clu_files)), clu1):\nclu = np.genfromtxt(\nos.path.join(path, basename + \".clu.\" + str(s)), dtype=np.int32\n)[1:]\nif np.max(clu) &gt; 1:  # getting rid of mua and noise\nres = np.genfromtxt(os.path.join(path, basename + \".res.\" + str(s)))\ntmp = np.unique(clu).astype(int)\nidx_clu = tmp[tmp &gt; 1]\nidx_out = np.arange(count, count + len(idx_clu))\nfor j, k in zip(idx_clu, idx_out):\nt = res[clu == j] / fs\nspikes[k] = nap.Ts(t=t, time_units=\"s\")\ngroup.loc[k] = s\ncount += len(idx_clu)\ngroup = group - 1  # better to start it a 0\nself.spikes = nap.TsGroup(\nspikes, time_support=time_support, time_units=\"s\", group=group\n)\n# adding some information to help parse the neurons\nnames = pd.Series(\nindex=group.index,\ndata=[self.ephys_information[group.loc[i]][\"name\"] for i in group.index],\n)\nif ~np.all(names.values == \"\"):\nself.spikes.set_info(name=names)\nlocations = pd.Series(\nindex=group.index,\ndata=[\nself.ephys_information[group.loc[i]][\"location\"] for i in group.index\n],\n)\nif ~np.all(locations.values == \"\"):\nself.spikes.set_info(location=locations)\nreturn\n</code></pre>"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_neurosuite_xml","title":"<code>load_neurosuite_xml(path)</code>","text":"<p>path should be the folder session containing the XML file</p> <p>Function reads : 1. the number of channels 2. the sampling frequency of the dat file or the eeg file depending of what is present in the folder     eeg file first if both are present or both are absent 3. the mappings shanks to channels as a dict</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>The path to the data</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If path does not contain the xml file.</p> Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/neurosuite.py</code> <pre><code>def load_neurosuite_xml(self, path):\n\"\"\"\n    path should be the folder session containing the XML file\n    Function reads :\n    1. the number of channels\n    2. the sampling frequency of the dat file or the eeg file depending of what is present in the folder\n        eeg file first if both are present or both are absent\n    3. the mappings shanks to channels as a dict\n    Parameters\n    ----------\n    path: str\n        The path to the data\n    Raises\n    ------\n    RuntimeError\n        If path does not contain the xml file.\n    \"\"\"\nlistdir = os.listdir(path)\nxmlfiles = [f for f in listdir if f.endswith(\".xml\")]\nif not len(xmlfiles):\nraise RuntimeError(\"Path {} contains no xml files;\".format(path))\nsys.exit()\nnew_path = os.path.join(path, xmlfiles[0])\nself.xmldoc = minidom.parse(new_path)\nself.nChannels = int(\nself.xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"nChannels\")[0]\n.firstChild.data\n)\nself.fs_dat = int(\nself.xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"samplingRate\")[0]\n.firstChild.data\n)\nself.fs_eeg = int(\nself.xmldoc.getElementsByTagName(\"fieldPotentials\")[0]\n.getElementsByTagName(\"lfpSamplingRate\")[0]\n.firstChild.data\n)\nself.group_to_channel = {}\ngroups = (\nself.xmldoc.getElementsByTagName(\"anatomicalDescription\")[0]\n.getElementsByTagName(\"channelGroups\")[0]\n.getElementsByTagName(\"group\")\n)\nfor i in range(len(groups)):\nself.group_to_channel[i] = np.array(\n[\nint(child.firstChild.data)\nfor child in groups[i].getElementsByTagName(\"channel\")\n]\n)\nreturn\n</code></pre>"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.save_data","title":"<code>save_data(path)</code>","text":"<p>Save the data to NWB format.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the data</p> required Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/neurosuite.py</code> <pre><code>def save_data(self, path):\n\"\"\"\n    Save the data to NWB format.\n    Parameters\n    ----------\n    path : str\n        The path to save the data\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nelectrode_groups = {}\nfor g in self.group_to_channel:\ndevice = nwbfile.create_device(\nname=self.ephys_information[g][\"device\"][\"name\"] + \"-\" + str(g),\ndescription=self.ephys_information[g][\"device\"][\"description\"],\nmanufacturer=self.ephys_information[g][\"device\"][\"manufacturer\"],\n)\nif (\nlen(self.ephys_information[g][\"position\"])\nand type(self.ephys_information[g][\"position\"]) is str\n):\nself.ephys_information[g][\"position\"] = re.split(\n\";|,| \", self.ephys_information[g][\"position\"]\n)\nelif self.ephys_information[g][\"position\"] == \"\":\nself.ephys_information[g][\"position\"] = None\nelectrode_groups[g] = nwbfile.create_electrode_group(\nname=\"group\" + str(g) + \"_\" + self.ephys_information[g][\"name\"],\ndescription=self.ephys_information[g][\"description\"],\nposition=self.ephys_information[g][\"position\"],\nlocation=self.ephys_information[g][\"location\"],\ndevice=device,\n)\nfor idx in self.group_to_channel[g]:\nnwbfile.add_electrode(\nid=idx,\nx=0.0,\ny=0.0,\nz=0.0,\nimp=0.0,\nlocation=self.ephys_information[g][\"location\"],\nfiltering=\"none\",\ngroup=electrode_groups[g],\n)\n# Adding units\nnwbfile.add_unit_column(\"location\", \"the anatomical location of this unit\")\nnwbfile.add_unit_column(\"group\", \"the group of the unit\")\nfor u in self.spikes.keys():\nnwbfile.add_unit(\nid=u,\nspike_times=self.spikes[u].as_units(\"s\").index.values,\nelectrode_group=electrode_groups[self.spikes.get_info(\"group\").loc[u]],\nlocation=self.ephys_information[self.spikes.get_info(\"group\").loc[u]][\n\"location\"\n],\ngroup=self.spikes.get_info(\"group\").loc[u],\n)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_nwb_spikes","title":"<code>load_nwb_spikes(path)</code>","text":"<p>Read the NWB spikes to extract the spike times.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data</p> required <p>Returns:</p> Type Description <code>TYPE</code> <p>Description</p> Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/neurosuite.py</code> <pre><code>def load_nwb_spikes(self, path):\n\"\"\"\n    Read the NWB spikes to extract the spike times.\n    Parameters\n    ----------\n    path : str\n        The path to the data\n    Returns\n    -------\n    TYPE\n        Description\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif nwbfile.units is None:\nio.close()\nreturn False\nelse:\nunits = nwbfile.units.to_dataframe()\nspikes = {\nn: nap.Ts(t=units.loc[n, \"spike_times\"], time_units=\"s\")\nfor n in units.index\n}\nself.spikes = nap.TsGroup(\nspikes,\ntime_support=self.time_support,\ntime_units=\"s\",\ngroup=units[\"group\"],\n)\nif ~np.all(units[\"location\"] == \"\"):\nself.spikes.set_info(location=units[\"location\"])\nio.close()\nreturn True\n</code></pre>"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_lfp","title":"<code>load_lfp(filename=None, channel=None, extension='.eeg', frequency=1250.0, precision='int16', bytes_size=2)</code>","text":"<p>Load the LFP.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str, optional</code> <p>The filename of the lfp file. It can be useful it multiple dat files are present in the data directory</p> <code>None</code> <code>channel</code> <code>int or list of int, optional</code> <p>The channel(s) to load. If None return a memory map of the dat file to avoid memory error</p> <code>None</code> <code>extension</code> <code>str, optional</code> <p>The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match</p> <code>'.eeg'</code> <code>frequency</code> <code>float, optional</code> <p>Default 1250 Hz for the eeg file</p> <code>1250.0</code> <code>precision</code> <code>str, optional</code> <p>The precision of the binary file</p> <code>'int16'</code> <code>bytes_size</code> <code>int, optional</code> <p>Bytes size of the lfp file</p> <code>2</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If can't find the lfp/eeg/dat file</p> <p>Returns:</p> Type Description <code>Tsd or TsdFrame</code> <p>The lfp in a time series format</p> Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/neurosuite.py</code> <pre><code>def load_lfp(\nself,\nfilename=None,\nchannel=None,\nextension=\".eeg\",\nfrequency=1250.0,\nprecision=\"int16\",\nbytes_size=2,\n):\n\"\"\"\n    Load the LFP.\n    Parameters\n    ----------\n    filename : str, optional\n        The filename of the lfp file.\n        It can be useful it multiple dat files are present in the data directory\n    channel : int or list of int, optional\n        The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n    extension : str, optional\n        The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n    frequency : float, optional\n        Default 1250 Hz for the eeg file\n    precision : str, optional\n        The precision of the binary file\n    bytes_size : int, optional\n        Bytes size of the lfp file\n    Raises\n    ------\n    RuntimeError\n        If can't find the lfp/eeg/dat file\n    Returns\n    -------\n    Tsd or TsdFrame\n        The lfp in a time series format\n    \"\"\"\nif filename is not None:\nfilepath = os.path.join(self.path, filename)\nelse:\nlistdir = os.listdir(self.path)\neegfile = [f for f in listdir if f.endswith(extension)]\nif not len(eegfile):\nraise RuntimeError(\n\"Path {} contains no {} files;\".format(self.path, extension)\n)\nfilepath = os.path.join(self.path, eegfile[0])\nself.load_neurosuite_xml(self.path)\nn_channels = int(self.nChannels)\nf = open(filepath, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nduration = n_samples / frequency\nf.close()\nfp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\ntimestep = np.arange(0, n_samples) / frequency\ntime_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\nif channel is None:\nreturn nap.TsdFrame(\nt=timestep, d=fp, time_units=\"s\", time_support=time_support\n)\nelif type(channel) is int:\nreturn nap.Tsd(\nt=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n)\nelif type(channel) is list:\nreturn nap.TsdFrame(\nt=timestep,\nd=fp[:, channel],\ntime_units=\"s\",\ntime_support=time_support,\ncolumns=channel,\n)\n</code></pre>"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.read_neuroscope_intervals","title":"<code>read_neuroscope_intervals(name=None, path2file=None)</code>","text":"<p>This function reads .evt files in which odd raws indicate the beginning of the time series and the even raws are the ends. If the file is present in the nwb, provide the just the name. If the file is not present in the nwb, it loads the events from the nwb directory. If just the path is provided but not the name, it takes the name from the file.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>name of the epoch in the nwb file, e.g. \"rem\" or desired name save the data in the nwb.</p> <code>None</code> <p>path2file: str     Path of the file you want to load.</p> <p>Returns:</p> Type Description <code>IntervalSet</code> <p>Contains two columns corresponding to the start and end of the intervals.</p> Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/neurosuite.py</code> <pre><code>def read_neuroscope_intervals(self, name=None, path2file=None):\n\"\"\"\n    This function reads .evt files in which odd raws indicate the beginning\n    of the time series and the even raws are the ends.\n    If the file is present in the nwb, provide the just the name. If the file\n    is not present in the nwb, it loads the events from the nwb directory.\n    If just the path is provided but not the name, it takes the name from the file.\n    Parameters\n    ----------\n    name: str\n        name of the epoch in the nwb file, e.g. \"rem\" or desired name save\n        the data in the nwb.\n    path2file: str\n        Path of the file you want to load.\n    Returns\n    -------\n    IntervalSet\n        Contains two columns corresponding to the start and end of the intervals.\n    \"\"\"\nif name:\nisets = self.load_nwb_intervals(name)\nif isinstance(isets, nap.IntervalSet):\nreturn isets\nif name is not None and path2file is None:\npath2file = os.path.join(self.path, self.basename + \".\" + name + \".evt\")\nif path2file is not None:\ntry:\n# df = pd.read_csv(path2file, delimiter=' ', usecols = [0], header = None)\ntmp = np.genfromtxt(path2file)[:, 0]\ndf = tmp.reshape(len(tmp) // 2, 2)\nexcept ValueError:\nprint(\"specify a valid name\")\nisets = nap.IntervalSet(df[:, 0], df[:, 1], time_units=\"ms\")\nif name is None:\nname = path2file.split(\".\")[-2]\nprint(\"*** saving file in the nwb as\", name)\nself.save_nwb_intervals(isets, name)\nelse:\nraise ValueError(\"specify a valid path\")\nreturn isets\n</code></pre>"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.write_neuroscope_intervals","title":"<code>write_neuroscope_intervals(extension, isets, name)</code>","text":"<p>Write events to load with neuroscope (e.g. ripples start and ends)</p> <p>Parameters:</p> Name Type Description Default <code>extension</code> <code>str</code> <p>The extension of the file (e.g. basename.evt.py.rip)</p> required <code>isets</code> <code>IntervalSet</code> <p>The IntervalSet to write</p> required <code>name</code> <code>str</code> <p>The name of the events (e.g. Ripples)</p> required Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/neurosuite.py</code> <pre><code>def write_neuroscope_intervals(self, extension, isets, name):\n\"\"\"Write events to load with neuroscope (e.g. ripples start and ends)\n    Parameters\n    ----------\n    extension : str\n        The extension of the file (e.g. basename.evt.py.rip)\n    isets : IntervalSet\n        The IntervalSet to write\n    name : str\n        The name of the events (e.g. Ripples)\n    \"\"\"\nstart = isets.as_units(\"ms\")[\"start\"].values\nends = isets.as_units(\"ms\")[\"end\"].values\ndatatowrite = np.vstack((start, ends)).T.flatten()\nn = len(isets)\ntexttowrite = np.vstack(\n(\n(np.repeat(np.array([name + \" start\"]), n)),\n(np.repeat(np.array([name + \" end\"]), n)),\n)\n).T.flatten()\nevt_file = os.path.join(self.path, self.basename + extension)\nf = open(evt_file, \"w\")\nfor t, n in zip(datatowrite, texttowrite):\nf.writelines(\"{:1.6f}\".format(t) + \"\\t\" + n + \"\\n\")\nf.close()\nreturn\n</code></pre>"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_mean_waveforms","title":"<code>load_mean_waveforms(epoch=None, waveform_window=None, spike_count=1000)</code>","text":"<p>Load the mean waveforms from a dat file.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>IntervalSet</code> <p>default = None Restrict spikes to an epoch.</p> <code>None</code> <code>waveform_window</code> <code>IntervalSet</code> <p>default interval nap.IntervalSet(start = -0.0005, end = 0.001, time_units = 'ms') Limit waveform extraction before and after spike time</p> <code>None</code> <code>spike_count</code> <code>int</code> <p>default = 1000 Number of spikes used per neuron for the calculation of waveforms</p> <code>1000</code> <p>Returns:</p> Type Description <code>dictionary</code> <p>the waveforms for all neurons</p> <code>pandas.Series</code> <p>the channel with the maximum waveform for each neuron</p> Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/neurosuite.py</code> <pre><code>def load_mean_waveforms(self, epoch=None, waveform_window=None, spike_count=1000):\n\"\"\"\n    Load the mean waveforms from a dat file.\n    Parameters\n    ----------\n    epoch : IntervalSet\n        default = None\n        Restrict spikes to an epoch.\n    waveform_window : IntervalSet\n        default interval nap.IntervalSet(start = -0.0005, end = 0.001, time_units = 'ms')\n        Limit waveform extraction before and after spike time\n    spike_count : int\n        default = 1000\n        Number of spikes used per neuron for the calculation of waveforms\n    Returns\n    -------\n    dictionary\n        the waveforms for all neurons\n    pandas.Series\n        the channel with the maximum waveform for each neuron\n    \"\"\"\nif not isinstance(waveform_window, nap.IntervalSet):\nwaveform_window = nap.IntervalSet(start=-0.5, end=1, time_units=\"ms\")\nspikes = self.spikes\nif not os.path.exists(self.path):  # check if path exists\nprint(\"The path \" + self.path + \" doesn't exist; Exiting ...\")\nsys.exit()\n# Load XML INFO\nself.load_neurosuite_xml(self.path)\nn_channels = self.nChannels\nfs = self.fs_dat\ngroup_to_channel = self.group_to_channel\ngroup = spikes.get_info(\"group\")\n# Check if there is an epoch, restrict spike times to epoch\nif epoch is not None:\nif type(epoch) is not nap.IntervalSet:\nprint(\"Epoch must be an IntervalSet\")\nsys.exit()\nelse:\nprint(\"Restricting spikes to epoch\")\nspikes = spikes.restrict(epoch)\nepstart = int(epoch.as_units(\"s\")[\"start\"].values[0] * fs)\nepend = int(epoch.as_units(\"s\")[\"end\"].values[0] * fs)\n# Find dat file\nfiles = os.listdir(self.path)\ndat_files = np.sort([f for f in files if \"dat\" in f and f[0] != \".\"])\n# Need n_samples collected in the entire recording from dat file to load\nfile = os.path.join(self.path, dat_files[0])\nf = open(\nfile, \"rb\"\n)  # open file to get number of samples collected in the entire recording\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\n# map to memory all samples for all channels, channels are numbered according to neuroscope number\nfp = np.memmap(file, np.int16, \"r\", shape=(n_samples, n_channels))\n# convert spike times to spikes in sample number\nsample_spikes = {\nneuron: (spikes[neuron].as_units(\"s\").index.values * fs).astype(\"int\")\nfor neuron in spikes\n}\n# prep for waveforms\noverlap = int(\nwaveform_window.tot_length(time_units=\"s\")\n)  # one spike's worth of overlap between windows\nwaveform_window = abs(np.array(waveform_window.as_units(\"s\"))[0] * fs).astype(\nint\n)  # convert time to sample number\nneuron_waveforms = {\nn: np.zeros([np.sum(waveform_window), len(group_to_channel[group[n]])])\nfor n in sample_spikes\n}\n# divide dat file into batches that slightly overlap for faster loading\nbatch_size = 3000000\nwindows = np.arange(0, int(endoffile / n_channels / bytes_size), batch_size)\nif epoch is not None:\nprint(\"Restricting dat file to epoch\")\nwindows = windows[(windows &gt;= epstart) &amp; (windows &lt;= epend)]\nbatches = []\nfor (\ni\n) in windows:  # make overlapping batches from the beginning to end of recording\nif i == windows[-1]:  # the last batch cannot overlap with the next one\nbatches.append([i, n_samples])\nelse:\nbatches.append([i, i + batch_size + overlap])\nbatches = [np.int32(batch) for batch in batches]\nsample_counted_spikes = {}\nfor index, neuron in enumerate(sample_spikes):\nif len(sample_spikes[neuron]) &gt;= spike_count:\nsample_counted_spikes[neuron] = np.array(\nnp.random.choice(list(sample_spikes[neuron]), spike_count)\n)\nelif len(sample_spikes[neuron]) &lt; spike_count:\nprint(\n\"Not enough spikes in neuron \" + str(index) + \"... using all spikes\"\n)\nsample_counted_spikes[neuron] = sample_spikes[neuron]\n# Make one array containing all selected spike times of all neurons - will be used to check for spikes before loading dat file\nspike_check = np.array(\n[\nint(spikes_neuron)\nfor spikes_neuron in sample_counted_spikes[neuron]\nfor neuron in sample_counted_spikes\n]\n)\nfor index, timestep in enumerate(batches):\nprint(\nf\"Extracting waveforms from dat file: window {index+1} / {len(windows)}\",\nend=\"\\r\",\n)\nif (\nlen(\nspike_check[\n(timestep[0] &lt; spike_check) &amp; (timestep[1] &gt; spike_check)\n]\n)\n== 0\n):\ncontinue  # if there are no spikes for any neurons in this batch, skip and go to the next one\n# Load dat file for timestep\ntmp = pd.DataFrame(\ndata=fp[timestep[0] : timestep[1], :],\ncolumns=np.arange(n_channels),\nindex=range(timestep[0], timestep[1]),\n)  # load dat file\n# Check if any spikes are present\nfor neuron in sample_counted_spikes:\nneurontmp = sample_counted_spikes[neuron]\ntmp2 = neurontmp[(timestep[0] &lt; neurontmp) &amp; (timestep[1] &gt; neurontmp)]\nif len(neurontmp) == 0:\ncontinue  # skip neuron if it has no spikes in this batch\ntmpn = tmp[\ngroup_to_channel[group[neuron]]\n]  # restrict dat file to the channel group of the neuron\nfor time in tmp2:  # add each spike waveform to neuron_waveform\nspikewindow = tmpn.loc[\ntime - waveform_window[0] : time + waveform_window[1] - 1\n]  # waveform for this spike time\ntry:\nneuron_waveforms[neuron] += spikewindow.values\nexcept (\nException\n):  # ignore if full waveform is not present in this batch\npass\nmeanwf = {\nn: pd.DataFrame(\ndata=np.array(neuron_waveforms[n]) / spike_count,\ncolumns=np.arange(len(group_to_channel[group[n]])),\nindex=np.array(np.arange(-waveform_window[0], waveform_window[1])) / fs,\n)\nfor n in sample_counted_spikes\n}\n# find the max channel for each neuron\nmaxch = pd.Series(\ndata=[meanwf[n][meanwf[n].loc[0].idxmin()].name for n in meanwf],\nindex=spikes.keys(),\n)\nreturn meanwf, maxch\n</code></pre>"},{"location":"io.phy/","title":"Phy","text":"<p>Class and functions for loading data processed with Phy2</p> <p>@author: Sara Mahallati, Guillaume Viejo</p>"},{"location":"io.phy/#pynapple.io.phy.Phy","title":"<code>Phy</code>","text":"<p>         Bases: <code>BaseLoader</code></p> <p>Loader for Phy data</p> Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/phy.py</code> <pre><code>class Phy(BaseLoader):\n\"\"\"\n    Loader for Phy data\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Instantiate the data class from a Phy folder.\n        Parameters\n        ----------\n        path : str or Path object\n            The path to the data.\n        \"\"\"\nself.time_support = None\nself.sample_rate = None\nself.n_channels_dat = None\nself.channel_map = None\nself.ch_to_sh = None\nself.spikes = None\nself.channel_positions = None\nsuper().__init__(path)\n# This path stuff should happen only once in the parent class\nself.path = Path(path)\nself.basename = self.path.name\nself.nwb_path = self.path / \"pynapplenwb\"\n# from what I can see in the loading function, only one nwb file per folder:\ntry:\nself.nwb_file = list(self.nwb_path.glob(\"*.nwb\"))[0]\nexcept IndexError:\nself.nwb_file = None\n# Need to check if nwb file exists and if data are there\n# if self.path is not None:  -&gt; are there any cases where this is None?\nif self.nwb_file is not None:\nloaded_spikes = self.load_nwb_spikes()\nif loaded_spikes is not None:\nreturn\n# Bypass if data have already been transferred to nwb\nself.load_phy_params()\napp = App()\nwindow = EphysGUI(app, path=path, groups=self.channel_map)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ephys_information = window.ephys_information\nself.load_phy_spikes(self.time_support)\nself.save_data()\napp.quit()\ndef load_phy_params(self):\n\"\"\"\n        path should be the folder session containing the params.py file\n        Function reads :\n        1. the number of channels\n        2. the sampling frequency of the dat file\n        Raises\n        ------\n        AssertionError\n            If path does not contain the params file or channel_map.npy\n        \"\"\"\nassert (\nself.path / \"params.py\"\n).exists(), f\"Can't find params.py in {self.path}\"\n# It is strongly recommended not to conflate parameters and code! Also, there's a library called params.\n# I would recommend putting in the folder a file called params.json, or .txt, or .yml, but not .py!\n# In this way we just read the file, and we don't have to add to sys to import...\n# TODO maybe remove this\nsys.path.append(str(self.path))\nimport params as params\nself.sample_rate = params.sample_rate\nself.n_channels_dat = params.n_channels_dat\nassert (\nself.path / \"channel_map.npy\"\n).exists(), f\"Can't find channel_map.npy in {self.path}\"\nchannel_map = np.load(self.path / \"channel_map.npy\")\nif (self.path / \"channel_shanks.npy\").exists():\nchannel_shank = np.load(self.path / \"channel_shanks.npy\")\nn_shanks = len(np.unique(channel_shank))\nself.channel_map = {\ni: channel_map[channel_shank == i] for i in range(n_shanks)\n}\nself.ch_to_sh = pd.Series(\nindex=channel_map.flatten(),\ndata=channel_shank.flatten(),\n)\nelse:\nself.channel_map = {i: channel_map[i] for i in range(len(channel_map))}\nself.ch_to_sh = pd.Series(\nindex=channel_map.flatten(),\ndata=np.hstack(\n[\nnp.ones(len(channel_map[i]), dtype=int) * i\nfor i in range(len(channel_map))\n]\n),\n)\nreturn\ndef load_phy_spikes(self, time_support=None):\n\"\"\"\n        Load Phy spike times and convert to NWB.\n        Instantiate automatically a TsGroup object.\n        The cluster group is taken first from cluster_info.tsv and second from cluster_group.tsv\n        Parameters\n        ----------\n        path : Path object\n            The path to the data\n        time_support : IntevalSet, optional\n            The time support of the data\n        Raises\n        ------\n        RuntimeError\n            If files are missing.\n            The function needs :\n            - cluster_info.tsv or cluster_group.tsv\n            - spike_times.npy\n            - spike_clusters.npy\n            - channel_positions.npy\n            - templates.npy\n        \"\"\"\n# Check if cluster_info.tsv or cluster_group.tsv exists. If both exist, cluster_info.tsv is used:\nhas_cluster_info = False\nif (self.path / \"cluster_info.tsv\").exists():\ncluster_info_file = self.path / \"cluster_info.tsv\"\nhas_cluster_info = True\nelif (self.path / \"cluster_group.tsv\").exists():\ncluster_info_file = self.path / \"cluster_group.tsv\"\nelse:\nraise RuntimeError(\n\"Can't find cluster_info.tsv or cluster_group.tsv in {};\".format(\nself.path\n)\n)\ncluster_info = pd.read_csv(cluster_info_file, sep=\"\\t\", index_col=\"cluster_id\")\n# In my processed data with KiloSort 3.0, the column is named KSLabel\nif \"group\" in cluster_info.columns:\ncluster_id_good = cluster_info[cluster_info.group == \"good\"].index.values\nelif \"KSLabel\" in cluster_info.columns:\ncluster_id_good = cluster_info[cluster_info.KSLabel == \"good\"].index.values\nelse:\nraise RuntimeError(\n\"Can't find column group or KSLabel in {};\".format(cluster_info_file)\n)\nspike_times = np.load(self.path / \"spike_times.npy\")\nspike_clusters = np.load(self.path / \"spike_clusters.npy\")\nspikes = {}\nfor n in cluster_id_good:\nspikes[n] = nap.Ts(\nt=spike_times[spike_clusters == n] / self.sample_rate,\ntime_support=time_support,\n)\nself.spikes = nap.TsGroup(spikes, time_support=time_support)\n# Adding the position of the electrodes in case\nself.channel_positions = np.load(self.path / \"channel_positions.npy\")\n# Adding shank group info from cluster_info if present\nif has_cluster_info:\ngroup = cluster_info.loc[cluster_id_good, \"sh\"]\nself.spikes.set_info(group=group)\nelse:\ntemplate = np.load(self.path / \"templates.npy\")\ntemplate = template[cluster_id_good]\nch = np.power(template, 2).max(1).argmax(1)\ngroup = pd.Series(index=cluster_id_good, data=self.ch_to_sh[ch].values)\nself.spikes.set_info(group=group)\nnames = pd.Series(\nindex=group.index,\ndata=[self.ephys_information[group.loc[i]][\"name\"] for i in group.index],\n)\nif ~np.all(names.values == \"\"):\nself.spikes.set_info(name=names)\nlocations = pd.Series(\nindex=group.index,\ndata=[\nself.ephys_information[group.loc[i]][\"location\"] for i in group.index\n],\n)\nif ~np.all(locations.values == \"\"):\nself.spikes.set_info(location=locations)\nreturn\ndef save_data(self):\n\"\"\"Save the data to NWB format.\"\"\"\nio = NWBHDF5IO(self.nwb_file, \"r+\")\nnwbfile = io.read()\nelectrode_groups = {}\nfor g in self.channel_map:\ndevice = nwbfile.create_device(\nname=self.ephys_information[g][\"device\"][\"name\"] + \"-\" + str(g),\ndescription=self.ephys_information[g][\"device\"][\"description\"],\nmanufacturer=self.ephys_information[g][\"device\"][\"manufacturer\"],\n)\nif (\nlen(self.ephys_information[g][\"position\"])\nand type(self.ephys_information[g][\"position\"]) is str\n):\nself.ephys_information[g][\"position\"] = re.split(\n\";|,| \", self.ephys_information[g][\"position\"]\n)\nelif self.ephys_information[g][\"position\"] == \"\":\nself.ephys_information[g][\"position\"] = None\nelectrode_groups[g] = nwbfile.create_electrode_group(\nname=\"group\" + str(g) + \"_\" + self.ephys_information[g][\"name\"],\ndescription=self.ephys_information[g][\"description\"],\nposition=self.ephys_information[g][\"position\"],\nlocation=self.ephys_information[g][\"location\"],\ndevice=device,\n)\nfor idx in self.channel_map[g]:\nnwbfile.add_electrode(\nid=idx,\nx=0.0,\ny=0.0,\nz=0.0,\nimp=0.0,\nlocation=self.ephys_information[g][\"location\"],\nfiltering=\"none\",\ngroup=electrode_groups[g],\n)\n# Adding units\nnwbfile.add_unit_column(\"location\", \"the anatomical location of this unit\")\nnwbfile.add_unit_column(\"group\", \"the group of the unit\")\nfor u in self.spikes.keys():\nnwbfile.add_unit(\nid=u,\nspike_times=self.spikes[u].as_units(\"s\").index.values,\nelectrode_group=electrode_groups[self.spikes.get_info(\"group\").loc[u]],\nlocation=self.ephys_information[self.spikes.get_info(\"group\").loc[u]][\n\"location\"\n],\ngroup=self.spikes.get_info(\"group\").loc[u],\n)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_nwb_spikes(self):\n\"\"\"Read the NWB spikes to extract the spike times.\n        Returns\n        -------\n        TYPE\n            Description\n        \"\"\"\nio = NWBHDF5IO(self.nwb_file, \"r\")\nnwbfile = io.read()\nif nwbfile.units is None:\nio.close()\nreturn None\nelse:\nunits = nwbfile.units.to_dataframe()\nspikes = {\nn: nap.Ts(t=units.loc[n, \"spike_times\"], time_units=\"s\")\nfor n in units.index\n}\nself.spikes = nap.TsGroup(\nspikes,\ntime_support=self.time_support,\ntime_units=\"s\",\ngroup=units[\"group\"],\n)\nif ~np.all(units[\"location\"] == \"\"):\nself.spikes.set_info(location=units[\"location\"])\nio.close()\nreturn True\ndef load_lfp(\nself,\nfilename=None,\nchannel=None,\nextension=\".eeg\",\nfrequency=1250.0,\nprecision=\"int16\",\nbytes_size=2,\n):\n\"\"\"\n        Load the LFP.\n        Parameters\n        ----------\n        filename : str, optional\n            The filename of the lfp file.\n            It can be useful it multiple dat files are present in the data directory\n        channel : int or list of int, optional\n            The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n        extension : str, optional\n            The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n        frequency : float, optional\n            Default 1250 Hz for the eeg file\n        precision : str, optional\n            The precision of the binary file\n        bytes_size : int, optional\n            Bytes size of the lfp file\n        Raises\n        ------\n        RuntimeError\n            If can't find the lfp/eeg/dat file\n        Returns\n        -------\n        Tsd or TsdFrame\n            The lfp in a time series format\n        \"\"\"\nif filename is not None:\nfilepath = self.path / filename\nelse:\ntry:\nfilepath = list(self.path.glob(f\"*{extension}\"))[0]\nexcept IndexError:\nraise RuntimeError(f\"Path {self.path} contains no {extension} files;\")\n# is it possible that this is a leftover from neurosuite data?\n# This is not implemented for this class.\nself.load_neurosuite_xml(self.path)\nn_channels = int(self.nChannels)\nf = open(filepath, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nduration = n_samples / frequency\nf.close()\nfp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\ntimestep = np.arange(0, n_samples) / frequency\ntime_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\nif channel is None:\nreturn nap.TsdFrame(\nt=timestep, d=fp, time_units=\"s\", time_support=time_support\n)\nelif type(channel) is int:\nreturn nap.Tsd(\nt=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n)\nelif type(channel) is list:\nreturn nap.TsdFrame(\nt=timestep,\nd=fp[:, channel],\ntime_units=\"s\",\ntime_support=time_support,\ncolumns=channel,\n)\n</code></pre>"},{"location":"io.phy/#pynapple.io.phy.Phy.__init__","title":"<code>__init__(path)</code>","text":"<p>Instantiate the data class from a Phy folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path object</code> <p>The path to the data.</p> required Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/phy.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Instantiate the data class from a Phy folder.\n    Parameters\n    ----------\n    path : str or Path object\n        The path to the data.\n    \"\"\"\nself.time_support = None\nself.sample_rate = None\nself.n_channels_dat = None\nself.channel_map = None\nself.ch_to_sh = None\nself.spikes = None\nself.channel_positions = None\nsuper().__init__(path)\n# This path stuff should happen only once in the parent class\nself.path = Path(path)\nself.basename = self.path.name\nself.nwb_path = self.path / \"pynapplenwb\"\n# from what I can see in the loading function, only one nwb file per folder:\ntry:\nself.nwb_file = list(self.nwb_path.glob(\"*.nwb\"))[0]\nexcept IndexError:\nself.nwb_file = None\n# Need to check if nwb file exists and if data are there\n# if self.path is not None:  -&gt; are there any cases where this is None?\nif self.nwb_file is not None:\nloaded_spikes = self.load_nwb_spikes()\nif loaded_spikes is not None:\nreturn\n# Bypass if data have already been transferred to nwb\nself.load_phy_params()\napp = App()\nwindow = EphysGUI(app, path=path, groups=self.channel_map)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ephys_information = window.ephys_information\nself.load_phy_spikes(self.time_support)\nself.save_data()\napp.quit()\n</code></pre>"},{"location":"io.phy/#pynapple.io.phy.Phy.load_phy_params","title":"<code>load_phy_params()</code>","text":"<p>path should be the folder session containing the params.py file</p> <p>Function reads : 1. the number of channels 2. the sampling frequency of the dat file</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If path does not contain the params file or channel_map.npy</p> Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/phy.py</code> <pre><code>def load_phy_params(self):\n\"\"\"\n    path should be the folder session containing the params.py file\n    Function reads :\n    1. the number of channels\n    2. the sampling frequency of the dat file\n    Raises\n    ------\n    AssertionError\n        If path does not contain the params file or channel_map.npy\n    \"\"\"\nassert (\nself.path / \"params.py\"\n).exists(), f\"Can't find params.py in {self.path}\"\n# It is strongly recommended not to conflate parameters and code! Also, there's a library called params.\n# I would recommend putting in the folder a file called params.json, or .txt, or .yml, but not .py!\n# In this way we just read the file, and we don't have to add to sys to import...\n# TODO maybe remove this\nsys.path.append(str(self.path))\nimport params as params\nself.sample_rate = params.sample_rate\nself.n_channels_dat = params.n_channels_dat\nassert (\nself.path / \"channel_map.npy\"\n).exists(), f\"Can't find channel_map.npy in {self.path}\"\nchannel_map = np.load(self.path / \"channel_map.npy\")\nif (self.path / \"channel_shanks.npy\").exists():\nchannel_shank = np.load(self.path / \"channel_shanks.npy\")\nn_shanks = len(np.unique(channel_shank))\nself.channel_map = {\ni: channel_map[channel_shank == i] for i in range(n_shanks)\n}\nself.ch_to_sh = pd.Series(\nindex=channel_map.flatten(),\ndata=channel_shank.flatten(),\n)\nelse:\nself.channel_map = {i: channel_map[i] for i in range(len(channel_map))}\nself.ch_to_sh = pd.Series(\nindex=channel_map.flatten(),\ndata=np.hstack(\n[\nnp.ones(len(channel_map[i]), dtype=int) * i\nfor i in range(len(channel_map))\n]\n),\n)\nreturn\n</code></pre>"},{"location":"io.phy/#pynapple.io.phy.Phy.load_phy_spikes","title":"<code>load_phy_spikes(time_support=None)</code>","text":"<p>Load Phy spike times and convert to NWB. Instantiate automatically a TsGroup object. The cluster group is taken first from cluster_info.tsv and second from cluster_group.tsv</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path object</code> <p>The path to the data</p> required <code>time_support</code> <code>IntevalSet, optional</code> <p>The time support of the data</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If files are missing. The function needs : - cluster_info.tsv or cluster_group.tsv - spike_times.npy - spike_clusters.npy - channel_positions.npy - templates.npy</p> Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/phy.py</code> <pre><code>def load_phy_spikes(self, time_support=None):\n\"\"\"\n    Load Phy spike times and convert to NWB.\n    Instantiate automatically a TsGroup object.\n    The cluster group is taken first from cluster_info.tsv and second from cluster_group.tsv\n    Parameters\n    ----------\n    path : Path object\n        The path to the data\n    time_support : IntevalSet, optional\n        The time support of the data\n    Raises\n    ------\n    RuntimeError\n        If files are missing.\n        The function needs :\n        - cluster_info.tsv or cluster_group.tsv\n        - spike_times.npy\n        - spike_clusters.npy\n        - channel_positions.npy\n        - templates.npy\n    \"\"\"\n# Check if cluster_info.tsv or cluster_group.tsv exists. If both exist, cluster_info.tsv is used:\nhas_cluster_info = False\nif (self.path / \"cluster_info.tsv\").exists():\ncluster_info_file = self.path / \"cluster_info.tsv\"\nhas_cluster_info = True\nelif (self.path / \"cluster_group.tsv\").exists():\ncluster_info_file = self.path / \"cluster_group.tsv\"\nelse:\nraise RuntimeError(\n\"Can't find cluster_info.tsv or cluster_group.tsv in {};\".format(\nself.path\n)\n)\ncluster_info = pd.read_csv(cluster_info_file, sep=\"\\t\", index_col=\"cluster_id\")\n# In my processed data with KiloSort 3.0, the column is named KSLabel\nif \"group\" in cluster_info.columns:\ncluster_id_good = cluster_info[cluster_info.group == \"good\"].index.values\nelif \"KSLabel\" in cluster_info.columns:\ncluster_id_good = cluster_info[cluster_info.KSLabel == \"good\"].index.values\nelse:\nraise RuntimeError(\n\"Can't find column group or KSLabel in {};\".format(cluster_info_file)\n)\nspike_times = np.load(self.path / \"spike_times.npy\")\nspike_clusters = np.load(self.path / \"spike_clusters.npy\")\nspikes = {}\nfor n in cluster_id_good:\nspikes[n] = nap.Ts(\nt=spike_times[spike_clusters == n] / self.sample_rate,\ntime_support=time_support,\n)\nself.spikes = nap.TsGroup(spikes, time_support=time_support)\n# Adding the position of the electrodes in case\nself.channel_positions = np.load(self.path / \"channel_positions.npy\")\n# Adding shank group info from cluster_info if present\nif has_cluster_info:\ngroup = cluster_info.loc[cluster_id_good, \"sh\"]\nself.spikes.set_info(group=group)\nelse:\ntemplate = np.load(self.path / \"templates.npy\")\ntemplate = template[cluster_id_good]\nch = np.power(template, 2).max(1).argmax(1)\ngroup = pd.Series(index=cluster_id_good, data=self.ch_to_sh[ch].values)\nself.spikes.set_info(group=group)\nnames = pd.Series(\nindex=group.index,\ndata=[self.ephys_information[group.loc[i]][\"name\"] for i in group.index],\n)\nif ~np.all(names.values == \"\"):\nself.spikes.set_info(name=names)\nlocations = pd.Series(\nindex=group.index,\ndata=[\nself.ephys_information[group.loc[i]][\"location\"] for i in group.index\n],\n)\nif ~np.all(locations.values == \"\"):\nself.spikes.set_info(location=locations)\nreturn\n</code></pre>"},{"location":"io.phy/#pynapple.io.phy.Phy.save_data","title":"<code>save_data()</code>","text":"<p>Save the data to NWB format.</p> Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/phy.py</code> <pre><code>def save_data(self):\n\"\"\"Save the data to NWB format.\"\"\"\nio = NWBHDF5IO(self.nwb_file, \"r+\")\nnwbfile = io.read()\nelectrode_groups = {}\nfor g in self.channel_map:\ndevice = nwbfile.create_device(\nname=self.ephys_information[g][\"device\"][\"name\"] + \"-\" + str(g),\ndescription=self.ephys_information[g][\"device\"][\"description\"],\nmanufacturer=self.ephys_information[g][\"device\"][\"manufacturer\"],\n)\nif (\nlen(self.ephys_information[g][\"position\"])\nand type(self.ephys_information[g][\"position\"]) is str\n):\nself.ephys_information[g][\"position\"] = re.split(\n\";|,| \", self.ephys_information[g][\"position\"]\n)\nelif self.ephys_information[g][\"position\"] == \"\":\nself.ephys_information[g][\"position\"] = None\nelectrode_groups[g] = nwbfile.create_electrode_group(\nname=\"group\" + str(g) + \"_\" + self.ephys_information[g][\"name\"],\ndescription=self.ephys_information[g][\"description\"],\nposition=self.ephys_information[g][\"position\"],\nlocation=self.ephys_information[g][\"location\"],\ndevice=device,\n)\nfor idx in self.channel_map[g]:\nnwbfile.add_electrode(\nid=idx,\nx=0.0,\ny=0.0,\nz=0.0,\nimp=0.0,\nlocation=self.ephys_information[g][\"location\"],\nfiltering=\"none\",\ngroup=electrode_groups[g],\n)\n# Adding units\nnwbfile.add_unit_column(\"location\", \"the anatomical location of this unit\")\nnwbfile.add_unit_column(\"group\", \"the group of the unit\")\nfor u in self.spikes.keys():\nnwbfile.add_unit(\nid=u,\nspike_times=self.spikes[u].as_units(\"s\").index.values,\nelectrode_group=electrode_groups[self.spikes.get_info(\"group\").loc[u]],\nlocation=self.ephys_information[self.spikes.get_info(\"group\").loc[u]][\n\"location\"\n],\ngroup=self.spikes.get_info(\"group\").loc[u],\n)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"io.phy/#pynapple.io.phy.Phy.load_nwb_spikes","title":"<code>load_nwb_spikes()</code>","text":"<p>Read the NWB spikes to extract the spike times.</p> <p>Returns:</p> Type Description <code>TYPE</code> <p>Description</p> Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/phy.py</code> <pre><code>def load_nwb_spikes(self):\n\"\"\"Read the NWB spikes to extract the spike times.\n    Returns\n    -------\n    TYPE\n        Description\n    \"\"\"\nio = NWBHDF5IO(self.nwb_file, \"r\")\nnwbfile = io.read()\nif nwbfile.units is None:\nio.close()\nreturn None\nelse:\nunits = nwbfile.units.to_dataframe()\nspikes = {\nn: nap.Ts(t=units.loc[n, \"spike_times\"], time_units=\"s\")\nfor n in units.index\n}\nself.spikes = nap.TsGroup(\nspikes,\ntime_support=self.time_support,\ntime_units=\"s\",\ngroup=units[\"group\"],\n)\nif ~np.all(units[\"location\"] == \"\"):\nself.spikes.set_info(location=units[\"location\"])\nio.close()\nreturn True\n</code></pre>"},{"location":"io.phy/#pynapple.io.phy.Phy.load_lfp","title":"<code>load_lfp(filename=None, channel=None, extension='.eeg', frequency=1250.0, precision='int16', bytes_size=2)</code>","text":"<p>Load the LFP.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str, optional</code> <p>The filename of the lfp file. It can be useful it multiple dat files are present in the data directory</p> <code>None</code> <code>channel</code> <code>int or list of int, optional</code> <p>The channel(s) to load. If None return a memory map of the dat file to avoid memory error</p> <code>None</code> <code>extension</code> <code>str, optional</code> <p>The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match</p> <code>'.eeg'</code> <code>frequency</code> <code>float, optional</code> <p>Default 1250 Hz for the eeg file</p> <code>1250.0</code> <code>precision</code> <code>str, optional</code> <p>The precision of the binary file</p> <code>'int16'</code> <code>bytes_size</code> <code>int, optional</code> <p>Bytes size of the lfp file</p> <code>2</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If can't find the lfp/eeg/dat file</p> <p>Returns:</p> Type Description <code>Tsd or TsdFrame</code> <p>The lfp in a time series format</p> Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/phy.py</code> <pre><code>def load_lfp(\nself,\nfilename=None,\nchannel=None,\nextension=\".eeg\",\nfrequency=1250.0,\nprecision=\"int16\",\nbytes_size=2,\n):\n\"\"\"\n    Load the LFP.\n    Parameters\n    ----------\n    filename : str, optional\n        The filename of the lfp file.\n        It can be useful it multiple dat files are present in the data directory\n    channel : int or list of int, optional\n        The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n    extension : str, optional\n        The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n    frequency : float, optional\n        Default 1250 Hz for the eeg file\n    precision : str, optional\n        The precision of the binary file\n    bytes_size : int, optional\n        Bytes size of the lfp file\n    Raises\n    ------\n    RuntimeError\n        If can't find the lfp/eeg/dat file\n    Returns\n    -------\n    Tsd or TsdFrame\n        The lfp in a time series format\n    \"\"\"\nif filename is not None:\nfilepath = self.path / filename\nelse:\ntry:\nfilepath = list(self.path.glob(f\"*{extension}\"))[0]\nexcept IndexError:\nraise RuntimeError(f\"Path {self.path} contains no {extension} files;\")\n# is it possible that this is a leftover from neurosuite data?\n# This is not implemented for this class.\nself.load_neurosuite_xml(self.path)\nn_channels = int(self.nChannels)\nf = open(filepath, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nduration = n_samples / frequency\nf.close()\nfp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\ntimestep = np.arange(0, n_samples) / frequency\ntime_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\nif channel is None:\nreturn nap.TsdFrame(\nt=timestep, d=fp, time_units=\"s\", time_support=time_support\n)\nelif type(channel) is int:\nreturn nap.Tsd(\nt=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n)\nelif type(channel) is list:\nreturn nap.TsdFrame(\nt=timestep,\nd=fp[:, channel],\ntime_units=\"s\",\ntime_support=time_support,\ncolumns=channel,\n)\n</code></pre>"},{"location":"io.suite2p/","title":"Suite2p","text":"<p>Loader for Suite2P https://github.com/MouseLand/suite2p</p>"},{"location":"io.suite2p/#pynapple.io.suite2p.Suite2P","title":"<code>Suite2P</code>","text":"<p>         Bases: <code>BaseLoader</code></p> <p>Loader for data processed with Suite2P.</p> <p>Pynapple will try to look for data in this order :</p> <ol> <li> <p>pynapplenwb/session_name.nwb</p> </li> <li> <p>suite2p/plane/.npy</p> </li> </ol> <p>Attributes:</p> Name Type Description <code>F</code> <code>TsdFrame</code> <p>Fluorescence traces (timepoints x ROIs) for all planes</p> <code>Fneu</code> <code>TsdFrame</code> <p>Neuropil fluorescence traces (timepoints x ROIs) for all planes</p> <code>spks</code> <code>TsdFrame</code> <p>Deconvolved traces (timepoints x ROIS) for all planes</p> <code>plane_info</code> <code>pandas.DataFrame</code> <p>Contains plane identity of each cell</p> <code>stats</code> <code>dict</code> <p>dictionnay of statistics from stat.npy for each planes only for the neurons that were classified as cells (Can be smaller when loading from the NWB file)</p> <code>ops</code> <code>dict</code> <p>Parameters from Suite2p. (Can be smaller when loading from the NWB file)</p> <code>iscell</code> <code>numpy.ndarray</code> <p>Cell classification</p> Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/suite2p.py</code> <pre><code>class Suite2P(BaseLoader):\n\"\"\"Loader for data processed with Suite2P.\n    Pynapple will try to look for data in this order :\n    1. pynapplenwb/session_name.nwb\n    2. suite2p/plane*/*.npy\n    Attributes\n    ----------\n    F : TsdFrame\n        Fluorescence traces (timepoints x ROIs) for all planes\n    Fneu : TsdFrame\n        Neuropil fluorescence traces (timepoints x ROIs) for all planes\n    spks : TsdFrame\n        Deconvolved traces (timepoints x ROIS) for all planes\n    plane_info : pandas.DataFrame\n        Contains plane identity of each cell\n    stats : dict\n        dictionnay of statistics from stat.npy for each planes only for the neurons that were classified as cells\n        (Can be smaller when loading from the NWB file)\n    ops : dict\n        Parameters from Suite2p. (Can be smaller when loading from the NWB file)\n    iscell : numpy.ndarray\n        Cell classification\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Parameters\n        ----------\n        path : str\n            The path of the session\n        \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_suite2p_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_suite2p(path)\nself.save_suite2p_nwb(path)\ndef load_suite2p(self, path):\n\"\"\"\n        Looking for suite2/plane*\n        Parameters\n        ----------\n        path : str\n            The path of the session\n        \"\"\"\nself.path_suite2p = os.path.join(path, \"suite2p\")\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ndata = {\n\"F\": [],\n\"Fneu\": [],\n\"spks\": [],\n}\nplane_info = []\nself.stats = {}\nself.pops = {}\nself.iscells = {}\nself.planes = []\nif os.path.exists(self.path_suite2p):\nplanes = glob.glob(os.path.join(self.path_suite2p, \"plane*\"))\nif len(planes):\n# count = 0\nfor plane_dir in planes:\nn = int(os.path.basename(plane_dir)[-1])\nself.planes.append(n)\n# Loading iscell.npy\ntry:\niscell = np.load(\nos.path.join(plane_dir, \"iscell.npy\"), allow_pickle=True\n)\nidx = np.where(iscell.astype(\"int\")[:, 0])[0]\nplane_info.append(np.ones(len(idx), dtype=\"int\") * n)\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Loading F.npy, Fneu.py and spks.npy\nfor obj in [\"F.npy\", \"Fneu.npy\", \"spks.npy\"]:\ntry:\nname = obj.split(\".\")[0]\ntmp = np.load(\nos.path.join(plane_dir, obj), allow_pickle=True\n)\ndata[name].append(tmp[idx])\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Loading stat.npy and ops.npy\ntry:\nstat = np.load(\nos.path.join(plane_dir, \"stat.npy\"), allow_pickle=True\n)\nops = np.load(\nos.path.join(plane_dir, \"ops.npy\"), allow_pickle=True\n).item()\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Saving stat, ops and iscell\nself.stats[n] = stat\nself.pops[n] = ops\nself.iscells[n] = iscell\n# count += len(idx)\nelse:\nwarnings.warn(\n\"Couldn't find planes in %s\" % self.path_suite2p, stacklevel=2\n)\nsys.exit()\nelse:\nwarnings.warn(\"No suite2p folder in %s\" % path, stacklevel=2)\nsys.exit()\n# Calcium transients\ndata[\"F\"] = np.transpose(np.vstack(data[\"F\"]))\ndata[\"Fneu\"] = np.transpose(np.vstack(data[\"Fneu\"]))\ndata[\"spks\"] = np.transpose(np.vstack(data[\"spks\"]))\ntime_index = np.arange(0, len(data[\"F\"])) / self.sampling_rate\nself.F = nap.TsdFrame(t=time_index, d=data[\"F\"])\nself.Fneu = nap.TsdFrame(t=time_index, d=data[\"Fneu\"])\nself.spks = nap.TsdFrame(t=time_index, d=data[\"spks\"])\nself.ops = self.pops[0]\nself.iscell = np.vstack([self.iscells[k] for k in self.iscells.keys()])\n# Metadata\nself.plane_info = pd.DataFrame.from_dict({\"plane\": np.hstack(plane_info)})\nreturn\ndef save_suite2p_nwb(self, path):\n\"\"\"\n        Save the data to NWB. To ensure continuity, this function is based on :\n        https://github.com/MouseLand/suite2p/blob/main/suite2p/io/nwb.py.\n        Parameters\n        ----------\n        path : str\n            The path of the session\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nmultiplane = True if len(self.planes) &gt; 1 else False\nops = self.pops[list(self.pops.keys())[0]]\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice = nwbfile.create_device(\nname=self.ophys_information[\"device\"][\"name\"],\ndescription=self.ophys_information[\"device\"][\"description\"],\nmanufacturer=self.ophys_information[\"device\"][\"manufacturer\"],\n)\nimaging_plane = nwbfile.create_imaging_plane(\nname=self.ophys_information[\"ImagingPlane\"][\"name\"],\noptical_channel=OpticalChannel(\nname=self.ophys_information[\"OpticalChannel\"][\"name\"],\ndescription=self.ophys_information[\"OpticalChannel\"][\"description\"],\nemission_lambda=float(\nself.ophys_information[\"OpticalChannel\"][\"emission_lambda\"]\n),\n),\nimaging_rate=self.sampling_rate,\ndescription=self.ophys_information[\"ImagingPlane\"][\"description\"],\ndevice=device,\nexcitation_lambda=float(\nself.ophys_information[\"ImagingPlane\"][\"excitation_lambda\"]\n),\nindicator=self.ophys_information[\"ImagingPlane\"][\"indicator\"],\nlocation=self.ophys_information[\"ImagingPlane\"][\"location\"],\ngrid_spacing=([2.0, 2.0, 30.0] if multiplane else [2.0, 2.0]),\ngrid_spacing_unit=\"microns\",\n)\n# link to external data\nimage_series = TwoPhotonSeries(\nname=\"TwoPhotonSeries\",\ndimension=[ops[\"Ly\"], ops[\"Lx\"]],\nexternal_file=(ops[\"filelist\"] if \"filelist\" in ops else [\"\"]),\nimaging_plane=imaging_plane,\nstarting_frame=[0],\nformat=\"external\",\nstarting_time=0.0,\nrate=ops[\"fs\"] * ops[\"nplanes\"],\n)\nnwbfile.add_acquisition(image_series)\n# processing\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=self.ophys_information[\"PlaneSegmentation\"][\"name\"],\ndescription=self.ophys_information[\"PlaneSegmentation\"][\"description\"],\nimaging_plane=imaging_plane,\n# reference_images=image_series,\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nophys_module.add(img_seg)\nfile_strs = [\"F.npy\", \"Fneu.npy\", \"spks.npy\"]\ntraces = []\nncells = np.zeros(len(self.pops), dtype=np.int_)\nNfr = np.array([self.pops[k][\"nframes\"] for k in self.pops.keys()]).max()\nfor iplane, ops in self.pops.items():\nif iplane == 0:\niscell = self.iscells[iplane]\nfor fstr in file_strs:\ntraces.append(np.load(os.path.join(ops[\"save_path\"], fstr)))\nPlaneCellsIdx = iplane * np.ones(len(iscell))\nelse:\niscell = np.append(\niscell,\nself.iscells[iplane],\naxis=0,\n)\nfor i, fstr in enumerate(file_strs):\ntrace = np.load(os.path.join(ops[\"save_path\"], fstr))\nif trace.shape[1] &lt; Nfr:\nfcat = np.zeros(\n(trace.shape[0], Nfr - trace.shape[1]), \"float32\"\n)\ntrace = np.concatenate((trace, fcat), axis=1)\ntraces[i] = np.append(traces[i], trace, axis=0)\nPlaneCellsIdx = np.append(\nPlaneCellsIdx, iplane * np.ones(len(iscell) - len(PlaneCellsIdx))\n)\nstat = self.stats[iplane]\nncells[iplane] = len(stat)\nfor n in range(ncells[iplane]):\nif multiplane:\npixel_mask = np.array(\n[\nstat[n][\"ypix\"],\nstat[n][\"xpix\"],\niplane * np.ones(stat[n][\"npix\"]),\nstat[n][\"lam\"],\n]\n)\nps.add_roi(voxel_mask=pixel_mask.T)\nelse:\npixel_mask = np.array(\n[stat[n][\"ypix\"], stat[n][\"xpix\"], stat[n][\"lam\"]]\n)\nps.add_roi(pixel_mask=pixel_mask.T)\nps.add_column(\"iscell\", \"two columns - iscell &amp; probcell\", iscell)\nrt_region = []\nfor iplane, ops in self.pops.items():\nif iplane == 0:\nrt_region.append(\nps.create_roi_table_region(\nregion=list(\nnp.arange(0, ncells[iplane]),\n),\ndescription=f\"ROIs for plane{int(iplane)}\",\n)\n)\nelse:\nrt_region.append(\nps.create_roi_table_region(\nregion=list(\nnp.arange(\nnp.sum(ncells[:iplane]),\nncells[iplane] + np.sum(ncells[:iplane]),\n)\n),\ndescription=f\"ROIs for plane{int(iplane)}\",\n)\n)\n# FLUORESCENCE (all are required)\nname_strs = [\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]\nfor i, (fstr, nstr) in enumerate(zip(file_strs, name_strs)):\nfor iplane, ops in self.pops.items():\nroi_resp_series = RoiResponseSeries(\nname=f\"plane{int(iplane)}\",\ndata=traces[i][PlaneCellsIdx == iplane],\nrois=rt_region[iplane],\nunit=\"lumens\",\nrate=ops[\"fs\"],\n)\nif iplane == 0:\nfl = Fluorescence(roi_response_series=roi_resp_series, name=nstr)\nelse:\nfl.add_roi_response_series(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_suite2p_nwb(self, path):\n\"\"\"\n        Load suite2p data from NWB\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\nophys = nwbfile.processing[\"ophys\"]\n#################################################################\n# STATS, OPS and ISCELL\n#################################################################\ndims = nwbfile.acquisition[\"TwoPhotonSeries\"].dimension[:]\nself.ops = {\"Ly\": dims[0], \"Lx\": dims[1]}\nself.rate = nwbfile.acquisition[\n\"TwoPhotonSeries\"\n].imaging_plane.imaging_rate\nself.stats = {0: {}}\nself.iscell = ophys[\"ImageSegmentation\"][\"PlaneSegmentation\"][\n\"iscell\"\n].data[:]\ninfo = pd.DataFrame(\ndata=self.iscell[:, 0].astype(\"int\"), columns=[\"iscell\"]\n)\n#################################################################\n# ROIS\n#################################################################\ntry:\nrois = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"pixel_mask\"]\nmultiplane = False\nexcept Exception:\nrois = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"voxel_mask\"]\nmultiplane = True\nidx = np.where(self.iscell[:, 0])[0]\ninfo[\"plane\"] = 0\nfor n in range(len(rois)):\nroi = pd.DataFrame(rois[n])\nif \"z\" in roi.columns:\npl = roi[\"z\"][0]\nelse:\npl = 0\ninfo.loc[n, \"plane\"] = pl\nif pl not in self.stats.keys():\nself.stats[pl] = {}\nif n in idx:\nself.stats[pl][n] = {\n\"xpix\": roi[\"y\"].values,\n\"ypix\": roi[\"x\"].values,\n\"lam\": roi[\"weight\"].values,\n}\n#################################################################\n# Time Series\n#################################################################\nfields = np.intersect1d(\n[\"Fluorescence\", \"Neuropil\", \"Deconvolved\"],\nlist(ophys.fields[\"data_interfaces\"].keys()),\n)\nif len(fields) == 0:\nprint(\n\"No \" + \" or \".join([\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]),\n\"found in nwb {}\".format(self.nwbfilepath),\n)\nreturn False\nkeys = ophys[fields[0]].roi_response_series.keys()\nplanes = [int(k[-1]) for k in keys if \"plane\" in k]\ndata = {}\nif multiplane:\nkeys = ophys[fields[0]].roi_response_series.keys()\nplanes = [int(k[-1]) for k in keys if \"plane\" in k]\nelse:\nplanes = [0]\nfor k, name in zip(\n[\"F\", \"Fneu\", \"spks\"], [\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]\n):\ntmp = []\ntimestamps = []\nfor i, n in enumerate(planes):\nif multiplane:\npl = \"plane{}\".format(n)\nelse:\npl = name  # This doesn't make sense\ntokeep = info[\"iscell\"][info[\"plane\"] == n].values == 1\nd = np.transpose(ophys[name][pl].data[:][tokeep])\nif ophys[name][pl].timestamps is not None:\nt = ophys[name][pl].timestamps[:]\nelse:\nt = (np.arange(0, len(d)) / self.rate) + ophys[name][\npl\n].starting_time\ntmp.append(d)\ntimestamps.append(t)\ndata[k] = nap.TsdFrame(t=timestamps[0], d=np.hstack(tmp))\nif \"F\" in data.keys():\nself.F = data[\"F\"]\nif \"Fneu\" in data.keys():\nself.Fneu = data[\"Fneu\"]\nif \"spks\" in data.keys():\nself.spks = data[\"spks\"]\nself.plane_info = pd.DataFrame(\ndata=info[\"plane\"][info[\"iscell\"] == 1].values, columns=[\"plane\"]\n)\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"io.suite2p/#pynapple.io.suite2p.Suite2P.__init__","title":"<code>__init__(path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the session</p> required Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/suite2p.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Parameters\n    ----------\n    path : str\n        The path of the session\n    \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_suite2p_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_suite2p(path)\nself.save_suite2p_nwb(path)\n</code></pre>"},{"location":"io.suite2p/#pynapple.io.suite2p.Suite2P.load_suite2p","title":"<code>load_suite2p(path)</code>","text":"<p>Looking for suite2/plane*</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the session</p> required Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/suite2p.py</code> <pre><code>def load_suite2p(self, path):\n\"\"\"\n    Looking for suite2/plane*\n    Parameters\n    ----------\n    path : str\n        The path of the session\n    \"\"\"\nself.path_suite2p = os.path.join(path, \"suite2p\")\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ndata = {\n\"F\": [],\n\"Fneu\": [],\n\"spks\": [],\n}\nplane_info = []\nself.stats = {}\nself.pops = {}\nself.iscells = {}\nself.planes = []\nif os.path.exists(self.path_suite2p):\nplanes = glob.glob(os.path.join(self.path_suite2p, \"plane*\"))\nif len(planes):\n# count = 0\nfor plane_dir in planes:\nn = int(os.path.basename(plane_dir)[-1])\nself.planes.append(n)\n# Loading iscell.npy\ntry:\niscell = np.load(\nos.path.join(plane_dir, \"iscell.npy\"), allow_pickle=True\n)\nidx = np.where(iscell.astype(\"int\")[:, 0])[0]\nplane_info.append(np.ones(len(idx), dtype=\"int\") * n)\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Loading F.npy, Fneu.py and spks.npy\nfor obj in [\"F.npy\", \"Fneu.npy\", \"spks.npy\"]:\ntry:\nname = obj.split(\".\")[0]\ntmp = np.load(\nos.path.join(plane_dir, obj), allow_pickle=True\n)\ndata[name].append(tmp[idx])\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Loading stat.npy and ops.npy\ntry:\nstat = np.load(\nos.path.join(plane_dir, \"stat.npy\"), allow_pickle=True\n)\nops = np.load(\nos.path.join(plane_dir, \"ops.npy\"), allow_pickle=True\n).item()\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Saving stat, ops and iscell\nself.stats[n] = stat\nself.pops[n] = ops\nself.iscells[n] = iscell\n# count += len(idx)\nelse:\nwarnings.warn(\n\"Couldn't find planes in %s\" % self.path_suite2p, stacklevel=2\n)\nsys.exit()\nelse:\nwarnings.warn(\"No suite2p folder in %s\" % path, stacklevel=2)\nsys.exit()\n# Calcium transients\ndata[\"F\"] = np.transpose(np.vstack(data[\"F\"]))\ndata[\"Fneu\"] = np.transpose(np.vstack(data[\"Fneu\"]))\ndata[\"spks\"] = np.transpose(np.vstack(data[\"spks\"]))\ntime_index = np.arange(0, len(data[\"F\"])) / self.sampling_rate\nself.F = nap.TsdFrame(t=time_index, d=data[\"F\"])\nself.Fneu = nap.TsdFrame(t=time_index, d=data[\"Fneu\"])\nself.spks = nap.TsdFrame(t=time_index, d=data[\"spks\"])\nself.ops = self.pops[0]\nself.iscell = np.vstack([self.iscells[k] for k in self.iscells.keys()])\n# Metadata\nself.plane_info = pd.DataFrame.from_dict({\"plane\": np.hstack(plane_info)})\nreturn\n</code></pre>"},{"location":"io.suite2p/#pynapple.io.suite2p.Suite2P.save_suite2p_nwb","title":"<code>save_suite2p_nwb(path)</code>","text":"<p>Save the data to NWB. To ensure continuity, this function is based on : https://github.com/MouseLand/suite2p/blob/main/suite2p/io/nwb.py.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the session</p> required Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/suite2p.py</code> <pre><code>def save_suite2p_nwb(self, path):\n\"\"\"\n    Save the data to NWB. To ensure continuity, this function is based on :\n    https://github.com/MouseLand/suite2p/blob/main/suite2p/io/nwb.py.\n    Parameters\n    ----------\n    path : str\n        The path of the session\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nmultiplane = True if len(self.planes) &gt; 1 else False\nops = self.pops[list(self.pops.keys())[0]]\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice = nwbfile.create_device(\nname=self.ophys_information[\"device\"][\"name\"],\ndescription=self.ophys_information[\"device\"][\"description\"],\nmanufacturer=self.ophys_information[\"device\"][\"manufacturer\"],\n)\nimaging_plane = nwbfile.create_imaging_plane(\nname=self.ophys_information[\"ImagingPlane\"][\"name\"],\noptical_channel=OpticalChannel(\nname=self.ophys_information[\"OpticalChannel\"][\"name\"],\ndescription=self.ophys_information[\"OpticalChannel\"][\"description\"],\nemission_lambda=float(\nself.ophys_information[\"OpticalChannel\"][\"emission_lambda\"]\n),\n),\nimaging_rate=self.sampling_rate,\ndescription=self.ophys_information[\"ImagingPlane\"][\"description\"],\ndevice=device,\nexcitation_lambda=float(\nself.ophys_information[\"ImagingPlane\"][\"excitation_lambda\"]\n),\nindicator=self.ophys_information[\"ImagingPlane\"][\"indicator\"],\nlocation=self.ophys_information[\"ImagingPlane\"][\"location\"],\ngrid_spacing=([2.0, 2.0, 30.0] if multiplane else [2.0, 2.0]),\ngrid_spacing_unit=\"microns\",\n)\n# link to external data\nimage_series = TwoPhotonSeries(\nname=\"TwoPhotonSeries\",\ndimension=[ops[\"Ly\"], ops[\"Lx\"]],\nexternal_file=(ops[\"filelist\"] if \"filelist\" in ops else [\"\"]),\nimaging_plane=imaging_plane,\nstarting_frame=[0],\nformat=\"external\",\nstarting_time=0.0,\nrate=ops[\"fs\"] * ops[\"nplanes\"],\n)\nnwbfile.add_acquisition(image_series)\n# processing\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=self.ophys_information[\"PlaneSegmentation\"][\"name\"],\ndescription=self.ophys_information[\"PlaneSegmentation\"][\"description\"],\nimaging_plane=imaging_plane,\n# reference_images=image_series,\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nophys_module.add(img_seg)\nfile_strs = [\"F.npy\", \"Fneu.npy\", \"spks.npy\"]\ntraces = []\nncells = np.zeros(len(self.pops), dtype=np.int_)\nNfr = np.array([self.pops[k][\"nframes\"] for k in self.pops.keys()]).max()\nfor iplane, ops in self.pops.items():\nif iplane == 0:\niscell = self.iscells[iplane]\nfor fstr in file_strs:\ntraces.append(np.load(os.path.join(ops[\"save_path\"], fstr)))\nPlaneCellsIdx = iplane * np.ones(len(iscell))\nelse:\niscell = np.append(\niscell,\nself.iscells[iplane],\naxis=0,\n)\nfor i, fstr in enumerate(file_strs):\ntrace = np.load(os.path.join(ops[\"save_path\"], fstr))\nif trace.shape[1] &lt; Nfr:\nfcat = np.zeros(\n(trace.shape[0], Nfr - trace.shape[1]), \"float32\"\n)\ntrace = np.concatenate((trace, fcat), axis=1)\ntraces[i] = np.append(traces[i], trace, axis=0)\nPlaneCellsIdx = np.append(\nPlaneCellsIdx, iplane * np.ones(len(iscell) - len(PlaneCellsIdx))\n)\nstat = self.stats[iplane]\nncells[iplane] = len(stat)\nfor n in range(ncells[iplane]):\nif multiplane:\npixel_mask = np.array(\n[\nstat[n][\"ypix\"],\nstat[n][\"xpix\"],\niplane * np.ones(stat[n][\"npix\"]),\nstat[n][\"lam\"],\n]\n)\nps.add_roi(voxel_mask=pixel_mask.T)\nelse:\npixel_mask = np.array(\n[stat[n][\"ypix\"], stat[n][\"xpix\"], stat[n][\"lam\"]]\n)\nps.add_roi(pixel_mask=pixel_mask.T)\nps.add_column(\"iscell\", \"two columns - iscell &amp; probcell\", iscell)\nrt_region = []\nfor iplane, ops in self.pops.items():\nif iplane == 0:\nrt_region.append(\nps.create_roi_table_region(\nregion=list(\nnp.arange(0, ncells[iplane]),\n),\ndescription=f\"ROIs for plane{int(iplane)}\",\n)\n)\nelse:\nrt_region.append(\nps.create_roi_table_region(\nregion=list(\nnp.arange(\nnp.sum(ncells[:iplane]),\nncells[iplane] + np.sum(ncells[:iplane]),\n)\n),\ndescription=f\"ROIs for plane{int(iplane)}\",\n)\n)\n# FLUORESCENCE (all are required)\nname_strs = [\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]\nfor i, (fstr, nstr) in enumerate(zip(file_strs, name_strs)):\nfor iplane, ops in self.pops.items():\nroi_resp_series = RoiResponseSeries(\nname=f\"plane{int(iplane)}\",\ndata=traces[i][PlaneCellsIdx == iplane],\nrois=rt_region[iplane],\nunit=\"lumens\",\nrate=ops[\"fs\"],\n)\nif iplane == 0:\nfl = Fluorescence(roi_response_series=roi_resp_series, name=nstr)\nelse:\nfl.add_roi_response_series(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"io.suite2p/#pynapple.io.suite2p.Suite2P.load_suite2p_nwb","title":"<code>load_suite2p_nwb(path)</code>","text":"<p>Load suite2p data from NWB</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>/mnt/home/gviejo/pynapple/pynapple/io/suite2p.py</code> <pre><code>def load_suite2p_nwb(self, path):\n\"\"\"\n    Load suite2p data from NWB\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\nophys = nwbfile.processing[\"ophys\"]\n#################################################################\n# STATS, OPS and ISCELL\n#################################################################\ndims = nwbfile.acquisition[\"TwoPhotonSeries\"].dimension[:]\nself.ops = {\"Ly\": dims[0], \"Lx\": dims[1]}\nself.rate = nwbfile.acquisition[\n\"TwoPhotonSeries\"\n].imaging_plane.imaging_rate\nself.stats = {0: {}}\nself.iscell = ophys[\"ImageSegmentation\"][\"PlaneSegmentation\"][\n\"iscell\"\n].data[:]\ninfo = pd.DataFrame(\ndata=self.iscell[:, 0].astype(\"int\"), columns=[\"iscell\"]\n)\n#################################################################\n# ROIS\n#################################################################\ntry:\nrois = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"pixel_mask\"]\nmultiplane = False\nexcept Exception:\nrois = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"voxel_mask\"]\nmultiplane = True\nidx = np.where(self.iscell[:, 0])[0]\ninfo[\"plane\"] = 0\nfor n in range(len(rois)):\nroi = pd.DataFrame(rois[n])\nif \"z\" in roi.columns:\npl = roi[\"z\"][0]\nelse:\npl = 0\ninfo.loc[n, \"plane\"] = pl\nif pl not in self.stats.keys():\nself.stats[pl] = {}\nif n in idx:\nself.stats[pl][n] = {\n\"xpix\": roi[\"y\"].values,\n\"ypix\": roi[\"x\"].values,\n\"lam\": roi[\"weight\"].values,\n}\n#################################################################\n# Time Series\n#################################################################\nfields = np.intersect1d(\n[\"Fluorescence\", \"Neuropil\", \"Deconvolved\"],\nlist(ophys.fields[\"data_interfaces\"].keys()),\n)\nif len(fields) == 0:\nprint(\n\"No \" + \" or \".join([\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]),\n\"found in nwb {}\".format(self.nwbfilepath),\n)\nreturn False\nkeys = ophys[fields[0]].roi_response_series.keys()\nplanes = [int(k[-1]) for k in keys if \"plane\" in k]\ndata = {}\nif multiplane:\nkeys = ophys[fields[0]].roi_response_series.keys()\nplanes = [int(k[-1]) for k in keys if \"plane\" in k]\nelse:\nplanes = [0]\nfor k, name in zip(\n[\"F\", \"Fneu\", \"spks\"], [\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]\n):\ntmp = []\ntimestamps = []\nfor i, n in enumerate(planes):\nif multiplane:\npl = \"plane{}\".format(n)\nelse:\npl = name  # This doesn't make sense\ntokeep = info[\"iscell\"][info[\"plane\"] == n].values == 1\nd = np.transpose(ophys[name][pl].data[:][tokeep])\nif ophys[name][pl].timestamps is not None:\nt = ophys[name][pl].timestamps[:]\nelse:\nt = (np.arange(0, len(d)) / self.rate) + ophys[name][\npl\n].starting_time\ntmp.append(d)\ntimestamps.append(t)\ndata[k] = nap.TsdFrame(t=timestamps[0], d=np.hstack(tmp))\nif \"F\" in data.keys():\nself.F = data[\"F\"]\nif \"Fneu\" in data.keys():\nself.Fneu = data[\"Fneu\"]\nif \"spks\" in data.keys():\nself.spks = data[\"spks\"]\nself.plane_info = pd.DataFrame(\ndata=info[\"plane\"][info[\"iscell\"] == 1].values, columns=[\"plane\"]\n)\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"nwbmatic-custom-io/","title":"Custom Loading IO","text":"<p>This example shows how to construct a custom IO loader.</p> <pre><code># -*- coding: utf-8 -*-\n# @Author: gviejo\n# @Date:   2022-01-06 20:01:32\n# @Last Modified by:   gviejo\n# @Last Modified time: 2022-01-06 20:01:57\nimport os\nfrom nwbmatic.loader import BaseLoader\nfrom pynwb import NWBFile, NWBHDF5IO\nclass MyCustomIO(BaseLoader):\ndef __init__(self, path):\n\"\"\"        \n        Parameters\n        ----------\n        path : str\n            The path to the data.\n        \"\"\"     \nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, 'pynapplenwb')\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith('.nwb')]):                    \nsuccess = self.load_my_nwb(path)\nif success: loading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\nself.load_my_data(path)\nself.save_my_data_in_nwb(path)\ndef load_my_data(self, path):\n\"\"\"\n        This load the raw data\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\n'''\n        Load Raw data here\n        '''\nprint(path)\nreturn None\ndef save_my_data_in_nwb(self, path):\n\"\"\"\n        Save the raw data to NWB\n        Parameters\n        ----------\n        path : TYPE\n            Description\n        \"\"\"\nself.nwb_path = os.path.join(path, 'pynapplenwb')\nif os.path.exists(self.nwb_path):\nfiles = os.listdir(self.nwb_path)\nelse:\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if 'nwb' in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, 'r+')\n'''\n        Save data in NWB here\n        '''\nio.close()\nreturn\ndef load_my_nwb(self, path):\n\"\"\"\n        This load the nwb that is already create by the base loader\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nself.nwb_path = os.path.join(path, 'pynapplenwb')\nif os.path.exists(self.nwb_path):\nfiles = os.listdir(self.nwb_path)\nelse:\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if 'nwb' in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, 'r')\nnwbfile = io.read()\n'''\n        Add code to write to nwb file here\n        '''\nio.close()\nmydata = MyCustomIO('.')\nprint(type(mydata))\n</code></pre>"},{"location":"nwbmatic-neurosuite-optitrack/","title":"Tutorial","text":"<p>In this example dataset, the data contains a sample recording from the anterodorsal nucleus of the thalamus and the hippocampus, with both a sleep and a wake phase. It contains both head-direction cells (i.e. cells that fire for a particular direction of the head in the horizontal plane) and place cells (i.e. cells that fire for a particular position in the environment).</p> <p>The example dataset looks like this:</p> <p></p> <p>Preprocessing of the data was made with Kilosort 2.0 and spike sorting was made with Klusters.</p> <p>Tracking of the animal was done with Motive Optitrack. The file A2929-200711_1.csv contains the tracking data (both position and rotation of the head of the animal).</p> <p>The binary file A2929-200711_1_analogin.dat contains the TTL pulses tracking the camera frames.</p> <p>This tutorial demonstrates how to load data with nwbmatic.</p>"},{"location":"nwbmatic-neurosuite-optitrack/#import-nwbmatic","title":"Import nwbmatic","text":"<pre><code>import nwbmatic as ntm\n</code></pre>"},{"location":"nwbmatic-neurosuite-optitrack/#session-loader","title":"Session loader","text":"<p>The first step is to call the function load_session. It will then open a GUI for filling manually the information. The following screenshots show what parameters to use.</p> <pre><code>data_directory = 'your/path/to/A2929-200711'\ndata = ntm.load_session(data_directory, 'neurosuite')\n</code></pre>"},{"location":"nwbmatic-neurosuite-optitrack/#session-information","title":"Session Information","text":"<p>The second step is to provides information about the session and the subject. All the fields shown are suggested by the NWB format (see here and here).</p>"},{"location":"nwbmatic-neurosuite-optitrack/#epochs","title":"Epochs","text":"<p>The epochs tab loads the epochs within the session (typically wake and sleep). In this case, we load the file Epoch_Ts.csv in the data folder. The first column contains the start of the epoch and the second column contains the end of the epoch.</p> <p>If the CSV file contains a third column with an epoch label, the loader will automatically write it in the label column. Otherwise, it is necessary to manually write the epoch labels.</p>"},{"location":"nwbmatic-neurosuite-optitrack/#tracking","title":"Tracking","text":"<p>The tracking tab allows to load tracking data saved with a CSV file. Reading a CSV file is always a challenge when the header is unknown. The default csv file should contains only one row for the header with the column names. The first column should be the time index in seconds. Other formats are DeepLabCut and Optitrack.</p> <p>Frame alignement can vary as well. Pynapple offers three ways to align the tracking frames :</p>"},{"location":"nwbmatic-neurosuite-optitrack/#global-timestamps","title":"Global timestamps","text":"<p>The time column of the CSV file contains the timestamps aligned to the global timeframe of the session.</p>"},{"location":"nwbmatic-neurosuite-optitrack/#local-timestamps","title":"Local timestamps","text":"<p>The time column of the CSV file contains the timestamps aligned to one epoch. In this case, the user should select which epoch.</p>"},{"location":"nwbmatic-neurosuite-optitrack/#ttl-detection","title":"TTL detection","text":"<p>A binary file containing TTL pulses for each tracking frame is located within the folder and can be loaded. Alignement is made with TTL detection.</p> <p>In this example session, Tracking was made with Optitrack and TTL pulses were written to an analogin file recorded by an Intan RHD2000 recording system. The parameters for the tracking tab are shown below.</p> <p></p>"},{"location":"nwbmatic-neurosuite-optitrack/#ephys-loader","title":"Ephys loader","text":"<p>The next step is specific to NeuroSuite. In this case, 2 electrophysiological probes were implanted, one to the ADN and another to the CA1. This step allows to label groups of electrodes as shown below.</p> <p></p>"},{"location":"nwbmatic-neurosuite-optitrack/#nwb-file","title":"NWB file","text":"<p>If successful, a NWB file should be created in session_folder/pynapplenwb/session_name.nwb</p> <p></p> <p>Calling the function load_session should directly read the NWB file and bypass the GUI loader.</p> <pre><code>data = ntm.load_session(data_directory, 'neurosuite')\n</code></pre> <p>In this case, the data that can be used for analysis are spikes, position and epochs.</p> <pre><code>spikes = data.spikes\nposition = data.position\nepochs = data.epochs\nprint(spikes, '\\n')\nprint(position, '\\n')\nprint(epochs, '\\n')\n</code></pre> <pre><code>  Index    Freq. (Hz)    group\n-------  ------------  -------\n      0          7.3         0\n      1          5.73        0\n      2          8.12        0\n      3          6.68        0\n      4         10.77        0\n      5         11           0\n      6         16.52        0\n      7          2.2         1\n      8          2.02        1\n      9          1.07        1\n     10          3.92        1\n     11          3.31        1\n     12          1.09        1\n     13          1.28        1\n     14          1.32        1\n\n                  rx        ry        rz         x         y         z\nTime (s)                                                              \n670.64070   0.343163  5.207148  5.933598 -0.042857  0.425023 -0.195725\n670.64900   0.346745  5.181029  5.917368 -0.043863  0.424850 -0.195110\n670.65735   0.344035  5.155508  5.905679 -0.044853  0.424697 -0.194674\n670.66565   0.322240  5.136537  5.892457 -0.045787  0.424574 -0.194342\n670.67400   0.315836  5.120850  5.891577 -0.046756  0.424563 -0.194059\n...              ...       ...       ...       ...       ...       ...\n1199.96160  6.009812  3.665954  0.230562  0.011241  0.037891 -0.001479\n1199.96995  6.014660  3.634619  0.260742  0.010974  0.038677 -0.002370\n1199.97825  6.031694  3.617849  0.276835  0.010786  0.039410 -0.003156\n1199.98660  6.040435  3.609446  0.287006  0.010661  0.040064 -0.003821\n1199.99495  6.050059  3.609375  0.293275  0.010624  0.040568 -0.004435\n\n[63527 rows x 6 columns]\n\n{'sleep':    start    end\n0    0.0  600.0, 'wake':    start     end\n0  600.0  1200.0}\n</code></pre>"}]}